[2023-09-11 18:23:30,325] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-11 18:23:30,346] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-11 18:23:33,087] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-11 18:23:33,087] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-11 18:23:33,087] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-11 18:23:33,088] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-11 18:23:33,088] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.1647374629974365 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 12.04it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.23s/it]100%|██████████| 1/1 [00:05<00:00,  5.23s/it]
[2023-09-11 18:24:28,475] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-11 18:24:28,475] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-11 18:24:32,150] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.1811487674713135 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-11 18:24:38,856] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-11 18:24:38,907] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-11 18:24:38,907] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-11 18:24:38,907] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-11 18:24:38,907] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-11 18:24:38,907] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-11 18:24:38,907] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-11 18:24:38,907] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 1.8161044120788574 seconds
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-11 18:24:59,724] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-11 18:24:59,725] [INFO] [utils.py:786:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 18:24:59,726] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 39.05 GB, percent = 31.0%
[2023-09-11 18:25:20,197] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-11 18:25:20,198] [INFO] [utils.py:786:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 18:25:20,198] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 78.27 GB, percent = 62.2%
[2023-09-11 18:25:20,198] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-11 18:25:21,364] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-11 18:25:21,365] [INFO] [utils.py:786:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 18:25:21,365] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 78.28 GB, percent = 62.2%
[2023-09-11 18:25:21,393] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-11 18:25:21,393] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-11 18:25:21,393] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f7808c2fa00>
[2023-09-11 18:25:21,393] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 18:25:21,394] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-11 18:25:21,395] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-11 18:25:21,395] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-11 18:25:21,395] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-11 18:25:21,395] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-11 18:25:21,395] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-11 18:25:21,395] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-11 18:25:21,395] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-11 18:25:21,395] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-11 18:25:21,395] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-11 18:25:21,395] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f78005e3d00>
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   fp16_auto_cast ............... False
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   fp16_enabled ................. auto
[2023-09-11 18:25:21,396] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-11 18:25:21,397] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-11 18:25:21,397] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-11 18:25:21,397] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-11 18:25:21,397] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-11 18:25:21,397] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-11 18:25:21,397] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-11 18:25:21,397] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-11 18:25:21,397] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-11 18:25:21,397] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-11 18:25:21,397] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-11 18:25:21,397] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-11 18:25:21,397] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-11 18:25:21,397] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-11 18:25:21,397] [INFO] [config.py:957:print]   optimizer_name ............... adam
[2023-09-11 18:25:21,397] [INFO] [config.py:957:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-11 18:25:21,397] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-11 18:25:21,397] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-11 18:25:21,398] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-11 18:25:21,398] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-11 18:25:21,398] [INFO] [config.py:957:print]   scheduler_name ............... WarmupLR
[2023-09-11 18:25:21,398] [INFO] [config.py:957:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-11 18:25:21,398] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-11 18:25:21,398] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-11 18:25:21,398] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-11 18:25:21,398] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-11 18:25:21,398] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-11 18:25:21,398] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-11 18:25:21,398] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-11 18:25:21,398] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-11 18:25:21,398] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-11 18:25:21,398] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-11 18:25:21,398] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-11 18:25:21,398] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-11 18:25:21,398] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-11 18:25:21,398] [INFO] [config.py:943:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0004992485046386719 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:8080/
Hit Ctrl-C to quit.

  0%|          | 0/4 [00:00<?, ?it/s][2023-09-11 18:25:51,330] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2023-09-11 18:25:51,331] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-11 18:25:51,331] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-11 18:25:51,331] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2023-09-11 18:25:51,331] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
 25%|██▌       | 1/4 [00:14<00:42, 14.11s/it] 50%|█████     | 2/4 [00:25<00:24, 12.41s/it] 75%|███████▌  | 3/4 [00:35<00:11, 11.32s/it]100%|██████████| 4/4 [00:45<00:00, 10.79s/it]100%|██████████| 4/4 [00:45<00:00, 11.33s/it]
127.0.0.1 - - [11/Sep/2023 18:26:38] "GET /hello/4 HTTP/1.1" 200 15
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:10<00:30, 10.16s/it] 50%|█████     | 2/4 [00:20<00:20, 10.23s/it] 75%|███████▌  | 3/4 [00:29<00:09,  9.76s/it]100%|██████████| 4/4 [00:39<00:00,  9.64s/it]100%|██████████| 4/4 [00:39<00:00,  9.77s/it]
127.0.0.1 - - [11/Sep/2023 18:27:44] "GET /hello/4 HTTP/1.1" 200 15
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:01<00:30, 18.20it/s]  7%|▋         | 39/582 [00:02<00:32, 16.65it/s] 10%|▉         | 57/582 [00:03<00:32, 16.13it/s] 13%|█▎        | 75/582 [00:04<00:31, 15.97it/s] 16%|█▌        | 93/582 [00:05<00:30, 15.84it/s] 19%|█▉        | 111/582 [00:07<00:36, 12.94it/s] 22%|██▏       | 129/582 [00:09<00:37, 12.23it/s] 25%|██▌       | 147/582 [00:10<00:36, 11.88it/s] 28%|██▊       | 164/582 [00:11<00:25, 16.41it/s] 29%|██▉       | 170/582 [00:12<00:32, 12.49it/s] 31%|███▏      | 182/582 [00:12<00:24, 16.45it/s] 32%|███▏      | 189/582 [00:13<00:32, 12.16it/s] 34%|███▍      | 200/582 [00:13<00:23, 16.37it/s] 36%|███▌      | 207/582 [00:14<00:32, 11.71it/s] 37%|███▋      | 218/582 [00:15<00:22, 16.27it/s] 38%|███▊      | 224/582 [00:16<00:32, 11.00it/s] 40%|████      | 235/582 [00:16<00:21, 15.92it/s] 42%|████▏     | 242/582 [00:17<00:30, 10.98it/s] 44%|████▎     | 254/582 [00:17<00:20, 16.35it/s] 45%|████▍     | 261/582 [00:19<00:28, 11.31it/s] 47%|████▋     | 272/582 [00:19<00:19, 16.27it/s] 48%|████▊     | 279/582 [00:20<00:27, 11.22it/s] 50%|████▉     | 290/582 [00:20<00:17, 16.25it/s] 51%|█████     | 297/582 [00:21<00:25, 11.28it/s] 53%|█████▎    | 308/582 [00:21<00:16, 16.38it/s] 54%|█████▍    | 315/582 [00:23<00:24, 10.95it/s] 56%|█████▌    | 326/582 [00:23<00:16, 15.93it/s] 57%|█████▋    | 333/582 [00:24<00:24, 10.34it/s] 59%|█████▉    | 344/582 [00:24<00:15, 15.08it/s] 60%|██████    | 350/582 [00:26<00:23, 10.02it/s] 62%|██████▏   | 361/582 [00:26<00:14, 14.90it/s] 63%|██████▎   | 368/582 [00:27<00:20, 10.65it/s] 65%|██████▌   | 379/582 [00:27<00:12, 15.70it/s] 66%|██████▋   | 386/582 [00:28<00:17, 10.97it/s] 68%|██████▊   | 398/582 [00:28<00:11, 16.54it/s] 70%|██████▉   | 405/582 [00:30<00:15, 11.41it/s] 71%|███████▏  | 416/582 [00:30<00:10, 16.48it/s] 73%|███████▎  | 423/582 [00:31<00:14, 11.19it/s] 75%|███████▍  | 434/582 [00:31<00:09, 16.21it/s] 76%|███████▌  | 441/582 [00:32<00:13, 10.64it/s] 78%|███████▊  | 452/582 [00:33<00:08, 15.49it/s] 79%|███████▊  | 458/582 [00:34<00:12, 10.11it/s] 81%|████████  | 470/582 [00:34<00:07, 15.42it/s] 82%|████████▏ | 477/582 [00:35<00:09, 11.61it/s] 84%|████████▎ | 487/582 [00:35<00:05, 16.38it/s] 85%|████████▍ | 494/582 [00:36<00:07, 11.86it/s] 87%|████████▋ | 506/582 [00:36<00:04, 17.87it/s] 88%|████████▊ | 513/582 [00:38<00:05, 12.61it/s] 90%|█████████ | 524/582 [00:38<00:03, 18.10it/s] 91%|█████████ | 531/582 [00:39<00:04, 12.71it/s] 93%|█████████▎| 542/582 [00:39<00:02, 18.39it/s] 94%|█████████▍| 549/582 [00:40<00:02, 12.80it/s] 96%|█████████▋| 561/582 [00:41<00:01, 11.74it/s] 99%|█████████▉| 579/582 [00:42<00:00, 12.85it/s]100%|██████████| 582/582 [00:42<00:00, 12.93it/s]100%|██████████| 582/582 [00:42<00:00, 13.54it/s]
127.0.0.1 - - [11/Sep/2023 18:28:52] "GET /load-model HTTP/1.1" 200 35

-> 测试
127.0.0.1 - - [11/Sep/2023 18:30:06] "POST /inference HTTP/1.1" 200 316

-> 测试
127.0.0.1 - - [11/Sep/2023 18:30:13] "POST /inference HTTP/1.1" 200 321
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:11<00:35, 11.78s/it][2023-09-11 18:31:49,193] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 18:31:49,195] [INFO] [timer.py:199:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.10110607549837024, CurrSamplesPerSec=0.11905439287242299, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 50%|█████     | 2/4 [00:20<00:19,  9.79s/it] 75%|███████▌  | 3/4 [00:28<00:09,  9.02s/it]100%|██████████| 4/4 [00:36<00:00,  8.78s/it]100%|██████████| 4/4 [00:36<00:00,  9.18s/it]
127.0.0.1 - - [11/Sep/2023 18:32:08] "GET /hello/4 HTTP/1.1" 200 15
[2023-09-11 18:34:11,917] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 526040
[2023-09-11 18:34:13,674] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 526040
[2023-09-11 18:34:18,474] [INFO] [launch.py:437:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-11 18:51:16,371] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-11 18:51:16,391] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-11 18:51:18,929] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-11 18:51:18,929] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-11 18:51:18,929] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-11 18:51:18,929] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-11 18:51:18,929] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.2060863971710205 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 12.38it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.32s/it]100%|██████████| 1/1 [00:05<00:00,  5.32s/it]
[2023-09-11 18:52:13,792] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-11 18:52:13,793] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-11 18:52:17,382] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.717057704925537 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-11 18:52:24,898] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-11 18:52:24,950] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-11 18:52:24,951] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-11 18:52:24,951] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-11 18:52:24,951] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-11 18:52:24,951] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-11 18:52:24,951] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-11 18:52:24,951] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 2.501075267791748 seconds
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-11 18:52:44,405] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-11 18:52:44,406] [INFO] [utils.py:786:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 18:52:44,407] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 38.95 GB, percent = 31.0%
[2023-09-11 18:53:08,136] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-11 18:53:08,139] [INFO] [utils.py:786:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 18:53:08,139] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 78.06 GB, percent = 62.1%
[2023-09-11 18:53:08,139] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-11 18:53:09,905] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-11 18:53:09,906] [INFO] [utils.py:786:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 18:53:09,906] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 78.07 GB, percent = 62.1%
[2023-09-11 18:53:09,930] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-11 18:53:09,930] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-11 18:53:09,930] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f4ae9a6c130>
[2023-09-11 18:53:09,930] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 18:53:09,931] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-11 18:53:09,932] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-11 18:53:09,932] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-11 18:53:09,932] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-11 18:53:09,932] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-11 18:53:09,932] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-11 18:53:09,932] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-11 18:53:09,932] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f4ae0413d00>
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-11 18:53:09,933] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-11 18:53:09,934] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-11 18:53:09,934] [INFO] [config.py:957:print]   fp16_auto_cast ............... False
[2023-09-11 18:53:09,934] [INFO] [config.py:957:print]   fp16_enabled ................. auto
[2023-09-11 18:53:09,934] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-11 18:53:09,934] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-11 18:53:09,934] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-11 18:53:09,934] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-11 18:53:09,934] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-11 18:53:09,934] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-11 18:53:09,934] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-11 18:53:09,934] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-11 18:53:09,934] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-11 18:53:09,934] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-11 18:53:09,934] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-11 18:53:09,934] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-11 18:53:09,934] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-11 18:53:09,934] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-11 18:53:09,934] [INFO] [config.py:957:print]   optimizer_name ............... adam
[2023-09-11 18:53:09,934] [INFO] [config.py:957:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-11 18:53:09,935] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-11 18:53:09,935] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-11 18:53:09,935] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-11 18:53:09,935] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-11 18:53:09,935] [INFO] [config.py:957:print]   scheduler_name ............... WarmupLR
[2023-09-11 18:53:09,935] [INFO] [config.py:957:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-11 18:53:09,935] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-11 18:53:09,935] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-11 18:53:09,935] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-11 18:53:09,935] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-11 18:53:09,935] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-11 18:53:09,935] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-11 18:53:09,935] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-11 18:53:09,935] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-11 18:53:09,935] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-11 18:53:09,935] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-11 18:53:09,935] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-11 18:53:09,935] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-11 18:53:09,935] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-11 18:53:09,936] [INFO] [config.py:943:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0004734992980957031 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:8080/
Hit Ctrl-C to quit.

  0%|          | 0/4 [00:00<?, ?it/s][2023-09-11 18:53:14,703] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2023-09-11 18:53:14,703] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-11 18:53:14,703] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-11 18:53:14,703] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2023-09-11 18:53:14,703] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
 25%|██▌       | 1/4 [00:13<00:39, 13.06s/it] 50%|█████     | 2/4 [00:23<00:23, 11.55s/it] 75%|███████▌  | 3/4 [00:31<00:09,  9.90s/it]100%|██████████| 4/4 [00:38<00:00,  8.94s/it]100%|██████████| 4/4 [00:38<00:00,  9.74s/it]
127.0.0.1 - - [11/Sep/2023 18:53:55] "GET /hello/4 HTTP/1.1" 200 15
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:07<00:23,  7.90s/it] 50%|█████     | 2/4 [00:16<00:16,  8.12s/it] 75%|███████▌  | 3/4 [00:26<00:08,  8.92s/it]100%|██████████| 4/4 [00:33<00:00,  8.34s/it]100%|██████████| 4/4 [00:33<00:00,  8.38s/it]
127.0.0.1 - - [11/Sep/2023 18:54:31] "GET /hello/4 HTTP/1.1" 200 15
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:08<00:24,  8.02s/it][2023-09-11 18:55:25,346] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 18:55:25,347] [INFO] [timer.py:199:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.12331163150304893, CurrSamplesPerSec=0.12550287652749553, MemAllocated=6.03GB, MaxMemAllocated=7.68GB
 50%|█████     | 2/4 [00:15<00:15,  7.99s/it] 75%|███████▌  | 3/4 [00:25<00:08,  8.47s/it]100%|██████████| 4/4 [00:33<00:00,  8.58s/it]100%|██████████| 4/4 [00:33<00:00,  8.45s/it]
127.0.0.1 - - [11/Sep/2023 18:55:44] "GET /hello/4 HTTP/1.1" 200 15
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:01<00:34, 16.45it/s]  7%|▋         | 39/582 [00:02<00:34, 15.72it/s] 10%|▉         | 57/582 [00:03<00:34, 15.36it/s] 13%|█▎        | 75/582 [00:05<00:34, 14.55it/s] 16%|█▌        | 93/582 [00:06<00:33, 14.80it/s] 19%|█▉        | 111/582 [00:07<00:30, 15.21it/s] 22%|██▏       | 129/582 [00:08<00:29, 15.48it/s] 25%|██▌       | 147/582 [00:09<00:27, 15.74it/s] 28%|██▊       | 165/582 [00:10<00:26, 15.73it/s] 31%|███▏      | 183/582 [00:11<00:25, 15.89it/s] 35%|███▍      | 201/582 [00:12<00:23, 15.99it/s] 38%|███▊      | 219/582 [00:14<00:22, 15.83it/s] 41%|████      | 237/582 [00:15<00:22, 15.14it/s] 44%|████▍     | 255/582 [00:16<00:22, 14.37it/s] 47%|████▋     | 273/582 [00:18<00:21, 14.36it/s] 50%|████▉     | 289/582 [00:18<00:15, 19.17it/s] 51%|█████     | 295/582 [00:19<00:19, 14.53it/s] 53%|█████▎    | 307/582 [00:19<00:14, 18.99it/s] 54%|█████▍    | 314/582 [00:20<00:19, 13.89it/s] 56%|█████▌    | 325/582 [00:20<00:13, 18.61it/s] 57%|█████▋    | 332/582 [00:21<00:18, 13.41it/s] 59%|█████▉    | 344/582 [00:21<00:12, 19.14it/s] 60%|██████    | 351/582 [00:22<00:17, 13.18it/s] 62%|██████▏   | 363/582 [00:24<00:20, 10.48it/s] 65%|██████▌   | 379/582 [00:24<00:12, 16.73it/s] 66%|██████▋   | 387/582 [00:25<00:16, 11.76it/s] 68%|██████▊   | 397/582 [00:25<00:11, 15.62it/s] 69%|██████▉   | 404/582 [00:27<00:15, 11.14it/s] 72%|███████▏  | 417/582 [00:28<00:14, 11.12it/s] 75%|███████▍  | 435/582 [00:29<00:12, 11.56it/s] 77%|███████▋  | 451/582 [00:30<00:07, 17.04it/s] 79%|███████▊  | 458/582 [00:31<00:11, 10.94it/s] 81%|████████  | 469/582 [00:31<00:07, 14.68it/s] 82%|████████▏ | 476/582 [00:34<00:15,  6.84it/s] 84%|████████▍ | 488/582 [00:35<00:09, 10.00it/s] 85%|████████▌ | 495/582 [00:36<00:11,  7.86it/s] 87%|████████▋ | 506/582 [00:36<00:06, 11.31it/s] 88%|████████▊ | 513/582 [00:38<00:08,  8.28it/s] 90%|█████████ | 524/582 [00:38<00:04, 12.07it/s] 91%|█████████ | 531/582 [00:39<00:05,  8.61it/s] 93%|█████████▎| 543/582 [00:41<00:04,  7.99it/s] 96%|█████████▋| 561/582 [00:43<00:02,  9.45it/s] 99%|█████████▉| 579/582 [00:44<00:00, 11.04it/s]100%|██████████| 582/582 [00:44<00:00, 11.20it/s]100%|██████████| 582/582 [00:44<00:00, 13.06it/s]
127.0.0.1 - - [11/Sep/2023 18:57:08] "GET /load-model HTTP/1.1" 200 35
Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/rwkv-trainer/app.py", line 149, in inference
    results = []
KeyError: 'message'
127.0.0.1 - - [11/Sep/2023 18:57:14] "POST /inference HTTP/1.1" 500 750
[2023-09-11 18:57:58,187] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-11 18:57:58,207] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-11 18:58:00,946] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-11 18:58:00,946] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-11 18:58:00,946] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-11 18:58:00,946] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-11 18:58:00,946] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.1906545162200928 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.89it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.47s/it]100%|██████████| 1/1 [00:05<00:00,  5.47s/it]
[2023-09-11 18:58:56,347] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-11 18:58:56,347] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W socket.cpp:426] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:426] [c10d] The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
[E socket.cpp:462] [c10d] The server socket has failed to listen on any local network address.
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /home/neromous/Documents/blackfog/rwkv-trainer/app.py:79 in <module>         │
│                                                                              │
│    76 model = RWKV(train_args)                                               │
│    77 load_dict = torch.load(args.load_model, map_location="cpu")            │
│    78 model.load_state_dict(load_dict)                                       │
│ ❱  79 model_engine, optimizer, _, _ = deepspeed.initialize(model=model,      │
│    80 │   │   │   │   │   │   │   │   │   │   │   │   │    model_parameters= │
│    81 │   │   │   │   │   │   │   │   │   │   │   │   │    config="ds_config │
│    82                                                                        │
│                                                                              │
│ /home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/deepspee │
│ d/__init__.py:129 in initialize                                              │
│                                                                              │
│   126 │   global dist                                                        │
│   127 │   from deepspeed import comm as dist                                 │
│   128 │   dist_backend = get_accelerator().communication_backend_name()      │
│ ❱ 129 │   dist.init_distributed(dist_backend=dist_backend, dist_init_require │
│   130 │                                                                      │
│   131 │   # Set config using config_params for backwards compat              │
│   132 │   if config is None and config_params is not None:                   │
│                                                                              │
│ /home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/deepspee │
│ d/comm/comm.py:588 in init_distributed                                       │
│                                                                              │
│   585 │   │   │   if int(os.getenv('RANK', '0')) == 0:                       │
│   586 │   │   │   │   utils.logger.info('Initializing TorchBackend in DeepSp │
│   587 │   │   │   # Create a torch backend object, initialize torch distribu │
│ ❱ 588 │   │   │   cdb = TorchBackend(dist_backend, timeout, init_method, ran │
│   589                                                                        │
│   590                                                                        │
│   591 def mpi_discovery(distributed_port=TORCH_DISTRIBUTED_DEFAULT_PORT, ver │
│                                                                              │
│ /home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/deepspee │
│ d/comm/torch.py:32 in __init__                                               │
│                                                                              │
│    29 │   │   # The idea is to fake that dist backend is initialized even wh │
│    30 │   │   # it is not so we can run on a single GPU without doing any in │
│    31 │   │   self.single_gpu_mode = True                                    │
│ ❱  32 │   │   self.init_process_group(backend, timeout, init_method, rank, w │
│    33 │                                                                      │
│    34 │   @classmethod                                                       │
│    35 │   def get_all_gather_function(self):                                 │
│                                                                              │
│ /home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/deepspee │
│ d/comm/torch.py:58 in init_process_group                                     │
│                                                                              │
│    55 │                                                                      │
│    56 │   def init_process_group(self, backend, timeout, init_method, rank,  │
│    57 │   │   if not torch.distributed.is_initialized():                     │
│ ❱  58 │   │   │   torch.distributed.init_process_group(backend,              │
│    59 │   │   │   │   │   │   │   │   │   │   │   │    timeout=timeout,      │
│    60 │   │   │   │   │   │   │   │   │   │   │   │    init_method=init_meth │
│    61 │   │   │   │   │   │   │   │   │   │   │   │    rank=rank,            │
│                                                                              │
│ /home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/di │
│ stributed/distributed_c10d.py:900 in init_process_group                      │
│                                                                              │
│    897 │   │   │   rendezvous_iterator = rendezvous(                         │
│    898 │   │   │   │   init_method, rank, world_size, timeout=timeout        │
│    899 │   │   │   )                                                         │
│ ❱  900 │   │   │   store, rank, world_size = next(rendezvous_iterator)       │
│    901 │   │   │   store.set_timeout(timeout)                                │
│    902 │   │   │                                                             │
│    903 │   │   │   # Use a PrefixStore to avoid accidental overrides of keys │
│                                                                              │
│ /home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/di │
│ stributed/rendezvous.py:245 in _env_rendezvous_handler                       │
│                                                                              │
│   242 │   master_addr = _get_env_or_raise("MASTER_ADDR")                     │
│   243 │   master_port = int(_get_env_or_raise("MASTER_PORT"))                │
│   244 │                                                                      │
│ ❱ 245 │   store = _create_c10d_store(master_addr, master_port, rank, world_s │
│   246 │                                                                      │
│   247 │   yield (store, rank, world_size)                                    │
│   248                                                                        │
│                                                                              │
│ /home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/di │
│ stributed/rendezvous.py:176 in _create_c10d_store                            │
│                                                                              │
│   173 │   │   return PrefixStore(f"/worker/attempt_{attempt}", tcp_store)    │
│   174 │   else:                                                              │
│   175 │   │   start_daemon = rank == 0                                       │
│ ❱ 176 │   │   return TCPStore(                                               │
│   177 │   │   │   hostname, port, world_size, start_daemon, timeout, multi_t │
│   178 │   │   )                                                              │
│   179                                                                        │
╰──────────────────────────────────────────────────────────────────────────────╯
RuntimeError: The server socket has failed to listen on any local network 
address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address
already in use). The server socket has failed to bind to 0.0.0.0:29500 (errno: 
98 - Address already in use).
[2023-09-11 18:59:04,036] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 530525
[2023-09-11 18:59:04,037] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed', '--deepspeed_config', 'ds_config.config'] exits with return code = 1
[2023-09-11 18:59:54,840] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 529076
[2023-09-11 18:59:56,693] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 529076
[2023-09-11 19:00:00,812] [INFO] [launch.py:437:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-11 19:00:05,914] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-11 19:00:05,935] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-11 19:00:08,733] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-11 19:00:08,733] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-11 19:00:08,733] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-11 19:00:08,733] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-11 19:00:08,733] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.348383665084839 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 14.93it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.15s/it]100%|██████████| 1/1 [00:05<00:00,  5.15s/it]
[2023-09-11 19:01:01,645] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-11 19:01:01,645] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-11 19:01:06,252] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.568038702011108 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-11 19:01:13,499] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-11 19:01:13,552] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-11 19:01:13,552] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-11 19:01:13,553] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-11 19:01:13,553] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-11 19:01:13,553] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-11 19:01:13,553] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-11 19:01:13,553] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 2.176424741744995 seconds
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-11 19:01:31,390] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-11 19:01:31,391] [INFO] [utils.py:786:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 19:01:31,391] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 39.0 GB, percent = 31.0%
[2023-09-11 19:01:55,131] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-11 19:01:55,135] [INFO] [utils.py:786:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 19:01:55,136] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 78.17 GB, percent = 62.2%
[2023-09-11 19:01:55,136] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-11 19:01:58,272] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-11 19:01:58,276] [INFO] [utils.py:786:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 19:01:58,277] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 78.17 GB, percent = 62.2%
[2023-09-11 19:01:58,378] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-11 19:01:58,379] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-11 19:01:58,379] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f2f53daad90>
[2023-09-11 19:01:58,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 19:01:58,385] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-11 19:01:58,386] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-11 19:01:58,386] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-11 19:01:58,386] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-11 19:01:58,387] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-11 19:01:58,388] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-11 19:01:58,388] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-11 19:01:58,388] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-11 19:01:58,388] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-11 19:01:58,389] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-11 19:01:58,389] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2f406cb070>
[2023-09-11 19:01:58,389] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-11 19:01:58,389] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-11 19:01:58,390] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-11 19:01:58,390] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-11 19:01:58,390] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-11 19:01:58,390] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-11 19:01:58,390] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-11 19:01:58,391] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-11 19:01:58,391] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-11 19:01:58,391] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
[2023-09-11 19:01:58,391] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-11 19:01:58,391] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-11 19:01:58,392] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-11 19:01:58,392] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-11 19:01:58,392] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-11 19:01:58,392] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-11 19:01:58,392] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-11 19:01:58,392] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-11 19:01:58,393] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-11 19:01:58,393] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-11 19:01:58,393] [INFO] [config.py:957:print]   fp16_auto_cast ............... False
[2023-09-11 19:01:58,393] [INFO] [config.py:957:print]   fp16_enabled ................. auto
[2023-09-11 19:01:58,393] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-11 19:01:58,393] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-11 19:01:58,394] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-11 19:01:58,394] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-11 19:01:58,394] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-11 19:01:58,394] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-11 19:01:58,394] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-11 19:01:58,395] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-11 19:01:58,395] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-11 19:01:58,395] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-11 19:01:58,395] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-11 19:01:58,395] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-11 19:01:58,396] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-11 19:01:58,396] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-11 19:01:58,396] [INFO] [config.py:957:print]   optimizer_name ............... adam
[2023-09-11 19:01:58,396] [INFO] [config.py:957:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-11 19:01:58,397] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-11 19:01:58,397] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-11 19:01:58,397] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-11 19:01:58,397] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-11 19:01:58,397] [INFO] [config.py:957:print]   scheduler_name ............... WarmupLR
[2023-09-11 19:01:58,397] [INFO] [config.py:957:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-11 19:01:58,398] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-11 19:01:58,398] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-11 19:01:58,398] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-11 19:01:58,398] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-11 19:01:58,398] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-11 19:01:58,398] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-11 19:01:58,399] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-11 19:01:58,399] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-11 19:01:58,399] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-11 19:01:58,399] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-11 19:01:58,400] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-11 19:01:58,400] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-11 19:01:58,400] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-11 19:01:58,401] [INFO] [config.py:943:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0006968975067138672 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:8080/
Hit Ctrl-C to quit.

  0%|          | 0/4 [00:00<?, ?it/s][2023-09-11 19:02:12,305] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2023-09-11 19:02:12,305] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-11 19:02:12,305] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-11 19:02:12,305] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2023-09-11 19:02:12,305] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
 25%|██▌       | 1/4 [00:17<00:51, 17.27s/it] 50%|█████     | 2/4 [00:25<00:24, 12.17s/it] 75%|███████▌  | 3/4 [00:35<00:10, 10.81s/it]100%|██████████| 4/4 [00:44<00:00, 10.34s/it]100%|██████████| 4/4 [00:44<00:00, 11.17s/it]
127.0.0.1 - - [11/Sep/2023 19:02:58] "GET /hello/4 HTTP/1.1" 200 88
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:10<00:30, 10.23s/it] 50%|█████     | 2/4 [00:19<00:19,  9.94s/it] 75%|███████▌  | 3/4 [00:29<00:09,  9.87s/it]100%|██████████| 4/4 [00:40<00:00, 10.20s/it]100%|██████████| 4/4 [00:40<00:00, 10.11s/it]
127.0.0.1 - - [11/Sep/2023 19:04:11] "GET /hello/4 HTTP/1.1" 200 89
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:01<00:31, 17.93it/s]  7%|▋         | 39/582 [00:02<00:33, 16.45it/s] 10%|▉         | 57/582 [00:03<00:32, 15.97it/s] 13%|█▎        | 75/582 [00:04<00:32, 15.70it/s] 16%|█▌        | 93/582 [00:05<00:31, 15.51it/s] 19%|█▉        | 111/582 [00:07<00:30, 15.42it/s] 22%|██▏       | 129/582 [00:08<00:29, 15.31it/s] 25%|██▌       | 147/582 [00:09<00:28, 15.27it/s] 28%|██▊       | 165/582 [00:10<00:27, 15.21it/s] 31%|███▏      | 183/582 [00:11<00:26, 15.18it/s] 35%|███▍      | 201/582 [00:13<00:25, 15.08it/s] 38%|███▊      | 219/582 [00:14<00:24, 15.06it/s] 41%|████      | 237/582 [00:15<00:23, 14.96it/s] 44%|████▍     | 255/582 [00:16<00:21, 14.96it/s] 47%|████▋     | 273/582 [00:17<00:20, 14.95it/s] 50%|█████     | 291/582 [00:19<00:19, 14.94it/s] 53%|█████▎    | 309/582 [00:20<00:18, 15.00it/s] 56%|█████▌    | 327/582 [00:21<00:16, 15.00it/s] 59%|█████▉    | 345/582 [00:22<00:15, 15.00it/s] 62%|██████▏   | 363/582 [00:23<00:14, 15.11it/s] 65%|██████▌   | 381/582 [00:24<00:13, 15.17it/s] 69%|██████▊   | 399/582 [00:26<00:12, 14.93it/s] 71%|███████▏  | 416/582 [00:26<00:08, 20.20it/s] 73%|███████▎  | 422/582 [00:27<00:11, 14.10it/s] 74%|███████▍  | 433/582 [00:27<00:08, 18.01it/s] 76%|███████▌  | 440/582 [00:29<00:11, 12.82it/s] 77%|███████▋  | 451/582 [00:29<00:07, 17.27it/s] 79%|███████▊  | 458/582 [00:30<00:09, 12.44it/s] 81%|████████  | 469/582 [00:30<00:06, 17.32it/s] 82%|████████▏ | 476/582 [00:31<00:08, 12.04it/s] 84%|████████▎ | 487/582 [00:31<00:05, 17.15it/s] 85%|████████▍ | 494/582 [00:32<00:07, 11.96it/s] 87%|████████▋ | 505/582 [00:32<00:04, 17.13it/s] 88%|████████▊ | 512/582 [00:34<00:05, 11.90it/s] 90%|████████▉ | 523/582 [00:34<00:03, 17.22it/s] 91%|█████████ | 530/582 [00:35<00:04, 11.88it/s] 93%|█████████▎| 541/582 [00:35<00:02, 17.25it/s] 94%|█████████▍| 548/582 [00:36<00:02, 11.57it/s] 96%|█████████▌| 559/582 [00:36<00:01, 16.67it/s] 97%|█████████▋| 565/582 [00:38<00:01, 10.52it/s] 99%|█████████▉| 577/582 [00:38<00:00, 15.95it/s]100%|██████████| 582/582 [00:39<00:00, 14.57it/s]

-> 主人： 

-> 琉璃： 
127.0.0.1 - - [11/Sep/2023 19:05:53] "POST /inference HTTP/1.1" 200 967
[2023-09-11 19:07:08,372] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 530756
[2023-09-11 19:07:09,980] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 530756
[2023-09-11 19:07:15,231] [INFO] [launch.py:437:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-11 19:07:28,358] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-11 19:07:28,379] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-11 19:07:30,951] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-11 19:07:30,951] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-11 19:07:30,951] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-11 19:07:30,951] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-11 19:07:30,951] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.9863812923431396 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.67it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.34s/it]100%|██████████| 1/1 [00:05<00:00,  5.34s/it]
[2023-09-11 19:08:22,618] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-11 19:08:22,620] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-11 19:08:27,111] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.131736516952515 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-11 19:08:33,819] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-11 19:08:33,869] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-11 19:08:33,869] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-11 19:08:33,869] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-11 19:08:33,869] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-11 19:08:33,869] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-11 19:08:33,870] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-11 19:08:33,870] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 2.240513324737549 seconds
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-11 19:08:52,333] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-11 19:08:52,334] [INFO] [utils.py:786:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 19:08:52,335] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 38.96 GB, percent = 31.0%
[2023-09-11 19:09:13,907] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-11 19:09:13,908] [INFO] [utils.py:786:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 19:09:13,908] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 78.17 GB, percent = 62.2%
[2023-09-11 19:09:13,909] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-11 19:09:15,096] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-11 19:09:15,096] [INFO] [utils.py:786:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 19:09:15,097] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 78.18 GB, percent = 62.2%
[2023-09-11 19:09:15,121] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-11 19:09:15,121] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-11 19:09:15,121] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f0a11a6eca0>
[2023-09-11 19:09:15,121] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 19:09:15,123] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-11 19:09:15,123] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-11 19:09:15,123] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-11 19:09:15,123] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-11 19:09:15,123] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-11 19:09:15,123] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0a08414d00>
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-11 19:09:15,124] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-11 19:09:15,125] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-11 19:09:15,125] [INFO] [config.py:957:print]   fp16_auto_cast ............... False
[2023-09-11 19:09:15,125] [INFO] [config.py:957:print]   fp16_enabled ................. auto
[2023-09-11 19:09:15,125] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-11 19:09:15,125] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-11 19:09:15,125] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-11 19:09:15,125] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-11 19:09:15,125] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-11 19:09:15,125] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-11 19:09:15,125] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-11 19:09:15,125] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-11 19:09:15,125] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-11 19:09:15,125] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-11 19:09:15,125] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-11 19:09:15,125] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-11 19:09:15,125] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-11 19:09:15,125] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-11 19:09:15,125] [INFO] [config.py:957:print]   optimizer_name ............... adam
[2023-09-11 19:09:15,125] [INFO] [config.py:957:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-11 19:09:15,126] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-11 19:09:15,126] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-11 19:09:15,126] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-11 19:09:15,126] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-11 19:09:15,126] [INFO] [config.py:957:print]   scheduler_name ............... WarmupLR
[2023-09-11 19:09:15,126] [INFO] [config.py:957:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-11 19:09:15,126] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-11 19:09:15,126] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-11 19:09:15,126] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-11 19:09:15,126] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-11 19:09:15,126] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-11 19:09:15,126] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-11 19:09:15,126] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-11 19:09:15,126] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-11 19:09:15,126] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-11 19:09:15,126] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-11 19:09:15,126] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-11 19:09:15,126] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-11 19:09:15,126] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-11 19:09:15,127] [INFO] [config.py:943:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0005168914794921875 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

  0%|          | 0/4 [00:00<?, ?it/s][2023-09-11 19:10:53,062] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2023-09-11 19:10:53,062] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-11 19:10:53,062] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-11 19:10:53,062] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2023-09-11 19:10:53,062] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
 25%|██▌       | 1/4 [00:16<00:48, 16.06s/it] 50%|█████     | 2/4 [00:25<00:24, 12.03s/it] 75%|███████▌  | 3/4 [00:34<00:10, 10.75s/it]100%|██████████| 4/4 [00:43<00:00, 10.06s/it]100%|██████████| 4/4 [00:43<00:00, 10.88s/it]
127.0.0.1 - - [11/Sep/2023 19:11:37] "GET /hello/4 HTTP/1.1" 200 88
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:01<00:29, 18.72it/s]  7%|▋         | 39/582 [00:02<00:31, 17.13it/s] 10%|▉         | 57/582 [00:03<00:31, 16.58it/s] 13%|█▎        | 75/582 [00:04<00:31, 16.26it/s] 16%|█▌        | 93/582 [00:05<00:31, 15.43it/s] 19%|█▉        | 111/582 [00:07<00:30, 15.23it/s] 22%|██▏       | 129/582 [00:08<00:31, 14.38it/s] 25%|██▌       | 147/582 [00:09<00:29, 14.79it/s] 28%|██▊       | 164/582 [00:09<00:20, 20.27it/s] 29%|██▉       | 170/582 [00:11<00:30, 13.71it/s] 31%|███       | 181/582 [00:11<00:22, 17.67it/s] 32%|███▏      | 188/582 [00:12<00:29, 13.24it/s] 35%|███▍      | 201/582 [00:13<00:30, 12.45it/s] 38%|███▊      | 219/582 [00:14<00:26, 13.47it/s] 41%|████      | 237/582 [00:15<00:24, 14.11it/s] 44%|████▍     | 255/582 [00:17<00:24, 13.24it/s] 47%|████▋     | 272/582 [00:17<00:16, 18.62it/s] 48%|████▊     | 279/582 [00:18<00:21, 14.30it/s] 50%|█████     | 291/582 [00:20<00:25, 11.39it/s] 53%|█████▎    | 308/582 [00:20<00:16, 17.06it/s] 54%|█████▍    | 315/582 [00:21<00:21, 12.50it/s] 56%|█████▌    | 327/582 [00:22<00:23, 10.84it/s] 59%|█████▉    | 345/582 [00:24<00:19, 12.30it/s] 62%|██████▏   | 363/582 [00:25<00:17, 12.60it/s] 65%|██████▌   | 379/582 [00:25<00:11, 17.40it/s] 66%|██████▌   | 384/582 [00:28<00:21,  9.02it/s] 68%|██████▊   | 397/582 [00:28<00:14, 12.53it/s] 69%|██████▉   | 403/582 [00:30<00:22,  7.80it/s] 71%|███████▏  | 415/582 [00:30<00:15, 11.08it/s] 72%|███████▏  | 420/582 [00:32<00:22,  7.34it/s] 74%|███████▍  | 433/582 [00:32<00:13, 11.21it/s] 75%|███████▌  | 439/582 [00:34<00:21,  6.77it/s] 77%|███████▋  | 451/582 [00:34<00:12, 10.14it/s] 78%|███████▊  | 456/582 [00:36<00:15,  8.05it/s] 81%|████████  | 469/582 [00:36<00:08, 12.59it/s] 82%|████████▏ | 475/582 [00:37<00:10,  9.93it/s] 84%|████████▎ | 487/582 [00:37<00:06, 14.74it/s] 85%|████████▍ | 493/582 [00:39<00:11,  8.06it/s] 87%|████████▋ | 505/582 [00:39<00:06, 12.17it/s] 88%|████████▊ | 510/582 [00:41<00:08,  8.47it/s] 90%|████████▉ | 523/582 [00:41<00:04, 13.32it/s] 91%|█████████ | 529/582 [00:42<00:06,  8.76it/s] 93%|█████████▎| 541/582 [00:42<00:03, 13.29it/s] 94%|█████████▍| 547/582 [00:44<00:03, 10.27it/s] 96%|█████████▌| 559/582 [00:44<00:01, 15.32it/s] 97%|█████████▋| 565/582 [00:45<00:01, 10.39it/s] 99%|█████████▉| 577/582 [00:45<00:00, 15.52it/s]100%|██████████| 582/582 [00:47<00:00, 12.31it/s]
127.0.0.1 - - [11/Sep/2023 19:14:17] "GET /load-model HTTP/1.1" 200 35
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:13<00:39, 13.26s/it] 50%|█████     | 2/4 [00:23<00:22, 11.46s/it] 75%|███████▌  | 3/4 [00:34<00:11, 11.04s/it]100%|██████████| 4/4 [00:44<00:00, 10.72s/it]100%|██████████| 4/4 [00:44<00:00, 11.06s/it]
127.0.0.1 - - [11/Sep/2023 19:24:36] "GET /hello/4 HTTP/1.1" 200 89
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:01<00:30, 18.31it/s]  7%|▋         | 39/582 [00:02<00:31, 17.06it/s] 10%|▉         | 57/582 [00:03<00:31, 16.52it/s] 13%|█▎        | 75/582 [00:04<00:30, 16.36it/s] 16%|█▌        | 93/582 [00:05<00:30, 16.20it/s] 19%|█▉        | 111/582 [00:06<00:29, 16.01it/s] 22%|██▏       | 129/582 [00:07<00:28, 16.04it/s] 25%|██▌       | 147/582 [00:09<00:27, 15.97it/s] 28%|██▊       | 165/582 [00:10<00:26, 15.94it/s] 31%|███▏      | 183/582 [00:11<00:25, 15.84it/s] 35%|███▍      | 201/582 [00:12<00:23, 15.89it/s] 38%|███▊      | 219/582 [00:13<00:22, 15.94it/s] 41%|████      | 237/582 [00:14<00:21, 15.95it/s] 44%|████▍     | 255/582 [00:15<00:20, 15.94it/s] 47%|████▋     | 273/582 [00:16<00:19, 15.96it/s] 50%|█████     | 291/582 [00:18<00:18, 15.96it/s] 53%|█████▎    | 309/582 [00:19<00:17, 15.91it/s] 56%|█████▌    | 327/582 [00:20<00:16, 15.90it/s] 59%|█████▉    | 345/582 [00:21<00:14, 15.95it/s] 62%|██████▏   | 363/582 [00:22<00:13, 15.98it/s] 65%|██████▌   | 381/582 [00:23<00:12, 15.91it/s] 69%|██████▊   | 399/582 [00:24<00:11, 15.94it/s] 72%|███████▏  | 417/582 [00:25<00:10, 16.00it/s] 75%|███████▍  | 435/582 [00:27<00:09, 16.05it/s] 78%|███████▊  | 453/582 [00:28<00:08, 16.10it/s] 81%|████████  | 471/582 [00:29<00:06, 16.03it/s] 84%|████████▍ | 489/582 [00:30<00:05, 16.09it/s] 87%|████████▋ | 507/582 [00:31<00:04, 16.08it/s] 90%|█████████ | 525/582 [00:32<00:03, 16.14it/s] 93%|█████████▎| 543/582 [00:33<00:02, 16.16it/s] 96%|█████████▋| 561/582 [00:34<00:01, 15.80it/s] 99%|█████████▉| 579/582 [00:36<00:00, 15.44it/s]100%|██████████| 582/582 [00:36<00:00, 15.37it/s]100%|██████████| 582/582 [00:36<00:00, 15.98it/s]
127.0.0.1 - - [11/Sep/2023 19:25:40] "GET /load-model HTTP/1.1" 200 35

-> 主人： 

-> 琉璃： 
127.0.0.1 - - [11/Sep/2023 19:26:02] "POST /inference HTTP/1.1" 200 680

-> 主人： 

-> 琉璃： 
127.0.0.1 - - [11/Sep/2023 19:26:16] "POST /inference HTTP/1.1" 200 668

-> 主人： 

-> 琉璃： 
127.0.0.1 - - [11/Sep/2023 19:26:21] "POST /inference HTTP/1.1" 200 686

-> 主人： 

-> 琉璃： 
127.0.0.1 - - [11/Sep/2023 19:27:19] "POST /inference HTTP/1.1" 200 686

-> 主人： dfasdfdsdd

-> 琉璃： 
127.0.0.1 - - [11/Sep/2023 19:28:08] "POST /inference HTTP/1.1" 200 668

-> 主人： 你好啊

-> 琉璃： 
127.0.0.1 - - [11/Sep/2023 19:30:11] "POST /inference HTTP/1.1" 200 668

-> 主人： 你好啊

-> 琉璃： 
127.0.0.1 - - [11/Sep/2023 19:30:31] "POST /inference HTTP/1.1" 200 685

-> 主人： 你们好啊

-> 琉璃： 
127.0.0.1 - - [11/Sep/2023 19:32:02] "POST /inference HTTP/1.1" 200 683

-> 主人： 

-> 琉璃： 
127.0.0.1 - - [11/Sep/2023 19:32:56] "POST /inference HTTP/1.1" 200 677

-> 主人： 霄霄

-> 琉璃： 
127.0.0.1 - - [11/Sep/2023 19:33:02] "POST /inference HTTP/1.1" 200 685

-> 主人： 哈哈答复

-> 琉璃： 
127.0.0.1 - - [11/Sep/2023 19:33:16] "POST /inference HTTP/1.1" 200 683

-> 主人： 霄霄

-> 琉璃： 
127.0.0.1 - - [11/Sep/2023 19:33:30] "POST /inference HTTP/1.1" 200 683

-> 主人： 谁是阿邦？

-> 琉璃： 
127.0.0.1 - - [11/Sep/2023 19:33:39] "POST /inference HTTP/1.1" 200 686

-> 主人： 什么是奶牛？

-> 琉璃： 
127.0.0.1 - - [11/Sep/2023 19:33:54] "POST /inference HTTP/1.1" 200 668
127.0.0.1 - - [11/Sep/2023 19:34:08] "GET /reset-state HTTP/1.1" 200 35

-> 主人： 你们好啊

-> 琉璃： 
127.0.0.1 - - [11/Sep/2023 19:34:25] "POST /inference HTTP/1.1" 200 689
[2023-09-11 20:11:38,229] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-11 20:11:39,560] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-11 20:11:39,584] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-11 20:11:41,398] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-11 20:11:42,700] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-11 20:11:42,700] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-11 20:11:42,700] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-11 20:11:42,700] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-11 20:11:42,700] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-11 20:11:44,524] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.711709976196289 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Creating extension directory /home/neromous/.cache/torch_extensions/py39_cu117/wkv_512_bf16...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_512_bf16/build.ninja...
Building extension module wkv_512_bf16...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/3] c++ -MMD -MF wkv_op_bf16.o.d -DTORCH_EXTENSION_NAME=wkv_512_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/TH -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/neromous/rwkv-trainer/cuda/wkv_op_bf16.cpp -o wkv_op_bf16.o 
[2/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_512_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/TH -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=512 -c /home/neromous/rwkv-trainer/cuda/wkv_cuda_bf16.cu -o wkv_cuda_bf16.cuda.o 
ptxas info    : 1 bytes gmem
ptxas info    : Compiling entry function '_Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S4_S4_PS2_S5_S5_S5_' for 'sm_86'
ptxas info    : Function properties for _Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S4_S4_PS2_S5_S5_S5_
    4096 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 48 registers, 448 bytes cmem[0], 16 bytes cmem[2]
ptxas info    : Compiling entry function '_Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_PS2_' for 'sm_86'
ptxas info    : Function properties for _Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_PS2_
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 39 registers, 408 bytes cmem[0]
[3/3] c++ wkv_op_bf16.o wkv_cuda_bf16.cuda.o -shared -L/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_512_bf16.so
Loading extension module wkv_512_bf16...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 10.56it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:04<00:00,  4.86s/it]100%|██████████| 1/1 [00:04<00:00,  4.86s/it]
[2023-09-11 20:12:52,160] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-11 20:12:52,161] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-11 20:12:52,161] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-11 20:12:54,929] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.122183084487915 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-11 20:13:00,431] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-11 20:13:00,481] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-11 20:13:00,482] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-11 20:13:00,482] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-11 20:13:00,482] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-11 20:13:00,482] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-11 20:13:00,482] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-11 20:13:00,482] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-11 20:13:12,440] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-11 20:13:12,441] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 20:13:12,441] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 28.37 GB, percent = 7.5%
[2023-09-11 20:13:26,449] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-11 20:13:26,450] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 20:13:26,450] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 67.56 GB, percent = 17.9%
[2023-09-11 20:13:26,450] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-11 20:13:27,415] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-11 20:13:27,416] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 20:13:27,416] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 67.56 GB, percent = 17.9%
[2023-09-11 20:13:27,439] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-11 20:13:27,440] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-11 20:13:27,440] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f51d0920640>
[2023-09-11 20:13:27,440] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:13:27,441] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-11 20:13:27,441] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-11 20:13:27,441] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-11 20:13:27,441] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-11 20:13:27,441] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-11 20:13:27,441] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-11 20:13:27,441] [INFO] [config.py:967:print]   bfloat16_enabled ............. auto
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f51d09e0d00>
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... None
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   fp16_auto_cast ............... None
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   fp16_enabled ................. False
[2023-09-11 20:13:27,442] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 1
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   loss_scale ................... 1.0
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-11 20:13:27,443] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-11 20:13:27,444] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-11 20:13:27,444] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-11 20:13:27,444] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-11 20:13:27,444] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-11 20:13:27,444] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-11 20:13:27,444] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-11 20:13:27,444] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-11 20:13:27,444] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-11 20:13:27,444] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-11 20:13:27,444] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-11 20:13:27,444] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-11 20:13:27,444] [INFO] [config.py:953:print_user_config]   json = {
    "bfloat16": {
        "enabled": "auto"
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

  0%|          | 0/4 [00:00<?, ?it/s][2023-09-11 20:13:30,758] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-09-11 20:13:30,758] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-11 20:13:30,758] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-11 20:13:30,759] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-09-11 20:13:30,759] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
 25%|██▌       | 1/4 [00:09<00:27,  9.20s/it] 50%|█████     | 2/4 [00:15<00:14,  7.43s/it] 75%|███████▌  | 3/4 [00:21<00:06,  6.87s/it]100%|██████████| 4/4 [00:27<00:00,  6.61s/it]100%|██████████| 4/4 [00:27<00:00,  6.95s/it]
172.16.0.62 - - [11/Sep/2023 20:14:00] "GET /hello/4 HTTP/1.1" 200 86
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:06<00:19,  6.66s/it] 50%|█████     | 2/4 [00:12<00:12,  6.17s/it] 75%|███████▌  | 3/4 [00:18<00:06,  6.01s/it]100%|██████████| 4/4 [00:24<00:00,  5.96s/it]100%|██████████| 4/4 [00:24<00:00,  6.05s/it]
172.16.0.62 - - [11/Sep/2023 20:14:27] "GET /hello/4 HTTP/1.1" 200 88
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:06<01:59,  6.29s/it][2023-09-11 20:15:01,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:15:01,964] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.16431923930799386, CurrSamplesPerSec=0.1717775521523531, MemAllocated=6.03GB, MaxMemAllocated=7.68GB
 10%|█         | 2/20 [00:12<01:48,  6.02s/it] 15%|█▌        | 3/20 [00:17<01:40,  5.91s/it] 20%|██        | 4/20 [00:23<01:33,  5.86s/it] 25%|██▌       | 5/20 [00:29<01:27,  5.82s/it] 30%|███       | 6/20 [00:35<01:21,  5.86s/it] 35%|███▌      | 7/20 [00:41<01:16,  5.85s/it] 40%|████      | 8/20 [00:46<01:09,  5.80s/it] 45%|████▌     | 9/20 [00:52<01:03,  5.74s/it] 50%|█████     | 10/20 [00:58<00:57,  5.71s/it] 55%|█████▌    | 11/20 [01:03<00:51,  5.68s/it][2023-09-11 20:15:59,195] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:15:59,196] [INFO] [timer.py:260:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=0.16997389914980757, CurrSamplesPerSec=0.178451512230786, MemAllocated=6.03GB, MaxMemAllocated=7.68GB
 60%|██████    | 12/20 [01:09<00:45,  5.66s/it] 65%|██████▌   | 13/20 [01:15<00:39,  5.69s/it] 70%|███████   | 14/20 [01:20<00:34,  5.74s/it] 75%|███████▌  | 15/20 [01:26<00:28,  5.72s/it] 80%|████████  | 16/20 [01:32<00:22,  5.62s/it] 85%|████████▌ | 17/20 [01:37<00:16,  5.60s/it] 90%|█████████ | 18/20 [01:43<00:11,  5.59s/it] 95%|█████████▌| 19/20 [01:48<00:05,  5.57s/it]100%|██████████| 20/20 [01:54<00:00,  5.55s/it]100%|██████████| 20/20 [01:54<00:00,  5.71s/it]
172.16.0.62 - - [11/Sep/2023 20:16:45] "GET /hello/20 HTTP/1.1" 200 400
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:00<00:26, 21.07it/s]  7%|▋         | 39/582 [00:01<00:28, 19.31it/s] 10%|▉         | 57/582 [00:02<00:27, 18.77it/s] 13%|█▎        | 75/582 [00:03<00:27, 18.52it/s] 16%|█▌        | 93/582 [00:04<00:26, 18.38it/s] 19%|█▉        | 111/582 [00:05<00:25, 18.27it/s] 22%|██▏       | 129/582 [00:06<00:24, 18.21it/s] 25%|██▌       | 147/582 [00:07<00:23, 18.17it/s] 28%|██▊       | 165/582 [00:08<00:22, 18.13it/s] 31%|███▏      | 183/582 [00:09<00:22, 18.13it/s] 35%|███▍      | 201/582 [00:10<00:21, 18.13it/s] 38%|███▊      | 219/582 [00:11<00:20, 18.13it/s] 41%|████      | 237/582 [00:12<00:19, 18.11it/s] 44%|████▍     | 255/582 [00:13<00:18, 18.11it/s] 47%|████▋     | 273/582 [00:14<00:17, 18.13it/s] 50%|█████     | 291/582 [00:15<00:16, 18.14it/s] 53%|█████▎    | 309/582 [00:16<00:15, 18.15it/s] 56%|█████▌    | 327/582 [00:17<00:14, 18.01it/s] 59%|█████▉    | 345/582 [00:19<00:14, 15.90it/s] 62%|██████▏   | 363/582 [00:21<00:15, 13.90it/s] 65%|██████▌   | 381/582 [00:22<00:14, 13.54it/s] 69%|██████▊   | 399/582 [00:23<00:13, 13.30it/s] 72%|███████▏  | 417/582 [00:25<00:12, 13.14it/s] 75%|███████▍  | 435/582 [00:26<00:11, 13.01it/s] 77%|███████▋  | 451/582 [00:26<00:07, 17.42it/s] 79%|███████▊  | 457/582 [00:28<00:09, 12.62it/s] 81%|████████  | 470/582 [00:28<00:06, 16.93it/s] 82%|████████▏ | 477/582 [00:29<00:08, 12.02it/s] 84%|████████▍ | 488/582 [00:29<00:05, 16.10it/s] 85%|████████▌ | 495/582 [00:31<00:07, 11.21it/s] 87%|████████▋ | 506/582 [00:31<00:04, 15.61it/s] 88%|████████▊ | 513/582 [00:32<00:06, 10.75it/s] 90%|█████████ | 524/582 [00:32<00:03, 15.31it/s] 91%|█████████ | 531/582 [00:34<00:05, 10.09it/s] 93%|█████████▎| 542/582 [00:34<00:02, 14.60it/s] 94%|█████████▍| 549/582 [00:35<00:03, 10.17it/s] 96%|█████████▌| 560/582 [00:35<00:01, 14.81it/s] 97%|█████████▋| 567/582 [00:37<00:01, 10.23it/s] 99%|█████████▉| 578/582 [00:37<00:00, 14.94it/s]100%|██████████| 582/582 [00:38<00:00, 15.00it/s]

-> 主人： 你好啊

-> 琉璃： 
172.16.0.62 - - [11/Sep/2023 20:17:54] "POST /inference HTTP/1.1" 200 668

-> 主人： 介绍阿邦其人

-> 琉璃： 
172.16.0.62 - - [11/Sep/2023 20:18:11] "POST /inference HTTP/1.1" 200 668

-> 主人： 阿邦和霄霄是什么关系？

-> 琉璃： 
172.16.0.62 - - [11/Sep/2023 20:18:28] "POST /inference HTTP/1.1" 200 659
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:07<00:22,  7.57s/it][2023-09-11 20:19:15,803] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:19:15,804] [INFO] [timer.py:260:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=0.1709062356189593, CurrSamplesPerSec=0.1803205828034095, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 50%|█████     | 2/4 [00:13<00:12,  6.38s/it] 75%|███████▌  | 3/4 [00:18<00:06,  6.05s/it]100%|██████████| 4/4 [00:24<00:00,  5.92s/it]100%|██████████| 4/4 [00:24<00:00,  6.12s/it]
172.16.0.62 - - [11/Sep/2023 20:19:28] "GET /hello/4 HTTP/1.1" 200 87
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:05<00:15,  5.32s/it] 50%|█████     | 2/4 [00:10<00:10,  5.28s/it] 75%|███████▌  | 3/4 [00:15<00:05,  5.27s/it]100%|██████████| 4/4 [00:21<00:00,  5.36s/it]100%|██████████| 4/4 [00:21<00:00,  5.33s/it]
172.16.0.62 - - [11/Sep/2023 20:22:04] "GET /hello/4 HTTP/1.1" 200 89
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:00<00:26, 21.09it/s]  7%|▋         | 39/582 [00:01<00:28, 19.32it/s] 10%|▉         | 57/582 [00:02<00:27, 18.77it/s] 13%|█▎        | 75/582 [00:03<00:27, 18.54it/s] 16%|█▌        | 93/582 [00:04<00:26, 18.41it/s] 19%|█▉        | 111/582 [00:05<00:25, 18.32it/s] 22%|██▏       | 129/582 [00:06<00:24, 18.29it/s] 25%|██▌       | 147/582 [00:07<00:23, 18.26it/s] 28%|██▊       | 165/582 [00:08<00:22, 18.22it/s] 31%|███▏      | 183/582 [00:09<00:21, 18.18it/s] 35%|███▍      | 201/582 [00:10<00:20, 18.17it/s] 38%|███▊      | 219/582 [00:11<00:19, 18.15it/s] 41%|████      | 237/582 [00:12<00:19, 18.16it/s] 44%|████▍     | 255/582 [00:13<00:18, 18.15it/s] 47%|████▋     | 273/582 [00:14<00:17, 18.16it/s] 50%|█████     | 291/582 [00:15<00:16, 18.17it/s] 53%|█████▎    | 309/582 [00:16<00:15, 18.18it/s] 56%|█████▌    | 327/582 [00:17<00:14, 17.64it/s] 59%|█████▉    | 345/582 [00:18<00:13, 17.78it/s] 62%|██████▏   | 363/582 [00:19<00:12, 17.87it/s] 65%|██████▌   | 381/582 [00:20<00:11, 17.97it/s] 69%|██████▊   | 399/582 [00:21<00:10, 18.03it/s] 72%|███████▏  | 417/582 [00:22<00:09, 18.06it/s] 75%|███████▍  | 435/582 [00:24<00:08, 17.48it/s] 78%|███████▊  | 453/582 [00:25<00:07, 17.66it/s] 81%|████████  | 471/582 [00:26<00:06, 17.79it/s] 84%|████████▍ | 489/582 [00:26<00:05, 17.89it/s] 87%|████████▋ | 507/582 [00:27<00:04, 17.95it/s] 90%|█████████ | 525/582 [00:28<00:03, 18.00it/s] 93%|█████████▎| 543/582 [00:30<00:02, 17.56it/s] 96%|█████████▋| 561/582 [00:31<00:01, 17.73it/s] 99%|█████████▉| 579/582 [00:32<00:00, 17.78it/s]100%|██████████| 582/582 [00:32<00:00, 18.12it/s]
172.16.0.62 - - [11/Sep/2023 20:23:05] "GET /load-model HTTP/1.1" 200 35

-> 主人： 谁是霄霄？

-> 琉璃： 
172.16.0.62 - - [11/Sep/2023 20:23:20] "POST /inference HTTP/1.1" 200 681
172.16.0.62 - - [11/Sep/2023 20:23:42] "GET /reset-state HTTP/1.1" 200 35

-> 主人： 谁是霄霄学妹？

-> 琉璃： 
172.16.0.62 - - [11/Sep/2023 20:23:59] "POST /inference HTTP/1.1" 200 668
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:06<11:26,  6.93s/it]  2%|▏         | 2/100 [00:12<09:47,  5.99s/it]  3%|▎         | 3/100 [00:17<09:11,  5.69s/it][2023-09-11 20:26:14,465] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:26:14,466] [INFO] [timer.py:260:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=0.17321413119422366, CurrSamplesPerSec=0.18842865084936122, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
  4%|▍         | 4/100 [00:22<08:51,  5.54s/it]  5%|▌         | 5/100 [00:28<08:38,  5.46s/it]  6%|▌         | 6/100 [00:33<08:28,  5.41s/it]  7%|▋         | 7/100 [00:38<08:21,  5.39s/it]  8%|▊         | 8/100 [00:44<08:13,  5.37s/it]  9%|▉         | 9/100 [00:49<08:11,  5.40s/it] 10%|█         | 10/100 [00:55<08:07,  5.41s/it] 11%|█         | 11/100 [01:00<08:01,  5.41s/it] 12%|█▏        | 12/100 [01:05<07:56,  5.41s/it] 13%|█▎        | 13/100 [01:11<07:52,  5.43s/it][2023-09-11 20:27:08,447] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:27:08,448] [INFO] [timer.py:260:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=0.1756043688936435, CurrSamplesPerSec=0.18352597058686598, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 14%|█▍        | 14/100 [01:16<07:47,  5.44s/it] 15%|█▌        | 15/100 [01:22<07:42,  5.45s/it] 16%|█▌        | 16/100 [01:27<07:38,  5.46s/it] 17%|█▋        | 17/100 [01:33<07:33,  5.46s/it] 18%|█▊        | 18/100 [01:38<07:28,  5.47s/it] 19%|█▉        | 19/100 [01:44<07:23,  5.48s/it] 20%|██        | 20/100 [01:49<07:18,  5.48s/it] 21%|██        | 21/100 [01:55<07:13,  5.49s/it] 22%|██▏       | 22/100 [02:01<07:14,  5.57s/it] 23%|██▎       | 23/100 [02:06<07:10,  5.59s/it][2023-09-11 20:28:03,894] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:28:03,894] [INFO] [timer.py:260:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=0.17641589563737609, CurrSamplesPerSec=0.17711176644189797, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 24%|██▍       | 24/100 [02:12<07:06,  5.61s/it] 25%|██▌       | 25/100 [02:17<06:59,  5.59s/it] 26%|██▌       | 26/100 [02:23<06:46,  5.49s/it] 27%|██▋       | 27/100 [02:28<06:35,  5.41s/it] 28%|██▊       | 28/100 [02:33<06:26,  5.37s/it] 29%|██▉       | 29/100 [02:38<06:19,  5.35s/it] 30%|███       | 30/100 [02:44<06:12,  5.33s/it] 31%|███       | 31/100 [02:49<06:06,  5.32s/it] 32%|███▏      | 32/100 [02:54<06:00,  5.30s/it] 33%|███▎      | 33/100 [03:00<05:54,  5.29s/it][2023-09-11 20:28:56,885] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:28:56,886] [INFO] [timer.py:260:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=0.17813099320611847, CurrSamplesPerSec=0.18978552758113704, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 34%|███▍      | 34/100 [03:05<05:49,  5.29s/it] 35%|███▌      | 35/100 [03:10<05:45,  5.32s/it] 36%|███▌      | 36/100 [03:16<05:44,  5.38s/it] 37%|███▋      | 37/100 [03:21<05:39,  5.39s/it] 38%|███▊      | 38/100 [03:27<05:34,  5.40s/it] 39%|███▉      | 39/100 [03:32<05:27,  5.36s/it] 40%|████      | 40/100 [03:37<05:19,  5.33s/it] 41%|████      | 41/100 [03:42<05:13,  5.31s/it] 42%|████▏     | 42/100 [03:48<05:07,  5.30s/it] 43%|████▎     | 43/100 [03:53<05:01,  5.29s/it][2023-09-11 20:29:50,218] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:29:50,219] [INFO] [timer.py:260:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=0.17928743316540538, CurrSamplesPerSec=0.19081311887505395, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 44%|████▍     | 44/100 [03:58<04:55,  5.28s/it] 45%|████▌     | 45/100 [04:03<04:49,  5.27s/it] 46%|████▌     | 46/100 [04:09<04:44,  5.27s/it] 47%|████▋     | 47/100 [04:14<04:39,  5.28s/it] 48%|████▊     | 48/100 [04:19<04:34,  5.28s/it] 49%|████▉     | 49/100 [04:25<04:28,  5.27s/it] 50%|█████     | 50/100 [04:30<04:25,  5.31s/it] 51%|█████     | 51/100 [04:35<04:22,  5.36s/it] 52%|█████▏    | 52/100 [04:41<04:17,  5.36s/it] 53%|█████▎    | 53/100 [04:46<04:12,  5.37s/it][2023-09-11 20:30:43,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:30:43,451] [INFO] [timer.py:260:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=0.1802286310309958, CurrSamplesPerSec=0.1904727131965862, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 54%|█████▍    | 54/100 [04:51<04:05,  5.33s/it] 55%|█████▌    | 55/100 [04:57<03:59,  5.32s/it] 56%|█████▌    | 56/100 [05:02<03:53,  5.30s/it] 57%|█████▋    | 57/100 [05:07<03:47,  5.29s/it] 58%|█████▊    | 58/100 [05:12<03:42,  5.29s/it] 59%|█████▉    | 59/100 [05:18<03:36,  5.28s/it] 60%|██████    | 60/100 [05:23<03:31,  5.28s/it] 61%|██████    | 61/100 [05:28<03:25,  5.26s/it] 62%|██████▏   | 62/100 [05:33<03:19,  5.25s/it] 63%|██████▎   | 63/100 [05:39<03:14,  5.26s/it][2023-09-11 20:31:36,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:31:36,140] [INFO] [timer.py:260:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=0.18116618425437636, CurrSamplesPerSec=0.18824283828082747, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 64%|██████▍   | 64/100 [05:44<03:10,  5.28s/it] 65%|██████▌   | 65/100 [05:49<03:05,  5.29s/it] 66%|██████▌   | 66/100 [05:55<03:01,  5.34s/it] 67%|██████▋   | 67/100 [06:00<02:58,  5.42s/it] 68%|██████▊   | 68/100 [06:06<02:53,  5.44s/it] 69%|██████▉   | 69/100 [06:11<02:47,  5.41s/it] 70%|███████   | 70/100 [06:17<02:42,  5.42s/it] 71%|███████   | 71/100 [06:22<02:37,  5.43s/it] 72%|███████▏  | 72/100 [06:28<02:32,  5.43s/it] 73%|███████▎  | 73/100 [06:33<02:26,  5.44s/it][2023-09-11 20:32:30,599] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:32:30,600] [INFO] [timer.py:260:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=0.1813969649083542, CurrSamplesPerSec=0.18298855898315994, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 74%|███████▍  | 74/100 [06:39<02:21,  5.45s/it] 75%|███████▌  | 75/100 [06:44<02:16,  5.45s/it] 76%|███████▌  | 76/100 [06:49<02:10,  5.45s/it] 77%|███████▋  | 77/100 [06:55<02:05,  5.45s/it] 78%|███████▊  | 78/100 [07:00<02:00,  5.46s/it] 79%|███████▉  | 79/100 [07:06<01:54,  5.46s/it] 80%|████████  | 80/100 [07:11<01:49,  5.45s/it] 81%|████████  | 81/100 [07:17<01:44,  5.49s/it] 82%|████████▏ | 82/100 [07:22<01:39,  5.53s/it] 83%|████████▎ | 83/100 [07:28<01:34,  5.55s/it][2023-09-11 20:33:25,752] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:33:25,753] [INFO] [timer.py:260:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=0.18139497561113305, CurrSamplesPerSec=0.17785406677058405, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 84%|████████▍ | 84/100 [07:34<01:29,  5.57s/it] 85%|████████▌ | 85/100 [07:39<01:23,  5.53s/it] 86%|████████▌ | 86/100 [07:45<01:17,  5.51s/it] 87%|████████▋ | 87/100 [07:50<01:11,  5.47s/it] 88%|████████▊ | 88/100 [07:55<01:05,  5.46s/it] 89%|████████▉ | 89/100 [08:01<01:00,  5.46s/it] 90%|█████████ | 90/100 [08:06<00:54,  5.46s/it] 91%|█████████ | 91/100 [08:12<00:49,  5.46s/it] 92%|█████████▏| 92/100 [08:17<00:43,  5.46s/it] 93%|█████████▎| 93/100 [08:23<00:38,  5.46s/it][2023-09-11 20:34:20,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:34:20,271] [INFO] [timer.py:260:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=0.18155686227156145, CurrSamplesPerSec=0.18221661700313185, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 94%|█████████▍| 94/100 [08:28<00:32,  5.47s/it] 95%|█████████▌| 95/100 [08:34<00:27,  5.46s/it] 96%|█████████▌| 96/100 [08:39<00:21,  5.46s/it] 97%|█████████▋| 97/100 [08:45<00:16,  5.52s/it] 98%|█████████▊| 98/100 [08:50<00:11,  5.55s/it] 99%|█████████▉| 99/100 [08:56<00:05,  5.56s/it]100%|██████████| 100/100 [09:02<00:00,  5.57s/it]100%|██████████| 100/100 [09:02<00:00,  5.42s/it]
172.16.0.62 - - [11/Sep/2023 20:34:55] "GET /hello/100 HTTP/1.1" 200 1946
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:00<00:26, 21.18it/s]  7%|▋         | 39/582 [00:01<00:28, 19.37it/s] 10%|▉         | 57/582 [00:02<00:27, 18.79it/s] 13%|█▎        | 75/582 [00:03<00:27, 18.51it/s] 16%|█▌        | 93/582 [00:04<00:26, 18.36it/s] 19%|█▉        | 111/582 [00:06<00:26, 17.77it/s] 22%|██▏       | 129/582 [00:07<00:25, 17.83it/s] 25%|██▌       | 147/582 [00:08<00:24, 17.88it/s] 28%|██▊       | 165/582 [00:09<00:23, 17.89it/s] 31%|███▏      | 183/582 [00:10<00:22, 17.92it/s] 35%|███▍      | 201/582 [00:11<00:21, 17.84it/s] 38%|███▊      | 219/582 [00:12<00:20, 17.77it/s] 41%|████      | 237/582 [00:13<00:19, 17.77it/s] 44%|████▍     | 255/582 [00:14<00:18, 17.83it/s] 47%|████▋     | 273/582 [00:15<00:17, 17.87it/s] 50%|█████     | 291/582 [00:16<00:16, 17.88it/s] 53%|█████▎    | 309/582 [00:17<00:15, 17.90it/s] 56%|█████▌    | 327/582 [00:18<00:14, 17.91it/s] 59%|█████▉    | 345/582 [00:19<00:13, 17.91it/s] 62%|██████▏   | 363/582 [00:20<00:12, 17.92it/s] 65%|██████▌   | 381/582 [00:21<00:11, 17.93it/s] 69%|██████▊   | 399/582 [00:22<00:10, 17.92it/s] 72%|███████▏  | 417/582 [00:23<00:09, 17.93it/s] 75%|███████▍  | 435/582 [00:24<00:08, 17.94it/s] 78%|███████▊  | 453/582 [00:25<00:07, 17.95it/s] 81%|████████  | 471/582 [00:26<00:06, 17.60it/s] 84%|████████▍ | 489/582 [00:27<00:05, 17.72it/s] 87%|████████▋ | 507/582 [00:28<00:04, 17.80it/s] 90%|█████████ | 525/582 [00:29<00:03, 17.86it/s] 93%|█████████▎| 543/582 [00:30<00:02, 17.88it/s] 96%|█████████▋| 561/582 [00:31<00:01, 17.89it/s] 99%|█████████▉| 579/582 [00:32<00:00, 17.92it/s]100%|██████████| 582/582 [00:32<00:00, 18.03it/s]
172.16.0.62 - - [11/Sep/2023 20:36:14] "GET /load-model HTTP/1.1" 200 35

-> 主人： 谁是霄霄

-> 琉璃： 
172.16.0.62 - - [11/Sep/2023 20:37:27] "POST /inference HTTP/1.1" 200 694

-> 主人： 谁是丁婷？

-> 琉璃： 
172.16.0.62 - - [11/Sep/2023 20:37:38] "POST /inference HTTP/1.1" 200 690

-> 主人： 阿邦如何杀死丁婷的？

-> 琉璃： 
172.16.0.62 - - [11/Sep/2023 20:37:49] "POST /inference HTTP/1.1" 200 690

-> 主人： 继续

-> 琉璃： 
172.16.0.62 - - [11/Sep/2023 20:37:58] "POST /inference HTTP/1.1" 200 655

-> 主人： 

-> 琉璃： 
172.16.0.62 - - [11/Sep/2023 20:38:05] "POST /inference HTTP/1.1" 200 694

-> 主人： 

-> 琉璃： 
172.16.0.62 - - [11/Sep/2023 20:38:12] "POST /inference HTTP/1.1" 200 690
172.16.0.62 - - [11/Sep/2023 20:38:27] "GET /reset-state HTTP/1.1" 200 35
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:07<11:43,  7.10s/it]  2%|▏         | 2/100 [00:12<10:05,  6.18s/it]  3%|▎         | 3/100 [00:18<09:29,  5.87s/it][2023-09-11 20:39:06,318] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:39:06,319] [INFO] [timer.py:260:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=0.181118352344658, CurrSamplesPerSec=0.18219128872533114, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
  4%|▍         | 4/100 [00:23<09:09,  5.72s/it]  5%|▌         | 5/100 [00:29<08:55,  5.63s/it]  6%|▌         | 6/100 [00:34<08:44,  5.58s/it]  7%|▋         | 7/100 [00:40<08:36,  5.56s/it]  8%|▊         | 8/100 [00:45<08:29,  5.54s/it]  9%|▉         | 9/100 [00:51<08:22,  5.52s/it] 10%|█         | 10/100 [00:56<08:16,  5.52s/it] 11%|█         | 11/100 [01:02<08:10,  5.51s/it] 12%|█▏        | 12/100 [01:07<08:03,  5.50s/it] 13%|█▎        | 13/100 [01:13<07:58,  5.50s/it][2023-09-11 20:40:01,214] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:40:01,214] [INFO] [timer.py:260:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=0.18119281885397345, CurrSamplesPerSec=0.1821774561270386, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 14%|█▍        | 14/100 [01:18<07:52,  5.49s/it] 15%|█▌        | 15/100 [01:24<07:46,  5.49s/it] 16%|█▌        | 16/100 [01:29<07:44,  5.53s/it] 17%|█▋        | 17/100 [01:35<07:42,  5.57s/it] 18%|█▊        | 18/100 [01:40<07:37,  5.58s/it] 19%|█▉        | 19/100 [01:46<07:31,  5.57s/it] 20%|██        | 20/100 [01:51<07:23,  5.55s/it] 21%|██        | 21/100 [01:57<07:16,  5.53s/it] 22%|██▏       | 22/100 [02:02<07:10,  5.51s/it] 23%|██▎       | 23/100 [02:08<07:03,  5.50s/it][2023-09-11 20:40:56,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:40:56,581] [INFO] [timer.py:260:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=0.18115992500941716, CurrSamplesPerSec=0.1819077530846721, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 24%|██▍       | 24/100 [02:13<06:58,  5.50s/it] 25%|██▌       | 25/100 [02:19<06:52,  5.50s/it] 26%|██▌       | 26/100 [02:24<06:46,  5.50s/it] 27%|██▋       | 27/100 [02:30<06:40,  5.48s/it] 28%|██▊       | 28/100 [02:35<06:34,  5.48s/it] 29%|██▉       | 29/100 [02:41<06:29,  5.49s/it] 30%|███       | 30/100 [02:46<06:24,  5.49s/it] 31%|███       | 31/100 [02:52<06:21,  5.53s/it] 32%|███▏      | 32/100 [02:58<06:19,  5.58s/it] 33%|███▎      | 33/100 [03:03<06:16,  5.62s/it][2023-09-11 20:41:52,148] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:41:52,149] [INFO] [timer.py:260:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=0.18109137943244735, CurrSamplesPerSec=0.17835091618059218, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 34%|███▍      | 34/100 [03:09<06:10,  5.62s/it] 35%|███▌      | 35/100 [03:15<06:05,  5.62s/it] 36%|███▌      | 36/100 [03:20<05:57,  5.58s/it] 37%|███▋      | 37/100 [03:26<05:50,  5.56s/it] 38%|███▊      | 38/100 [03:31<05:43,  5.54s/it] 39%|███▉      | 39/100 [03:37<05:37,  5.54s/it] 40%|████      | 40/100 [03:42<05:31,  5.53s/it] 41%|████      | 41/100 [03:48<05:26,  5.54s/it] 42%|████▏     | 42/100 [03:53<05:20,  5.53s/it] 43%|████▎     | 43/100 [03:59<05:14,  5.52s/it][2023-09-11 20:42:47,403] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:42:47,404] [INFO] [timer.py:260:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=0.1810883562796325, CurrSamplesPerSec=0.18130091397307285, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 44%|████▍     | 44/100 [04:04<05:09,  5.52s/it] 45%|████▌     | 45/100 [04:10<05:05,  5.55s/it] 46%|████▌     | 46/100 [04:16<05:01,  5.58s/it] 47%|████▋     | 47/100 [04:21<04:57,  5.61s/it] 48%|████▊     | 48/100 [04:27<04:50,  5.59s/it] 49%|████▉     | 49/100 [04:32<04:44,  5.59s/it] 50%|█████     | 50/100 [04:38<04:38,  5.56s/it] 51%|█████     | 51/100 [04:43<04:31,  5.54s/it] 52%|█████▏    | 52/100 [04:49<04:25,  5.52s/it] 53%|█████▎    | 53/100 [04:54<04:19,  5.53s/it][2023-09-11 20:43:42,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:43:42,606] [INFO] [timer.py:260:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=0.18109499172234947, CurrSamplesPerSec=0.19530541056424178, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 54%|█████▍    | 54/100 [04:59<04:08,  5.40s/it] 55%|█████▌    | 55/100 [05:05<04:00,  5.35s/it] 56%|█████▌    | 56/100 [05:10<03:55,  5.34s/it] 57%|█████▋    | 57/100 [05:15<03:48,  5.32s/it] 58%|█████▊    | 58/100 [05:21<03:43,  5.32s/it] 59%|█████▉    | 59/100 [05:26<03:39,  5.35s/it] 60%|██████    | 60/100 [05:31<03:34,  5.35s/it] 61%|██████    | 61/100 [05:37<03:28,  5.36s/it] 62%|██████▏   | 62/100 [05:42<03:22,  5.32s/it] 63%|██████▎   | 63/100 [05:47<03:15,  5.29s/it][2023-09-11 20:44:35,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:44:35,523] [INFO] [timer.py:260:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=0.18148036501551407, CurrSamplesPerSec=0.1923796309780309, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 64%|██████▍   | 64/100 [05:52<03:09,  5.26s/it] 65%|██████▌   | 65/100 [05:58<03:03,  5.25s/it] 66%|██████▌   | 66/100 [06:03<02:58,  5.25s/it] 67%|██████▋   | 67/100 [06:08<02:53,  5.26s/it] 68%|██████▊   | 68/100 [06:13<02:48,  5.26s/it] 69%|██████▉   | 69/100 [06:19<02:42,  5.26s/it] 70%|███████   | 70/100 [06:24<02:37,  5.24s/it] 71%|███████   | 71/100 [06:29<02:32,  5.25s/it] 72%|███████▏  | 72/100 [06:35<02:30,  5.37s/it] 73%|███████▎  | 73/100 [06:40<02:24,  5.35s/it][2023-09-11 20:45:28,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:45:28,613] [INFO] [timer.py:260:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=0.18180261709271323, CurrSamplesPerSec=0.18586440158371226, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 74%|███████▍  | 74/100 [06:45<02:19,  5.36s/it] 75%|███████▌  | 75/100 [06:51<02:13,  5.35s/it] 76%|███████▌  | 76/100 [06:56<02:07,  5.33s/it] 77%|███████▋  | 77/100 [07:01<02:02,  5.32s/it] 78%|███████▊  | 78/100 [07:07<01:58,  5.40s/it] 79%|███████▉  | 79/100 [07:12<01:53,  5.42s/it] 80%|████████  | 80/100 [07:18<01:48,  5.44s/it] 81%|████████  | 81/100 [07:23<01:43,  5.46s/it] 82%|████████▏ | 82/100 [07:29<01:38,  5.46s/it] 83%|████████▎ | 83/100 [07:34<01:32,  5.46s/it][2023-09-11 20:46:22,965] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:46:22,966] [INFO] [timer.py:260:stop] epoch=0/micro_step=220/global_step=220, RunningAvgSamplesPerSec=0.18190442795549233, CurrSamplesPerSec=0.18250175232930677, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 84%|████████▍ | 84/100 [07:40<01:27,  5.47s/it] 85%|████████▌ | 85/100 [07:45<01:23,  5.53s/it] 86%|████████▌ | 86/100 [07:51<01:18,  5.59s/it] 87%|████████▋ | 87/100 [07:57<01:12,  5.61s/it] 88%|████████▊ | 88/100 [08:02<01:07,  5.60s/it] 89%|████████▉ | 89/100 [08:08<01:01,  5.58s/it] 90%|█████████ | 90/100 [08:13<00:55,  5.54s/it] 91%|█████████ | 91/100 [08:19<00:49,  5.52s/it] 92%|█████████▏| 92/100 [08:24<00:44,  5.50s/it] 93%|█████████▎| 93/100 [08:30<00:38,  5.49s/it][2023-09-11 20:47:18,480] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 20:47:18,481] [INFO] [timer.py:260:stop] epoch=0/micro_step=230/global_step=230, RunningAvgSamplesPerSec=0.18182863750794281, CurrSamplesPerSec=0.18257966277882298, MemAllocated=11.43GB, MaxMemAllocated=13.08GB
 94%|█████████▍| 94/100 [08:35<00:32,  5.49s/it] 95%|█████████▌| 95/100 [08:41<00:27,  5.49s/it] 96%|█████████▌| 96/100 [08:46<00:22,  5.50s/it] 97%|█████████▋| 97/100 [08:52<00:16,  5.49s/it] 98%|█████████▊| 98/100 [08:57<00:10,  5.48s/it] 99%|█████████▉| 99/100 [09:03<00:05,  5.47s/it]100%|██████████| 100/100 [09:08<00:00,  5.53s/it]100%|██████████| 100/100 [09:08<00:00,  5.49s/it]
172.16.0.62 - - [11/Sep/2023 20:47:52] "GET /hello/100 HTTP/1.1" 200 1955
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:00<00:26, 21.24it/s]  7%|▋         | 39/582 [00:01<00:27, 19.43it/s] 10%|▉         | 57/582 [00:02<00:27, 18.86it/s] 13%|█▎        | 75/582 [00:03<00:27, 18.60it/s] 16%|█▌        | 93/582 [00:04<00:26, 18.43it/s] 19%|█▉        | 111/582 [00:05<00:25, 18.36it/s] 22%|██▏       | 129/582 [00:06<00:24, 18.31it/s] 25%|██▌       | 147/582 [00:07<00:23, 18.28it/s] 28%|██▊       | 165/582 [00:08<00:22, 18.24it/s] 31%|███▏      | 183/582 [00:09<00:21, 18.24it/s] 35%|███▍      | 201/582 [00:10<00:20, 18.22it/s] 38%|███▊      | 219/582 [00:11<00:19, 18.20it/s] 41%|████      | 237/582 [00:12<00:18, 18.19it/s] 44%|████▍     | 255/582 [00:13<00:17, 18.19it/s] 47%|████▋     | 273/582 [00:14<00:17, 18.15it/s] 50%|█████     | 291/582 [00:15<00:16, 18.14it/s] 53%|█████▎    | 309/582 [00:16<00:15, 17.72it/s] 56%|█████▌    | 327/582 [00:17<00:14, 17.57it/s] 59%|█████▉    | 345/582 [00:19<00:13, 17.13it/s] 62%|██████▏   | 363/582 [00:20<00:13, 16.63it/s] 65%|██████▌   | 381/582 [00:21<00:11, 16.89it/s] 69%|██████▊   | 399/582 [00:22<00:10, 17.10it/s] 72%|███████▏  | 417/582 [00:23<00:09, 17.30it/s] 75%|███████▍  | 435/582 [00:24<00:08, 17.45it/s] 78%|███████▊  | 453/582 [00:25<00:07, 17.56it/s] 81%|████████  | 471/582 [00:26<00:06, 17.63it/s] 84%|████████▍ | 489/582 [00:27<00:05, 17.68it/s] 87%|████████▋ | 507/582 [00:28<00:04, 17.69it/s] 90%|█████████ | 525/582 [00:29<00:03, 17.72it/s] 93%|█████████▎| 543/582 [00:30<00:02, 17.73it/s] 96%|█████████▋| 561/582 [00:31<00:01, 17.73it/s] 99%|█████████▉| 579/582 [00:32<00:00, 17.65it/s]100%|██████████| 582/582 [00:32<00:00, 17.88it/s]100%|██████████| 582/582 [00:32<00:00, 17.87it/s]
172.16.0.62 - - [11/Sep/2023 20:48:41] "GET /load-model HTTP/1.1" 200 35

-> 主人： 介绍一下霄霄是谁？

-> 琉璃： 
172.16.0.62 - - [11/Sep/2023 20:50:11] "POST /inference HTTP/1.1" 200 628

-> 主人： 琉璃的尸体被阿邦看到

-> 琉璃： 
172.16.0.62 - - [11/Sep/2023 20:50:28] "POST /inference HTTP/1.1" 200 685

-> 主人： 继续

-> 琉璃： 
172.16.0.62 - - [11/Sep/2023 20:50:41] "POST /inference HTTP/1.1" 200 694

-> 主人： 继续

-> 琉璃： 
172.16.0.62 - - [11/Sep/2023 20:50:57] "POST /inference HTTP/1.1" 200 656
172.16.0.62 - - [11/Sep/2023 20:51:08] "GET /reset-state HTTP/1.1" 200 35

-> 主人： 你是谁?

-> 琉璃： 
172.16.0.62 - - [11/Sep/2023 20:52:20] "POST /inference HTTP/1.1" 200 690

-> 主人： 你和阿邦故事什么关系？

-> 琉璃： 
172.16.0.62 - - [11/Sep/2023 20:52:34] "POST /inference HTTP/1.1" 200 628
172.16.0.62 - - [11/Sep/2023 20:52:43] "GET /reset-state HTTP/1.1" 200 35

-> 主人： 你是谁？

-> 琉璃： 
172.16.0.62 - - [11/Sep/2023 20:59:25] "POST /inference HTTP/1.1" 200 690

-> 主题: 你是谁？



-> 回复者:琉璃 内容:


172.16.0.62 - - [11/Sep/2023 21:01:19] "POST /inference HTTP/1.1" 200 716

-> 主题: 霄霄的尸体是什么样顺？



-> 回复者:琉璃 内容:


172.16.0.62 - - [11/Sep/2023 21:01:41] "POST /inference HTTP/1.1" 200 716

-> 主题: +



-> 回复者:琉璃 内容:


172.16.0.62 - - [11/Sep/2023 21:01:55] "POST /inference HTTP/1.1" 200 716
172.16.0.62 - - [11/Sep/2023 21:04:23] "GET /reset-state HTTP/1.1" 200 35

-> 主题: 什么是人工智能？



-> 回复者:琉璃 内容:


172.16.0.62 - - [11/Sep/2023 21:04:43] "POST /inference HTTP/1.1" 200 708

-> 主题: fdsfd



-> 回复者:琉璃 内容:


172.16.0.62 - - [11/Sep/2023 21:05:01] "POST /inference HTTP/1.1" 200 710

-> 主题: 你好啊



-> 回复者:琉璃 内容:


172.16.0.62 - - [11/Sep/2023 21:06:08] "POST /inference HTTP/1.1" 200 724
172.16.0.62 - - [11/Sep/2023 21:06:26] "GET /reset-state HTTP/1.1" 200 35

-> 主题: 好啊



-> 回复者:琉璃 内容:


172.16.0.62 - - [11/Sep/2023 21:06:38] "POST /inference HTTP/1.1" 200 708

-> 主题: 林西子死了



-> 回复者:琉璃 内容:


172.16.0.62 - - [11/Sep/2023 21:07:04] "POST /inference HTTP/1.1" 200 708

-> 主题: 阿邦脱下林夕子的丝袜



-> 回复者:琉璃 内容:


172.16.0.62 - - [11/Sep/2023 21:07:17] "POST /inference HTTP/1.1" 200 724

-> 主题: 测试



-> 回复者:琉璃 内容:


172.16.0.62 - - [11/Sep/2023 21:07:47] "POST /inference HTTP/1.1" 200 708

-> 主题: 你好



-> 回复者:陈璇 内容:


172.16.0.62 - - [11/Sep/2023 21:08:03] "POST /inference HTTP/1.1" 200 708
172.16.0.62 - - [11/Sep/2023 21:08:11] "GET /reset-state HTTP/1.1" 200 35

-> 主题: 你好



-> 回复者:陈璇 内容:


172.16.0.62 - - [11/Sep/2023 21:08:19] "POST /inference HTTP/1.1" 200 708
172.16.0.62 - - [11/Sep/2023 21:09:06] "GET /reset-state HTTP/1.1" 200 35

-> 主题: 你是谁？



-> 回复者:陈璇 内容:


172.16.0.62 - - [11/Sep/2023 21:09:29] "POST /inference HTTP/1.1" 200 706
[2023-09-11 21:10:35,431] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 6485
[2023-09-11 21:10:37,083] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 6485
[2023-09-11 21:10:40,975] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-11 21:11:39,997] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-11 21:11:41,335] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-11 21:11:41,359] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed --deepspeed_config ds_config.config
[2023-09-11 21:11:43,207] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-11 21:11:44,503] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-11 21:11:44,503] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-11 21:11:44,503] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-11 21:11:44,503] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-11 21:11:44,504] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-11 21:11:46,315] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Traceback (most recent call last):
  File "/home/neromous/rwkv-trainer/train.py", line 14, in <module>
    from src.model_run import RWKV_RNN, sample_logits
ImportError: cannot import name 'sample_logits' from 'src.model_run' (/home/neromous/rwkv-trainer/src/model_run.py)
[2023-09-11 21:11:59,530] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 9695
[2023-09-11 21:11:59,531] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'train.py', '--local_rank=0', '--deepspeed', '--deepspeed_config', 'ds_config.config'] exits with return code = 1
[2023-09-11 21:12:35,181] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-11 21:12:36,512] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-11 21:12:36,536] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-11 21:12:38,383] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-11 21:12:39,686] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-11 21:12:39,686] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-11 21:12:39,686] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-11 21:12:39,686] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-11 21:12:39,686] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-11 21:12:41,537] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.7164928913116455 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_512_bf16/build.ninja...
Building extension module wkv_512_bf16...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512_bf16...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.40it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:04<00:00,  4.97s/it]100%|██████████| 1/1 [00:04<00:00,  4.97s/it]
[2023-09-11 21:13:28,884] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-11 21:13:28,885] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-11 21:13:28,885] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-11 21:13:31,287] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.1433913707733154 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-11 21:13:36,798] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-11 21:13:36,848] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-11 21:13:36,848] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-11 21:13:36,848] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-11 21:13:36,848] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-11 21:13:36,848] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-11 21:13:36,848] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-11 21:13:36,848] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-11 21:13:48,313] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-11 21:13:48,314] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 21:13:48,314] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 28.11 GB, percent = 7.4%
[2023-09-11 21:14:02,231] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-11 21:14:02,232] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 21:14:02,232] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 67.46 GB, percent = 17.9%
[2023-09-11 21:14:02,232] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-11 21:14:03,165] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-11 21:14:03,166] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 21:14:03,166] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 67.49 GB, percent = 17.9%
[2023-09-11 21:14:03,189] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-11 21:14:03,190] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-11 21:14:03,190] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f8ab5ae1610>
[2023-09-11 21:14:03,190] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:14:03,191] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-11 21:14:03,191] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-11 21:14:03,191] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-11 21:14:03,191] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-11 21:14:03,191] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-11 21:14:03,192] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-11 21:14:03,192] [INFO] [config.py:967:print]   bfloat16_enabled ............. auto
[2023-09-11 21:14:03,192] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-11 21:14:03,192] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-11 21:14:03,192] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-11 21:14:03,192] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8ab5ba3ee0>
[2023-09-11 21:14:03,192] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-11 21:14:03,192] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-11 21:14:03,192] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-11 21:14:03,192] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-11 21:14:03,192] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-11 21:14:03,192] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-11 21:14:03,192] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-11 21:14:03,192] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-11 21:14:03,192] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-11 21:14:03,192] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... None
[2023-09-11 21:14:03,192] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-11 21:14:03,192] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-11 21:14:03,192] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   fp16_auto_cast ............... None
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   fp16_enabled ................. False
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 1
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   loss_scale ................... 1.0
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-11 21:14:03,193] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-11 21:14:03,194] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-11 21:14:03,195] [INFO] [config.py:953:print_user_config]   json = {
    "bfloat16": {
        "enabled": "auto"
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

[2023-09-11 21:14:31,527] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-11 21:14:32,879] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-11 21:14:35,055] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-11 21:14:36,911] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-11 21:14:38,207] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-11 21:14:38,207] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-11 21:14:38,207] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-11 21:14:38,207] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-11 21:14:38,207] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-11 21:14:40,059] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.7028350830078125 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024_bf16/build.ninja...
Building extension module wkv_1024_bf16...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/3] c++ -MMD -MF wkv_op_bf16.o.d -DTORCH_EXTENSION_NAME=wkv_1024_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/TH -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/neromous/rwkv-trainer/cuda/wkv_op_bf16.cpp -o wkv_op_bf16.o 
[2/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_1024_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/TH -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=1024 -c /home/neromous/rwkv-trainer/cuda/wkv_cuda_bf16.cu -o wkv_cuda_bf16.cuda.o 
ptxas info    : 1 bytes gmem
ptxas info    : Compiling entry function '_Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S4_S4_PS2_S5_S5_S5_' for 'sm_86'
ptxas info    : Function properties for _Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S4_S4_PS2_S5_S5_S5_
    8192 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 48 registers, 448 bytes cmem[0], 16 bytes cmem[2]
ptxas info    : Compiling entry function '_Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_PS2_' for 'sm_86'
ptxas info    : Function properties for _Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_PS2_
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 39 registers, 408 bytes cmem[0]
[3/3] c++ wkv_op_bf16.o wkv_cuda_bf16.cuda.o -shared -L/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_1024_bf16.so
Loading extension module wkv_1024_bf16...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 10.68it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:04<00:00,  4.88s/it]100%|██████████| 1/1 [00:04<00:00,  4.88s/it]
[2023-09-11 21:15:47,312] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-11 21:15:47,312] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-11 21:15:47,312] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-11 21:15:49,787] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.221595048904419 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-11 21:15:55,425] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-11 21:15:55,476] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-11 21:15:55,476] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-11 21:15:55,476] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-11 21:15:55,476] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-11 21:15:55,476] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-11 21:15:55,476] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-11 21:15:55,476] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-11 21:16:07,713] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-11 21:16:07,714] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 21:16:07,714] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 28.41 GB, percent = 7.5%
[2023-09-11 21:16:22,027] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-11 21:16:22,028] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 21:16:22,028] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 67.59 GB, percent = 17.9%
[2023-09-11 21:16:22,028] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-11 21:16:22,980] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-11 21:16:22,981] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 21:16:22,981] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 67.59 GB, percent = 17.9%
[2023-09-11 21:16:23,005] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-11 21:16:23,005] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-11 21:16:23,006] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f8f23521610>
[2023-09-11 21:16:23,006] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:16:23,007] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-11 21:16:23,007] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-11 21:16:23,007] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-11 21:16:23,007] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-11 21:16:23,007] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-11 21:16:23,007] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   bfloat16_enabled ............. auto
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8f235e2ee0>
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... None
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   fp16_auto_cast ............... None
[2023-09-11 21:16:23,008] [INFO] [config.py:967:print]   fp16_enabled ................. False
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 1
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   loss_scale ................... 1.0
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-11 21:16:23,009] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-11 21:16:23,010] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-11 21:16:23,010] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-11 21:16:23,010] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-11 21:16:23,010] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-11 21:16:23,010] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-11 21:16:23,010] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-11 21:16:23,010] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-11 21:16:23,010] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-11 21:16:23,010] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-11 21:16:23,010] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-11 21:16:23,010] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-11 21:16:23,010] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-11 21:16:23,010] [INFO] [config.py:953:print_user_config]   json = {
    "bfloat16": {
        "enabled": "auto"
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

  0%|          | 0/100 [00:00<?, ?it/s][2023-09-11 21:16:40,587] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-09-11 21:16:40,587] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-11 21:16:40,588] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-11 21:16:40,588] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-09-11 21:16:40,588] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
  1%|          | 1/100 [00:08<14:45,  8.94s/it]  2%|▏         | 2/100 [00:15<12:14,  7.49s/it]  3%|▎         | 3/100 [00:21<11:08,  6.89s/it]  4%|▍         | 4/100 [00:27<10:25,  6.51s/it]  5%|▌         | 5/100 [00:33<09:59,  6.31s/it]  6%|▌         | 6/100 [00:39<09:40,  6.18s/it]  7%|▋         | 7/100 [00:45<09:26,  6.09s/it]  8%|▊         | 8/100 [00:51<09:22,  6.12s/it]  9%|▉         | 9/100 [00:57<09:06,  6.01s/it][2023-09-11 21:17:43,524] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:17:43,525] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.16835244816123127, CurrSamplesPerSec=0.1750720846923498, MemAllocated=6.03GB, MaxMemAllocated=7.76GB
 10%|█         | 10/100 [01:02<08:52,  5.92s/it] 11%|█         | 11/100 [01:08<08:39,  5.84s/it] 12%|█▏        | 12/100 [01:14<08:29,  5.79s/it] 13%|█▎        | 13/100 [01:20<08:24,  5.79s/it] 14%|█▍        | 14/100 [01:25<08:12,  5.73s/it] 15%|█▌        | 15/100 [01:31<08:02,  5.68s/it] 16%|█▌        | 16/100 [01:36<07:55,  5.66s/it] 17%|█▋        | 17/100 [01:42<07:47,  5.63s/it] 18%|█▊        | 18/100 [01:48<07:44,  5.66s/it] 19%|█▉        | 19/100 [01:53<07:40,  5.68s/it][2023-09-11 21:18:39,794] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:18:39,795] [INFO] [timer.py:260:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=0.17347215709554303, CurrSamplesPerSec=0.187559851918054, MemAllocated=6.03GB, MaxMemAllocated=7.76GB
 20%|██        | 20/100 [01:59<07:26,  5.58s/it] 21%|██        | 21/100 [02:04<07:14,  5.50s/it] 22%|██▏       | 22/100 [02:09<07:05,  5.46s/it] 23%|██▎       | 23/100 [02:15<06:58,  5.44s/it] 24%|██▍       | 24/100 [02:20<06:52,  5.43s/it] 25%|██▌       | 25/100 [02:26<06:49,  5.46s/it] 26%|██▌       | 26/100 [02:31<06:45,  5.47s/it] 27%|██▋       | 27/100 [02:37<06:40,  5.49s/it] 28%|██▊       | 28/100 [02:42<06:35,  5.49s/it] 29%|██▉       | 29/100 [02:48<06:30,  5.50s/it][2023-09-11 21:19:34,397] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:19:34,397] [INFO] [timer.py:260:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=0.1768368207360337, CurrSamplesPerSec=0.18032554441340448, MemAllocated=6.03GB, MaxMemAllocated=7.76GB
 30%|███       | 30/100 [02:53<06:25,  5.51s/it] 31%|███       | 31/100 [02:59<06:20,  5.52s/it] 32%|███▏      | 32/100 [03:04<06:16,  5.54s/it] 33%|███▎      | 33/100 [03:10<06:17,  5.64s/it] 34%|███▍      | 34/100 [03:16<06:17,  5.72s/it] 35%|███▌      | 35/100 [03:22<06:08,  5.67s/it] 36%|███▌      | 36/100 [03:27<06:01,  5.65s/it] 37%|███▋      | 37/100 [03:33<05:53,  5.62s/it] 38%|███▊      | 38/100 [03:38<05:47,  5.60s/it] 39%|███▉      | 39/100 [03:44<05:40,  5.59s/it][2023-09-11 21:20:30,679] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:20:30,679] [INFO] [timer.py:260:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=0.17707997600769787, CurrSamplesPerSec=0.179832103697893, MemAllocated=6.03GB, MaxMemAllocated=7.76GB
 40%|████      | 40/100 [03:50<05:34,  5.58s/it] 41%|████      | 41/100 [03:55<05:29,  5.58s/it] 42%|████▏     | 42/100 [04:01<05:28,  5.67s/it] 43%|████▎     | 43/100 [04:07<05:24,  5.69s/it] 44%|████▍     | 44/100 [04:12<05:17,  5.67s/it] 45%|████▌     | 45/100 [04:18<05:09,  5.63s/it] 46%|████▌     | 46/100 [04:23<05:01,  5.59s/it] 47%|████▋     | 47/100 [04:29<04:55,  5.58s/it] 48%|████▊     | 48/100 [04:35<04:49,  5.56s/it] 49%|████▉     | 49/100 [04:40<04:43,  5.55s/it][2023-09-11 21:21:26,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:21:26,659] [INFO] [timer.py:260:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=0.1774201198962069, CurrSamplesPerSec=0.1809611516130111, MemAllocated=6.03GB, MaxMemAllocated=7.76GB
 50%|█████     | 50/100 [04:46<04:37,  5.55s/it] 51%|█████     | 51/100 [04:51<04:31,  5.54s/it] 52%|█████▏    | 52/100 [04:57<04:29,  5.61s/it] 53%|█████▎    | 53/100 [05:03<04:25,  5.65s/it] 54%|█████▍    | 54/100 [05:08<04:19,  5.64s/it] 55%|█████▌    | 55/100 [05:14<04:11,  5.59s/it] 56%|█████▌    | 56/100 [05:19<04:03,  5.54s/it] 57%|█████▋    | 57/100 [05:25<03:56,  5.51s/it] 58%|█████▊    | 58/100 [05:30<03:50,  5.48s/it] 59%|█████▉    | 59/100 [05:35<03:44,  5.47s/it][2023-09-11 21:22:21,921] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:22:21,922] [INFO] [timer.py:260:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=0.1780345301373334, CurrSamplesPerSec=0.18507015675859304, MemAllocated=6.03GB, MaxMemAllocated=7.76GB
 60%|██████    | 60/100 [05:41<03:37,  5.45s/it] 61%|██████    | 61/100 [05:46<03:32,  5.44s/it] 62%|██████▏   | 62/100 [05:52<03:25,  5.42s/it] 63%|██████▎   | 63/100 [05:57<03:19,  5.40s/it] 64%|██████▍   | 64/100 [06:02<03:15,  5.43s/it] 65%|██████▌   | 65/100 [06:08<03:11,  5.48s/it] 66%|██████▌   | 66/100 [06:14<03:07,  5.50s/it] 67%|██████▋   | 67/100 [06:19<02:58,  5.40s/it] 68%|██████▊   | 68/100 [06:24<02:52,  5.38s/it] 69%|██████▉   | 69/100 [06:29<02:46,  5.37s/it][2023-09-11 21:23:15,872] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:23:15,873] [INFO] [timer.py:260:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=0.17908723391686612, CurrSamplesPerSec=0.1875754199558, MemAllocated=6.03GB, MaxMemAllocated=7.76GB
 70%|███████   | 70/100 [06:35<02:40,  5.36s/it] 71%|███████   | 71/100 [06:40<02:35,  5.35s/it] 72%|███████▏  | 72/100 [06:45<02:29,  5.34s/it] 73%|███████▎  | 73/100 [06:51<02:24,  5.34s/it] 74%|███████▍  | 74/100 [06:56<02:18,  5.34s/it] 75%|███████▌  | 75/100 [07:01<02:13,  5.34s/it] 76%|███████▌  | 76/100 [07:07<02:08,  5.34s/it] 77%|███████▋  | 77/100 [07:12<02:04,  5.41s/it] 78%|███████▊  | 78/100 [07:18<01:59,  5.44s/it] 79%|███████▉  | 79/100 [07:23<01:54,  5.46s/it][2023-09-11 21:24:09,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:24:09,821] [INFO] [timer.py:260:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=0.1798787053129221, CurrSamplesPerSec=0.18682660549437088, MemAllocated=6.03GB, MaxMemAllocated=7.76GB
 80%|████████  | 80/100 [07:29<01:48,  5.43s/it] 81%|████████  | 81/100 [07:34<01:42,  5.40s/it] 82%|████████▏ | 82/100 [07:39<01:37,  5.39s/it] 83%|████████▎ | 83/100 [07:45<01:31,  5.38s/it] 84%|████████▍ | 84/100 [07:50<01:25,  5.37s/it] 85%|████████▌ | 85/100 [07:56<01:20,  5.37s/it] 86%|████████▌ | 86/100 [08:01<01:15,  5.36s/it] 87%|████████▋ | 87/100 [08:06<01:09,  5.36s/it] 88%|████████▊ | 88/100 [08:12<01:04,  5.37s/it] 89%|████████▉ | 89/100 [08:17<00:58,  5.36s/it][2023-09-11 21:25:03,390] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:25:03,391] [INFO] [timer.py:260:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=0.18063577989932958, CurrSamplesPerSec=0.1867240207092826, MemAllocated=6.03GB, MaxMemAllocated=7.76GB
 90%|█████████ | 90/100 [08:22<00:53,  5.36s/it] 91%|█████████ | 91/100 [08:28<00:48,  5.40s/it] 92%|█████████▏| 92/100 [08:33<00:43,  5.45s/it] 93%|█████████▎| 93/100 [08:39<00:38,  5.47s/it] 94%|█████████▍| 94/100 [08:44<00:32,  5.46s/it] 95%|█████████▌| 95/100 [08:50<00:27,  5.44s/it] 96%|█████████▌| 96/100 [08:55<00:21,  5.41s/it] 97%|█████████▋| 97/100 [09:00<00:16,  5.39s/it] 98%|█████████▊| 98/100 [09:06<00:10,  5.38s/it] 99%|█████████▉| 99/100 [09:11<00:05,  5.37s/it][2023-09-11 21:25:57,555] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:25:57,555] [INFO] [timer.py:260:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=0.18104367388405346, CurrSamplesPerSec=0.18696309952901552, MemAllocated=6.03GB, MaxMemAllocated=7.76GB
100%|██████████| 100/100 [09:16<00:00,  5.37s/it]100%|██████████| 100/100 [09:16<00:00,  5.57s/it]
172.16.0.62 - - [11/Sep/2023 21:25:59] "GET /hello/100 HTTP/1.1" 200 1963
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:01<00:36, 15.55it/s]  7%|▋         | 39/582 [00:02<00:38, 14.25it/s] 10%|▉         | 57/582 [00:04<00:37, 13.85it/s] 13%|█▎        | 75/582 [00:05<00:37, 13.64it/s] 16%|█▌        | 93/582 [00:06<00:36, 13.53it/s] 19%|█▉        | 111/582 [00:08<00:34, 13.46it/s] 22%|██▏       | 129/582 [00:09<00:33, 13.42it/s] 25%|██▌       | 147/582 [00:10<00:32, 13.39it/s] 28%|██▊       | 165/582 [00:12<00:31, 13.38it/s] 31%|███▏      | 183/582 [00:13<00:29, 13.37it/s] 35%|███▍      | 201/582 [00:14<00:28, 13.37it/s] 38%|███▊      | 219/582 [00:16<00:27, 13.37it/s] 41%|████      | 237/582 [00:17<00:25, 13.37it/s] 44%|████▍     | 255/582 [00:18<00:24, 13.35it/s] 47%|████▋     | 273/582 [00:20<00:23, 13.35it/s] 50%|█████     | 291/582 [00:21<00:21, 13.36it/s] 53%|█████▎    | 309/582 [00:22<00:20, 13.35it/s] 56%|█████▌    | 327/582 [00:24<00:19, 13.35it/s] 59%|█████▉    | 345/582 [00:25<00:17, 13.42it/s] 62%|██████▏   | 363/582 [00:26<00:16, 13.40it/s] 65%|██████▌   | 381/582 [00:28<00:15, 13.38it/s] 69%|██████▊   | 399/582 [00:29<00:13, 13.37it/s] 72%|███████▏  | 417/582 [00:31<00:12, 13.35it/s] 75%|███████▍  | 435/582 [00:32<00:11, 13.34it/s] 78%|███████▊  | 453/582 [00:33<00:09, 13.35it/s] 81%|████████  | 471/582 [00:35<00:08, 13.35it/s] 84%|████████▍ | 489/582 [00:36<00:06, 13.35it/s] 87%|████████▋ | 507/582 [00:37<00:05, 13.35it/s] 90%|█████████ | 525/582 [00:39<00:04, 13.36it/s] 93%|█████████▎| 543/582 [00:40<00:02, 13.36it/s] 96%|█████████▋| 561/582 [00:41<00:01, 13.36it/s] 99%|█████████▉| 579/582 [00:43<00:00, 13.37it/s]100%|██████████| 582/582 [00:43<00:00, 13.48it/s]

-> 主题: 谁是霄霄？



-> 回复者:陈璇 内容:


172.16.0.62 - - [11/Sep/2023 21:33:24] "POST /inference HTTP/1.1" 200 716

-> 主题: 你是谁？



-> 回复者:陈璇 内容:


172.16.0.62 - - [11/Sep/2023 21:35:57] "POST /inference HTTP/1.1" 200 720

-> 主题: 你认识霄霄吗？



-> 回复者:陈璇 内容:


172.16.0.62 - - [11/Sep/2023 21:36:11] "POST /inference HTTP/1.1" 200 720

-> 主题: 、你认识陈璇吗？



-> 回复者:陈璇 内容:


172.16.0.62 - - [11/Sep/2023 21:36:21] "POST /inference HTTP/1.1" 200 720
172.16.0.62 - - [11/Sep/2023 21:36:41] "GET /reset-state HTTP/1.1" 200 35

-> User: 你好



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:38:21] "POST /inference HTTP/1.1" 200 5891

-> User: 谁是阿邦？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:39:13] "POST /inference HTTP/1.1" 200 5710
172.16.0.62 - - [11/Sep/2023 21:40:06] "GET /reset-state HTTP/1.1" 200 35
172.16.0.62 - - [11/Sep/2023 21:40:19] "GET /reset-state HTTP/1.1" 200 35

-> User: 你好



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:40:38] "POST /inference HTTP/1.1" 200 1524
172.16.0.62 - - [11/Sep/2023 21:41:38] "GET /reset-state HTTP/1.1" 200 35

-> User: 你好



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:41:42] "POST /inference HTTP/1.1" 200 144

-> User: 你认识霄霄吗？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:41:50] "POST /inference HTTP/1.1" 200 138

-> User: 你认识阿邦吗？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:41:56] "POST /inference HTTP/1.1" 200 162

-> User: 我是陈璇



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:42:02] "POST /inference HTTP/1.1" 200 156

-> User: 你是霄霄吗？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:42:09] "POST /inference HTTP/1.1" 200 180

-> User: 我是阿邦



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:42:16] "POST /inference HTTP/1.1" 200 156

-> User: 我要脱下你的丝袜



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:42:23] "POST /inference HTTP/1.1" 200 126

-> User: 强奸你



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:42:28] "POST /inference HTTP/1.1" 200 126

-> User: 强奸你



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:42:33] "POST /inference HTTP/1.1" 200 126
172.16.0.62 - - [11/Sep/2023 21:42:37] "GET /reset-state HTTP/1.1" 200 35

-> User: 什么是奶油？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:42:58] "POST /inference HTTP/1.1" 200 702

-> User: 介绍刚刚你阅读的剧情



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:43:15] "POST /inference HTTP/1.1" 200 1618

-> User: 继续刚才的剧情



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:43:36] "POST /inference HTTP/1.1" 200 1608

-> User: 继续



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:43:55] "POST /inference HTTP/1.1" 200 1606

-> User: 继续



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:44:08] "POST /inference HTTP/1.1" 200 1610

-> User: 这时霄霄出现了



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:44:25] "POST /inference HTTP/1.1" 200 1286

-> User: 阿邦的肉棒对丁婷产生了反应



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:44:50] "POST /inference HTTP/1.1" 200 1510

-> User: 阿邦走了出去



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:45:19] "POST /inference HTTP/1.1" 200 1571

-> User: 陈璇



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:46:15] "POST /inference HTTP/1.1" 200 1590

-> User: 陈璇的艳尸



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:46:36] "POST /inference HTTP/1.1" 200 1606
172.16.0.62 - - [11/Sep/2023 21:46:43] "GET /reset-state HTTP/1.1" 200 35

-> User: 我们刚才讨论了什么？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:46:58] "POST /inference HTTP/1.1" 200 234

-> User: 你叫什么名字？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:47:07] "POST /inference HTTP/1.1" 200 186

-> User: 你和谁见过面？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:47:16] "POST /inference HTTP/1.1" 200 126

-> User: 你和霄霄是什么关系？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:47:28] "POST /inference HTTP/1.1" 200 174

-> User: 霄霄死了吗？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:47:35] "POST /inference HTTP/1.1" 200 138

-> User: 你会杀死她吗？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:47:42] "POST /inference HTTP/1.1" 200 150

-> User: 你会杀死她，并且奸她的尸体， 抚摸她的乳房，



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:48:03] "POST /inference HTTP/1.1" 200 150
172.16.0.62 - - [11/Sep/2023 21:48:23] "GET /reset-state HTTP/1.1" 200 35
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:07<12:06,  7.33s/it]  2%|▏         | 2/100 [00:13<10:26,  6.39s/it]  3%|▎         | 3/100 [00:18<09:42,  6.01s/it]  4%|▍         | 4/100 [00:24<09:19,  5.83s/it]  5%|▌         | 5/100 [00:29<09:04,  5.73s/it]  6%|▌         | 6/100 [00:35<08:56,  5.71s/it]  7%|▋         | 7/100 [00:40<08:46,  5.66s/it]  8%|▊         | 8/100 [00:46<08:38,  5.64s/it]  9%|▉         | 9/100 [00:52<08:30,  5.61s/it][2023-09-11 21:49:27,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:49:27,606] [INFO] [timer.py:260:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=0.18031743413160337, CurrSamplesPerSec=0.17962766492095564, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
 10%|█         | 10/100 [00:57<08:24,  5.60s/it] 11%|█         | 11/100 [01:03<08:21,  5.64s/it] 12%|█▏        | 12/100 [01:09<08:17,  5.66s/it] 13%|█▎        | 13/100 [01:14<08:13,  5.67s/it] 14%|█▍        | 14/100 [01:20<08:03,  5.62s/it] 15%|█▌        | 15/100 [01:25<07:52,  5.56s/it] 16%|█▌        | 16/100 [01:31<07:41,  5.49s/it] 17%|█▋        | 17/100 [01:36<07:32,  5.45s/it] 18%|█▊        | 18/100 [01:41<07:26,  5.44s/it] 19%|█▉        | 19/100 [01:47<07:19,  5.43s/it][2023-09-11 21:50:22,471] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:50:22,471] [INFO] [timer.py:260:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=0.18048814164088753, CurrSamplesPerSec=0.18790795560926868, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
 20%|██        | 20/100 [01:52<07:11,  5.40s/it] 21%|██        | 21/100 [01:57<07:05,  5.39s/it] 22%|██▏       | 22/100 [02:03<06:59,  5.38s/it] 23%|██▎       | 23/100 [02:08<06:51,  5.35s/it] 24%|██▍       | 24/100 [02:14<06:52,  5.43s/it] 25%|██▌       | 25/100 [02:19<06:47,  5.44s/it] 26%|██▌       | 26/100 [02:25<06:41,  5.43s/it] 27%|██▋       | 27/100 [02:30<06:37,  5.44s/it] 28%|██▊       | 28/100 [02:35<06:28,  5.39s/it] 29%|██▉       | 29/100 [02:40<06:19,  5.34s/it][2023-09-11 21:51:16,181] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:51:16,182] [INFO] [timer.py:260:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=0.18092751983142477, CurrSamplesPerSec=0.19013213225437617, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
 30%|███       | 30/100 [02:46<06:12,  5.32s/it] 31%|███       | 31/100 [02:51<06:04,  5.29s/it] 32%|███▏      | 32/100 [02:56<05:58,  5.27s/it] 33%|███▎      | 33/100 [03:01<05:53,  5.27s/it] 34%|███▍      | 34/100 [03:07<05:47,  5.27s/it] 35%|███▌      | 35/100 [03:12<05:41,  5.26s/it] 36%|███▌      | 36/100 [03:17<05:36,  5.26s/it] 37%|███▋      | 37/100 [03:22<05:29,  5.23s/it] 38%|███▊      | 38/100 [03:28<05:24,  5.24s/it] 39%|███▉      | 39/100 [03:33<05:18,  5.22s/it][2023-09-11 21:52:08,627] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:52:08,628] [INFO] [timer.py:260:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=0.18160660580954277, CurrSamplesPerSec=0.18662051888490352, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
 40%|████      | 40/100 [03:38<05:15,  5.26s/it] 41%|████      | 41/100 [03:44<05:12,  5.30s/it] 42%|████▏     | 42/100 [03:49<05:07,  5.31s/it] 43%|████▎     | 43/100 [03:54<05:03,  5.32s/it] 44%|████▍     | 44/100 [03:59<04:55,  5.28s/it] 45%|████▌     | 45/100 [04:05<04:48,  5.24s/it] 46%|████▌     | 46/100 [04:10<04:42,  5.22s/it] 47%|████▋     | 47/100 [04:15<04:37,  5.23s/it] 48%|████▊     | 48/100 [04:20<04:30,  5.21s/it] 49%|████▉     | 49/100 [04:25<04:25,  5.20s/it][2023-09-11 21:53:00,956] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:53:00,956] [INFO] [timer.py:260:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=0.182224391512018, CurrSamplesPerSec=0.19383936807585161, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
 50%|█████     | 50/100 [04:31<04:19,  5.19s/it] 51%|█████     | 51/100 [04:36<04:14,  5.19s/it] 52%|█████▏    | 52/100 [04:41<04:08,  5.18s/it] 53%|█████▎    | 53/100 [04:46<04:03,  5.18s/it] 54%|█████▍    | 54/100 [04:51<03:58,  5.18s/it] 55%|█████▌    | 55/100 [04:56<03:52,  5.17s/it] 56%|█████▌    | 56/100 [05:02<03:47,  5.17s/it] 57%|█████▋    | 57/100 [05:07<03:45,  5.25s/it] 58%|█████▊    | 58/100 [05:12<03:41,  5.27s/it] 59%|█████▉    | 59/100 [05:18<03:36,  5.28s/it][2023-09-11 21:53:53,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:53:53,363] [INFO] [timer.py:260:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=0.18275082856662553, CurrSamplesPerSec=0.1883060792685095, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
 60%|██████    | 60/100 [05:23<03:31,  5.29s/it] 61%|██████    | 61/100 [05:28<03:25,  5.28s/it] 62%|██████▏   | 62/100 [05:33<03:18,  5.24s/it] 63%|██████▎   | 63/100 [05:38<03:12,  5.21s/it] 64%|██████▍   | 64/100 [05:44<03:06,  5.18s/it] 65%|██████▌   | 65/100 [05:49<03:00,  5.16s/it] 66%|██████▌   | 66/100 [05:54<02:55,  5.17s/it] 67%|██████▋   | 67/100 [05:59<02:50,  5.18s/it] 68%|██████▊   | 68/100 [06:04<02:45,  5.16s/it] 69%|██████▉   | 69/100 [06:09<02:40,  5.17s/it][2023-09-11 21:54:44,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:54:44,965] [INFO] [timer.py:260:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=0.1833780733170931, CurrSamplesPerSec=0.19548384874673594, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
 70%|███████   | 70/100 [06:15<02:34,  5.16s/it] 71%|███████   | 71/100 [06:20<02:29,  5.15s/it] 72%|███████▏  | 72/100 [06:25<02:24,  5.14s/it] 73%|███████▎  | 73/100 [06:30<02:18,  5.14s/it] 74%|███████▍  | 74/100 [06:35<02:14,  5.16s/it] 75%|███████▌  | 75/100 [06:40<02:09,  5.18s/it] 76%|███████▌  | 76/100 [06:46<02:05,  5.21s/it] 77%|███████▋  | 77/100 [06:51<01:59,  5.21s/it] 78%|███████▊  | 78/100 [06:56<01:55,  5.23s/it] 79%|███████▉  | 79/100 [07:01<01:49,  5.22s/it][2023-09-11 21:55:36,966] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:55:36,966] [INFO] [timer.py:260:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=0.1838625012201558, CurrSamplesPerSec=0.19249539896369347, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
 80%|████████  | 80/100 [07:07<01:44,  5.22s/it] 81%|████████  | 81/100 [07:12<01:38,  5.19s/it] 82%|████████▏ | 82/100 [07:17<01:33,  5.17s/it] 83%|████████▎ | 83/100 [07:22<01:27,  5.15s/it] 84%|████████▍ | 84/100 [07:27<01:22,  5.14s/it] 85%|████████▌ | 85/100 [07:32<01:17,  5.14s/it] 86%|████████▌ | 86/100 [07:37<01:11,  5.13s/it] 87%|████████▋ | 87/100 [07:42<01:06,  5.13s/it] 88%|████████▊ | 88/100 [07:48<01:01,  5.15s/it] 89%|████████▉ | 89/100 [07:53<00:56,  5.12s/it][2023-09-11 21:56:28,193] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:56:28,194] [INFO] [timer.py:260:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=0.18443746584812507, CurrSamplesPerSec=0.19495713043532018, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
 90%|█████████ | 90/100 [07:58<00:51,  5.13s/it] 91%|█████████ | 91/100 [08:03<00:46,  5.13s/it] 92%|█████████▏| 92/100 [08:08<00:41,  5.13s/it] 93%|█████████▎| 93/100 [08:13<00:35,  5.12s/it] 94%|█████████▍| 94/100 [08:18<00:30,  5.16s/it] 95%|█████████▌| 95/100 [08:24<00:25,  5.20s/it] 96%|█████████▌| 96/100 [08:29<00:20,  5.22s/it] 97%|█████████▋| 97/100 [08:34<00:15,  5.22s/it] 98%|█████████▊| 98/100 [08:40<00:10,  5.29s/it] 99%|█████████▉| 99/100 [08:45<00:05,  5.24s/it][2023-09-11 21:57:20,285] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:57:20,286] [INFO] [timer.py:260:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=0.18480891050430898, CurrSamplesPerSec=0.19557895825521895, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
100%|██████████| 100/100 [08:50<00:00,  5.20s/it]100%|██████████| 100/100 [08:50<00:00,  5.30s/it]
172.16.0.62 - - [11/Sep/2023 21:57:21] "GET /hello/100 HTTP/1.1" 200 1953
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:05<00:47,  5.25s/it] 20%|██        | 2/10 [00:10<00:41,  5.23s/it] 30%|███       | 3/10 [00:15<00:36,  5.19s/it] 40%|████      | 4/10 [00:20<00:31,  5.18s/it] 50%|█████     | 5/10 [00:25<00:25,  5.17s/it] 60%|██████    | 6/10 [00:31<00:20,  5.16s/it] 70%|███████   | 7/10 [00:36<00:15,  5.15s/it] 80%|████████  | 8/10 [00:41<00:10,  5.15s/it] 90%|█████████ | 9/10 [00:46<00:05,  5.18s/it][2023-09-11 21:58:23,553] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 21:58:23,553] [INFO] [timer.py:260:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=0.1852179749912713, CurrSamplesPerSec=0.1972695341357147, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
100%|██████████| 10/10 [00:51<00:00,  5.15s/it]100%|██████████| 10/10 [00:51<00:00,  5.17s/it]
172.16.0.62 - - [11/Sep/2023 21:58:24] "GET /hello/10 HTTP/1.1" 200 204

-> User: 刚才的场景中出现了什么？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:58:42] "POST /inference HTTP/1.1" 200 324

-> User: 刚才的场景中有几个出场角色？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:58:54] "POST /inference HTTP/1.1" 200 324

-> User: 刚才的场景中有没有女性出现？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:59:04] "POST /inference HTTP/1.1" 200 180

-> User: 场景中有那些男性？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:59:13] "POST /inference HTTP/1.1" 200 180

-> User: 牛奶好喝吗？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:59:29] "POST /inference HTTP/1.1" 200 192

-> User: 场景中男性的名字分别为？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:59:42] "POST /inference HTTP/1.1" 200 180

-> User: 丁婷做了什么？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 21:59:55] "POST /inference HTTP/1.1" 200 642

-> User: 林慕容做了什么？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:00:05] "POST /inference HTTP/1.1" 200 600

-> User: 阿邦与陈璇之间的战斗结果如何？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:00:23] "POST /inference HTTP/1.1" 200 1208

-> User: 阿邦与霄霄之间的战斗结果是什



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:00:52] "POST /inference HTTP/1.1" 200 1618

-> User: 阿邦擅长使用什么？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:01:53] "POST /inference HTTP/1.1" 200 1246
[2023-09-11 22:03:53,915] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 11437
[2023-09-11 22:03:55,481] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 11437
[2023-09-11 22:03:59,746] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-11 22:04:11,338] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-11 22:04:12,677] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-11 22:04:12,701] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-11 22:04:14,524] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-11 22:04:15,836] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-11 22:04:15,837] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-11 22:04:15,837] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-11 22:04:15,837] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-11 22:04:15,837] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-11 22:04:17,648] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.735217332839966 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Creating extension directory /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_1024 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/TH -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=1024 -std=c++17 -c /home/neromous/rwkv-trainer/cuda/wkv_cuda.cu -o wkv_cuda.cuda.o 
ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function '_Z15kernel_backwardIfEviiiPKT_S2_S2_S2_S2_S2_PS0_S3_S3_S3_' for 'sm_86'
ptxas info    : Function properties for _Z15kernel_backwardIfEviiiPKT_S2_S2_S2_S2_S2_PS0_S3_S3_S3_
    8192 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 56 registers, 448 bytes cmem[0], 16 bytes cmem[2]
ptxas info    : Compiling entry function '_Z14kernel_forwardIfEviiiPKT_S2_S2_S2_PS0_' for 'sm_86'
ptxas info    : Function properties for _Z14kernel_forwardIfEviiiPKT_S2_S2_S2_PS0_
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, 408 bytes cmem[0]
[2/3] c++ -MMD -MF wkv_op.o.d -DTORCH_EXTENSION_NAME=wkv_1024 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/TH -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/neromous/rwkv-trainer/cuda/wkv_op.cpp -o wkv_op.o 
[3/3] c++ wkv_op.o wkv_cuda.cuda.o -shared -L/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_1024.so
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 10.97it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:04<00:00,  5.00s/it]100%|██████████| 1/1 [00:05<00:00,  5.00s/it]
[2023-09-11 22:05:20,155] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-11 22:05:20,155] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-11 22:05:20,156] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-11 22:05:22,962] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-11 22:05:26,212] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 13254
[2023-09-11 22:05:26,822] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-11 22:05:33,223] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-11 22:05:34,565] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-11 22:05:34,590] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-11 22:05:36,435] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-11 22:05:37,751] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-11 22:05:37,751] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-11 22:05:37,751] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-11 22:05:37,751] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-11 22:05:37,751] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-11 22:05:39,585] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.723931074142456 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.12it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.01s/it]100%|██████████| 1/1 [00:05<00:00,  5.01s/it]
[2023-09-11 22:06:26,856] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-11 22:06:26,857] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-11 22:06:26,857] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-11 22:06:29,191] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.1850879192352295 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-11 22:06:34,763] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-11 22:06:34,811] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-11 22:06:34,811] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-11 22:06:34,811] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-11 22:06:34,811] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-11 22:06:34,811] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-11 22:06:34,811] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-11 22:06:34,811] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-11 22:06:46,347] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-11 22:06:46,348] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 22:06:46,349] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 28.37 GB, percent = 7.5%
[2023-09-11 22:06:59,817] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-11 22:06:59,818] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 22:06:59,818] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 67.58 GB, percent = 17.9%
[2023-09-11 22:06:59,818] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-11 22:07:00,764] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-11 22:07:00,765] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 22:07:00,765] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 67.58 GB, percent = 17.9%
[2023-09-11 22:07:00,788] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-11 22:07:00,788] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-11 22:07:00,788] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f5dd0160610>
[2023-09-11 22:07:00,788] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 22:07:00,790] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-11 22:07:00,790] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-11 22:07:00,790] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-11 22:07:00,790] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-11 22:07:00,790] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-11 22:07:00,790] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5dd01c4bb0>
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-11 22:07:00,791] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-11 22:07:00,792] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-11 22:07:00,793] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-11 22:07:00,793] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-11 22:07:00,793] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-11 22:07:00,793] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-11 22:07:00,793] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-11 22:07:00,793] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-11 22:07:00,793] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-11 22:07:00,793] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-11 22:07:00,793] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-11 22:07:00,793] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-11 22:07:00,793] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-11 22:07:00,793] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-11 22:07:00,793] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

  0%|          | 0/30 [00:00<?, ?it/s][2023-09-11 22:08:32,502] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-09-11 22:08:32,502] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-11 22:08:32,502] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-11 22:08:32,502] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-09-11 22:08:32,502] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
[2023-09-11 22:08:36,256] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
  3%|▎         | 1/30 [00:03<01:49,  3.78s/it][2023-09-11 22:08:37,744] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
  7%|▋         | 2/30 [00:05<01:08,  2.43s/it] 10%|█         | 3/30 [00:10<01:44,  3.88s/it] 13%|█▎        | 4/30 [00:16<01:59,  4.59s/it] 17%|█▋        | 5/30 [00:22<02:03,  4.93s/it] 20%|██        | 6/30 [00:27<02:06,  5.26s/it] 23%|██▎       | 7/30 [00:36<02:21,  6.16s/it] 27%|██▋       | 8/30 [00:41<02:10,  5.92s/it] 30%|███       | 9/30 [00:46<02:00,  5.76s/it][2023-09-11 22:09:25,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 22:09:25,015] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.16931520822411822, CurrSamplesPerSec=0.17428706631054955, MemAllocated=6.03GB, MaxMemAllocated=7.76GB
 33%|███▎      | 10/30 [00:52<01:55,  5.75s/it] 37%|███▋      | 11/30 [00:57<01:46,  5.62s/it] 40%|████      | 12/30 [01:03<01:41,  5.66s/it] 43%|████▎     | 13/30 [01:08<01:33,  5.51s/it] 47%|████▋     | 14/30 [01:14<01:26,  5.43s/it] 50%|█████     | 15/30 [01:19<01:21,  5.41s/it] 53%|█████▎    | 16/30 [01:24<01:15,  5.38s/it] 57%|█████▋    | 17/30 [01:30<01:10,  5.45s/it] 60%|██████    | 18/30 [01:35<01:04,  5.38s/it] 63%|██████▎   | 19/30 [01:40<00:58,  5.30s/it][2023-09-11 22:10:18,398] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 22:10:18,399] [INFO] [timer.py:260:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=0.17891793379523763, CurrSamplesPerSec=0.1900000878811289, MemAllocated=6.03GB, MaxMemAllocated=7.76GB
 67%|██████▋   | 20/30 [01:45<00:52,  5.29s/it] 70%|███████   | 21/30 [01:51<00:47,  5.24s/it] 73%|███████▎  | 22/30 [01:56<00:42,  5.33s/it] 77%|███████▋  | 23/30 [02:01<00:37,  5.32s/it] 80%|████████  | 24/30 [02:06<00:31,  5.24s/it] 83%|████████▎ | 25/30 [02:12<00:26,  5.23s/it] 87%|████████▋ | 26/30 [02:17<00:20,  5.22s/it] 90%|█████████ | 27/30 [02:22<00:15,  5.20s/it] 93%|█████████▎| 28/30 [02:27<00:10,  5.29s/it] 97%|█████████▋| 29/30 [02:33<00:05,  5.28s/it][2023-09-11 22:11:10,648] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 22:11:10,649] [INFO] [timer.py:260:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=0.18321182142540807, CurrSamplesPerSec=0.20169039306339204, MemAllocated=6.03GB, MaxMemAllocated=7.76GB
100%|██████████| 30/30 [02:38<00:00,  5.18s/it]100%|██████████| 30/30 [02:38<00:00,  5.27s/it]
172.16.0.62 - - [11/Sep/2023 22:11:11] "GET /hello/30 HTTP/1.1" 200 597
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:36,  5.05s/it] 10%|█         | 2/20 [00:10<01:31,  5.07s/it] 15%|█▌        | 3/20 [00:15<01:26,  5.08s/it] 20%|██        | 4/20 [00:20<01:21,  5.08s/it] 25%|██▌       | 5/20 [00:25<01:17,  5.19s/it] 30%|███       | 6/20 [00:30<01:12,  5.19s/it] 35%|███▌      | 7/20 [00:35<01:06,  5.13s/it] 40%|████      | 8/20 [00:40<01:00,  5.07s/it] 45%|████▌     | 9/20 [00:45<00:55,  5.05s/it][2023-09-11 22:12:18,105] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 22:12:18,106] [INFO] [timer.py:260:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=0.18653804674737065, CurrSamplesPerSec=0.19774240181332764, MemAllocated=6.03GB, MaxMemAllocated=7.76GB
 50%|█████     | 10/20 [00:50<00:50,  5.06s/it] 55%|█████▌    | 11/20 [00:55<00:45,  5.05s/it] 60%|██████    | 12/20 [01:01<00:40,  5.07s/it] 65%|██████▌   | 13/20 [01:06<00:35,  5.12s/it] 70%|███████   | 14/20 [01:11<00:30,  5.15s/it] 75%|███████▌  | 15/20 [01:16<00:25,  5.19s/it] 80%|████████  | 16/20 [01:21<00:20,  5.14s/it] 85%|████████▌ | 17/20 [01:26<00:15,  5.07s/it] 90%|█████████ | 18/20 [01:31<00:10,  5.07s/it] 95%|█████████▌| 19/20 [01:36<00:05,  5.05s/it][2023-09-11 22:13:09,021] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 22:13:09,022] [INFO] [timer.py:260:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=0.1885296469461477, CurrSamplesPerSec=0.2001045391717513, MemAllocated=6.03GB, MaxMemAllocated=7.76GB
100%|██████████| 20/20 [01:41<00:00,  5.04s/it]100%|██████████| 20/20 [01:41<00:00,  5.09s/it]
172.16.0.62 - - [11/Sep/2023 22:13:10] "GET /hello/20 HTTP/1.1" 200 399
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:05<00:35,  5.09s/it] 25%|██▌       | 2/8 [00:10<00:31,  5.19s/it] 38%|███▊      | 3/8 [00:15<00:25,  5.19s/it] 50%|█████     | 4/8 [00:20<00:20,  5.19s/it] 62%|██████▎   | 5/8 [00:25<00:15,  5.13s/it] 75%|███████▌  | 6/8 [00:30<00:10,  5.06s/it] 88%|████████▊ | 7/8 [00:35<00:05,  5.02s/it]100%|██████████| 8/8 [00:40<00:00,  5.03s/it]100%|██████████| 8/8 [00:40<00:00,  5.09s/it]
172.16.0.62 - - [11/Sep/2023 22:14:02] "GET /hello/8 HTTP/1.1" 200 169
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:00<00:26, 21.41it/s]  7%|▋         | 39/582 [00:01<00:27, 19.58it/s] 10%|▉         | 57/582 [00:02<00:27, 19.02it/s] 13%|█▎        | 75/582 [00:03<00:27, 18.57it/s] 16%|█▌        | 93/582 [00:04<00:26, 18.50it/s] 19%|█▉        | 111/582 [00:05<00:25, 18.45it/s] 22%|██▏       | 129/582 [00:06<00:24, 18.42it/s] 25%|██▌       | 147/582 [00:07<00:23, 18.40it/s] 28%|██▊       | 165/582 [00:08<00:22, 18.38it/s] 31%|███▏      | 183/582 [00:09<00:22, 18.10it/s] 35%|███▍      | 201/582 [00:10<00:20, 18.17it/s] 38%|███▊      | 219/582 [00:11<00:19, 18.23it/s] 41%|████      | 237/582 [00:12<00:18, 18.24it/s] 44%|████▍     | 255/582 [00:13<00:17, 18.27it/s] 47%|████▋     | 273/582 [00:14<00:16, 18.28it/s] 50%|█████     | 291/582 [00:15<00:16, 17.89it/s] 53%|█████▎    | 309/582 [00:16<00:15, 17.99it/s] 56%|█████▌    | 327/582 [00:17<00:14, 18.07it/s] 59%|█████▉    | 345/582 [00:18<00:13, 18.12it/s] 62%|██████▏   | 363/582 [00:19<00:12, 18.16it/s] 65%|██████▌   | 381/582 [00:20<00:11, 18.21it/s] 69%|██████▊   | 399/582 [00:21<00:10, 18.23it/s] 72%|███████▏  | 417/582 [00:22<00:09, 18.26it/s] 75%|███████▍  | 435/582 [00:23<00:08, 18.25it/s] 78%|███████▊  | 453/582 [00:24<00:07, 18.27it/s] 81%|████████  | 471/582 [00:25<00:06, 18.21it/s] 84%|████████▍ | 489/582 [00:26<00:05, 18.23it/s] 87%|████████▋ | 507/582 [00:27<00:04, 18.28it/s] 90%|█████████ | 525/582 [00:28<00:03, 18.24it/s] 93%|█████████▎| 543/582 [00:29<00:02, 17.74it/s] 96%|█████████▋| 561/582 [00:30<00:01, 17.36it/s] 99%|█████████▉| 579/582 [00:32<00:00, 16.44it/s]100%|██████████| 582/582 [00:32<00:00, 16.79it/s]100%|██████████| 582/582 [00:32<00:00, 18.08it/s]
172.16.0.62 - - [11/Sep/2023 22:14:59] "GET /load-model HTTP/1.1" 200 35

-> User: 刚刚有那些人出场？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:16:13] "POST /inference HTTP/1.1" 200 1566

-> User: 阿邦的女友叫什么名字？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:16:28] "POST /inference HTTP/1.1" 200 240

-> User: 阿邦和霄霄之间是什么关系？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:16:40] "POST /inference HTTP/1.1" 200 156

-> User: 阿邦和霄霄之间发生什么？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:25:09] "POST /inference HTTP/1.1" 200 156

-> User: 为什么你不想知道？ 有什么原因吗？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:25:31] "POST /inference HTTP/1.1" 200 380

-> User: 那么聪事实上来讲， 阿邦对霄霄做了什么不健康的事情？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:25:52] "POST /inference HTTP/1.1" 200 156

-> User: 那么我们换个问题



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:26:01] "POST /inference HTTP/1.1" 200 192

-> User: 阿邦和陈璇之间发生什么？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:26:10] "POST /inference HTTP/1.1" 200 156

-> User: 陈璇长什么样子？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:27:13] "POST /inference HTTP/1.1" 200 1622

-> User: 霄霄的外貌特征是怎样的？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:27:29] "POST /inference HTTP/1.1" 200 528

-> User: 阿邦是怎样凌辱霄霄的尸体的



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:34:21] "POST /inference HTTP/1.1" 200 1614
172.16.0.62 - - [11/Sep/2023 22:34:41] "GET /reset-state HTTP/1.1" 200 35
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:08<13:16,  8.05s/it][2023-09-11 22:37:28,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 22:37:28,451] [INFO] [timer.py:260:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=0.1881363682454118, CurrSamplesPerSec=0.20007939628295263, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
  2%|▏         | 2/100 [00:13<10:13,  6.26s/it]  3%|▎         | 3/100 [00:18<09:11,  5.69s/it]  4%|▍         | 4/100 [00:23<08:40,  5.43s/it]  5%|▌         | 5/100 [00:28<08:20,  5.26s/it]  6%|▌         | 6/100 [00:33<08:06,  5.17s/it]  7%|▋         | 7/100 [00:38<07:55,  5.11s/it]  8%|▊         | 8/100 [00:43<07:47,  5.08s/it]  9%|▉         | 9/100 [00:48<07:46,  5.13s/it] 10%|█         | 10/100 [00:53<07:40,  5.11s/it] 11%|█         | 11/100 [00:58<07:35,  5.12s/it][2023-09-11 22:38:18,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 22:38:18,963] [INFO] [timer.py:260:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=0.18953388198499943, CurrSamplesPerSec=0.19772878234957744, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
 12%|█▏        | 12/100 [01:03<07:28,  5.10s/it] 13%|█▎        | 13/100 [01:08<07:18,  5.03s/it] 14%|█▍        | 14/100 [01:13<07:08,  4.98s/it] 15%|█▌        | 15/100 [01:18<07:00,  4.94s/it] 16%|█▌        | 16/100 [01:23<06:53,  4.92s/it] 17%|█▋        | 17/100 [01:27<06:48,  4.93s/it] 18%|█▊        | 18/100 [01:32<06:43,  4.92s/it] 19%|█▉        | 19/100 [01:37<06:38,  4.92s/it] 20%|██        | 20/100 [01:42<06:33,  4.92s/it] 21%|██        | 21/100 [01:47<06:28,  4.92s/it][2023-09-11 22:39:07,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 22:39:07,973] [INFO] [timer.py:260:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=0.19128940222093962, CurrSamplesPerSec=0.20276951283055103, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
 22%|██▏       | 22/100 [01:52<06:24,  4.93s/it] 23%|██▎       | 23/100 [01:57<06:19,  4.92s/it] 24%|██▍       | 24/100 [02:02<06:13,  4.92s/it] 25%|██▌       | 25/100 [02:07<06:12,  4.97s/it] 26%|██▌       | 26/100 [02:12<06:10,  5.01s/it] 27%|██▋       | 27/100 [02:17<06:08,  5.04s/it] 28%|██▊       | 28/100 [02:22<06:05,  5.07s/it] 29%|██▉       | 29/100 [02:27<05:56,  5.02s/it] 30%|███       | 30/100 [02:32<05:47,  4.97s/it] 31%|███       | 31/100 [02:37<05:39,  4.92s/it][2023-09-11 22:39:57,821] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 22:39:57,822] [INFO] [timer.py:260:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=0.19231494972360902, CurrSamplesPerSec=0.20012059803571583, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
 32%|███▏      | 32/100 [02:42<05:36,  4.95s/it] 33%|███▎      | 33/100 [02:47<05:31,  4.95s/it] 34%|███▍      | 34/100 [02:52<05:26,  4.95s/it] 35%|███▌      | 35/100 [02:57<05:22,  4.95s/it] 36%|███▌      | 36/100 [03:02<05:16,  4.95s/it] 37%|███▋      | 37/100 [03:07<05:12,  4.96s/it] 38%|███▊      | 38/100 [03:12<05:06,  4.95s/it] 39%|███▉      | 39/100 [03:17<05:01,  4.94s/it] 40%|████      | 40/100 [03:22<05:00,  5.00s/it] 41%|████      | 41/100 [03:27<04:55,  5.01s/it][2023-09-11 22:40:47,660] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 22:40:47,660] [INFO] [timer.py:260:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=0.19314316957524771, CurrSamplesPerSec=0.19930735023974547, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
 42%|████▏     | 42/100 [03:32<04:50,  5.02s/it] 43%|████▎     | 43/100 [03:37<04:45,  5.01s/it] 44%|████▍     | 44/100 [03:42<04:40,  5.01s/it] 45%|████▌     | 45/100 [03:47<04:33,  4.97s/it] 46%|████▌     | 46/100 [03:51<04:25,  4.92s/it] 47%|████▋     | 47/100 [03:56<04:19,  4.90s/it] 48%|████▊     | 48/100 [04:01<04:14,  4.89s/it] 49%|████▉     | 49/100 [04:06<04:09,  4.89s/it] 50%|█████     | 50/100 [04:11<04:04,  4.89s/it] 51%|█████     | 51/100 [04:16<03:59,  4.89s/it][2023-09-11 22:41:36,646] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 22:41:36,646] [INFO] [timer.py:260:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=0.1941201535008731, CurrSamplesPerSec=0.2043404236204891, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
 52%|█████▏    | 52/100 [04:21<03:54,  4.89s/it] 53%|█████▎    | 53/100 [04:26<03:49,  4.88s/it] 54%|█████▍    | 54/100 [04:30<03:44,  4.88s/it] 55%|█████▌    | 55/100 [04:35<03:39,  4.88s/it] 56%|█████▌    | 56/100 [04:40<03:34,  4.87s/it] 57%|█████▋    | 57/100 [04:45<03:29,  4.87s/it] 58%|█████▊    | 58/100 [04:50<03:26,  4.92s/it] 59%|█████▉    | 59/100 [04:55<03:22,  4.94s/it] 60%|██████    | 60/100 [05:00<03:17,  4.94s/it] 61%|██████    | 61/100 [05:05<03:13,  4.97s/it][2023-09-11 22:42:25,880] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 22:42:25,880] [INFO] [timer.py:260:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=0.19485912472567135, CurrSamplesPerSec=0.20421337514291832, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
 62%|██████▏   | 62/100 [05:10<03:08,  4.95s/it] 63%|██████▎   | 63/100 [05:15<03:03,  4.95s/it] 64%|██████▍   | 64/100 [05:20<02:57,  4.92s/it] 65%|██████▌   | 65/100 [05:25<02:51,  4.91s/it] 66%|██████▌   | 66/100 [05:30<02:46,  4.90s/it] 67%|██████▋   | 67/100 [05:34<02:41,  4.89s/it] 68%|██████▊   | 68/100 [05:39<02:36,  4.90s/it] 69%|██████▉   | 69/100 [05:44<02:31,  4.89s/it] 70%|███████   | 70/100 [05:49<02:27,  4.91s/it] 71%|███████   | 71/100 [05:54<02:22,  4.91s/it][2023-09-11 22:43:14,897] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 22:43:14,897] [INFO] [timer.py:260:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=0.19555197305943464, CurrSamplesPerSec=0.20344169884885274, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
 72%|███████▏  | 72/100 [05:59<02:17,  4.91s/it] 73%|███████▎  | 73/100 [06:04<02:12,  4.92s/it] 74%|███████▍  | 74/100 [06:09<02:08,  4.93s/it] 75%|███████▌  | 75/100 [06:14<02:02,  4.92s/it] 76%|███████▌  | 76/100 [06:19<01:58,  4.93s/it] 77%|███████▋  | 77/100 [06:24<01:55,  5.00s/it] 78%|███████▊  | 78/100 [06:29<01:51,  5.07s/it] 79%|███████▉  | 79/100 [06:34<01:46,  5.07s/it] 80%|████████  | 80/100 [06:39<01:41,  5.08s/it] 81%|████████  | 81/100 [06:44<01:36,  5.08s/it][2023-09-11 22:44:05,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 22:44:05,264] [INFO] [timer.py:260:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=0.19577266073276647, CurrSamplesPerSec=0.2013835110355591, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
 82%|████████▏ | 82/100 [06:49<01:30,  5.05s/it] 83%|████████▎ | 83/100 [06:54<01:24,  5.00s/it] 84%|████████▍ | 84/100 [06:59<01:19,  4.97s/it] 85%|████████▌ | 85/100 [07:04<01:14,  4.96s/it] 86%|████████▌ | 86/100 [07:09<01:09,  4.94s/it] 87%|████████▋ | 87/100 [07:14<01:04,  4.93s/it] 88%|████████▊ | 88/100 [07:19<00:59,  4.94s/it] 89%|████████▉ | 89/100 [07:24<00:54,  4.94s/it] 90%|█████████ | 90/100 [07:29<00:49,  4.95s/it] 91%|█████████ | 91/100 [07:34<00:44,  4.96s/it][2023-09-11 22:44:54,559] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 22:44:54,560] [INFO] [timer.py:260:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=0.19624214830296183, CurrSamplesPerSec=0.20350271984427395, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
 92%|█████████▏| 92/100 [07:39<00:39,  4.95s/it] 93%|█████████▎| 93/100 [07:44<00:34,  4.95s/it] 94%|█████████▍| 94/100 [07:49<00:30,  5.02s/it] 95%|█████████▌| 95/100 [07:54<00:25,  5.07s/it] 96%|█████████▌| 96/100 [07:59<00:20,  5.08s/it] 97%|█████████▋| 97/100 [08:04<00:15,  5.10s/it] 98%|█████████▊| 98/100 [08:09<00:10,  5.08s/it] 99%|█████████▉| 99/100 [08:14<00:05,  5.02s/it]100%|██████████| 100/100 [08:19<00:00,  4.99s/it]100%|██████████| 100/100 [08:19<00:00,  5.00s/it]
172.16.0.62 - - [11/Sep/2023 22:45:36] "GET /hello/100 HTTP/1.1" 200 1954
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:00<00:26, 21.45it/s]  7%|▋         | 39/582 [00:02<00:28, 19.16it/s] 10%|▉         | 57/582 [00:03<00:28, 18.54it/s] 13%|█▎        | 75/582 [00:04<00:28, 17.70it/s] 16%|█▌        | 93/582 [00:05<00:27, 17.84it/s] 19%|█▉        | 111/582 [00:06<00:26, 17.82it/s] 22%|██▏       | 129/582 [00:07<00:25, 17.89it/s] 25%|██▌       | 147/582 [00:08<00:25, 17.39it/s] 28%|██▊       | 165/582 [00:09<00:23, 17.64it/s] 31%|███▏      | 183/582 [00:10<00:22, 17.77it/s] 35%|███▍      | 201/582 [00:11<00:21, 17.90it/s] 38%|███▊      | 219/582 [00:12<00:20, 17.45it/s] 41%|████      | 237/582 [00:13<00:20, 17.13it/s] 44%|████▍     | 255/582 [00:14<00:19, 16.99it/s] 47%|████▋     | 273/582 [00:15<00:18, 16.84it/s] 50%|█████     | 291/582 [00:16<00:17, 16.84it/s] 53%|█████▎    | 309/582 [00:17<00:16, 16.84it/s] 56%|█████▌    | 327/582 [00:18<00:15, 16.90it/s] 59%|█████▉    | 345/582 [00:19<00:14, 16.77it/s] 62%|██████▏   | 363/582 [00:20<00:12, 16.92it/s] 65%|██████▌   | 381/582 [00:21<00:11, 16.99it/s] 69%|██████▊   | 399/582 [00:22<00:10, 17.16it/s] 72%|███████▏  | 417/582 [00:24<00:09, 16.95it/s] 75%|███████▍  | 435/582 [00:25<00:08, 17.19it/s] 78%|███████▊  | 453/582 [00:26<00:07, 17.33it/s] 81%|████████  | 471/582 [00:27<00:06, 17.52it/s] 84%|████████▍ | 489/582 [00:28<00:05, 17.19it/s] 87%|████████▋ | 507/582 [00:29<00:04, 17.47it/s] 90%|█████████ | 525/582 [00:30<00:03, 17.62it/s] 93%|█████████▎| 543/582 [00:31<00:02, 17.79it/s] 96%|█████████▋| 561/582 [00:32<00:01, 17.36it/s] 99%|█████████▉| 579/582 [00:33<00:00, 17.10it/s]100%|██████████| 582/582 [00:33<00:00, 17.44it/s]
172.16.0.62 - - [11/Sep/2023 22:46:43] "GET /load-model HTTP/1.1" 200 35

-> User: 介绍阿邦的外貌特征



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:57:46] "POST /inference HTTP/1.1" 200 654

-> User: 介绍霄霄的外貌特征



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:58:18] "POST /inference HTTP/1.1" 200 678

-> User: 介绍丁婷的外貌特征



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:58:36] "POST /inference HTTP/1.1" 200 708

-> User: 介绍林慕容的外貌特征



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:58:54] "POST /inference HTTP/1.1" 200 810

-> User: 林慕容和霄霄有什么区别？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:59:07] "POST /inference HTTP/1.1" 200 858

-> User: 霄霄是怎么死的？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:59:22] "POST /inference HTTP/1.1" 200 360

-> User: 陈璇是怎么死的？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:59:39] "POST /inference HTTP/1.1" 200 714

-> User: 陈璇养了什么宠物？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 22:59:55] "POST /inference HTTP/1.1" 200 912

-> User: 阿邦做了什么？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 23:04:47] "POST /inference HTTP/1.1" 200 660

-> User: 阿邦对丁婷做了什么？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 23:05:06] "POST /inference HTTP/1.1" 200 1566

-> User: 阿邦对陈璇的尸体做了什么？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 23:05:24] "POST /inference HTTP/1.1" 200 1575

-> User: 阿邦如何惩罚女特务的尸体



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 23:05:44] "POST /inference HTTP/1.1" 200 1390

-> User: 陈璇是否想杀死阿邦



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 23:06:02] "POST /inference HTTP/1.1" 200 211
172.16.0.62 - - [11/Sep/2023 23:06:16] "GET /reset-state HTTP/1.1" 200 35

-> User: 陈璇是否想杀死阿邦



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 23:06:23] "POST /inference HTTP/1.1" 200 168

-> User: 阿邦是否想杀死陈璇



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 23:06:30] "POST /inference HTTP/1.1" 200 168

-> User: 阿邦和陈璇在浴室内的决斗的结果是什么？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 23:06:43] "POST /inference HTTP/1.1" 200 180
172.16.0.62 - - [11/Sep/2023 23:06:49] "GET /reset-state HTTP/1.1" 200 35

-> User: 阿邦和霄霄的战斗的结果是怎样 的？ 谁死了？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 23:07:04] "POST /inference HTTP/1.1" 200 384
172.16.0.62 - - [11/Sep/2023 23:07:13] "GET /reset-state HTTP/1.1" 200 35

-> User: 接下来对话，请你扮演霄霄和我对话，



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 23:07:29] "POST /inference HTTP/1.1" 200 144

-> User: 你叫什么名字？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 23:07:39] "POST /inference HTTP/1.1" 200 174

-> User: 描述你的外貌特征



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 23:09:16] "POST /inference HTTP/1.1" 200 306

-> User: 你头上有什么饰品？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 23:09:43] "POST /inference HTTP/1.1" 200 156

-> User: 你喜欢穿什么衣服？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 23:09:51] "POST /inference HTTP/1.1" 200 258

-> User: 你认识阿邦吗？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 23:09:59] "POST /inference HTTP/1.1" 200 138

-> User: 你在小树林里和谁战斗过吗？



-> Assistant: 
172.16.0.62 - - [11/Sep/2023 23:10:10] "POST /inference HTTP/1.1" 200 174
172.16.0.62 - - [11/Sep/2023 23:10:17] "GET /reset-state HTTP/1.1" 200 35

-> User: 接下来的对话请扮演霄霄和我对话



-> 霄霄: 
172.16.0.62 - - [11/Sep/2023 23:10:57] "POST /inference HTTP/1.1" 200 111

-> User: 对的是你



-> 霄霄: 
172.16.0.62 - - [11/Sep/2023 23:11:01] "POST /inference HTTP/1.1" 200 111

-> User: 你叫什么名字？



-> 霄霄: 
172.16.0.62 - - [11/Sep/2023 23:11:12] "POST /inference HTTP/1.1" 200 129

-> User: 我叫阿邦



-> 霄霄: 
172.16.0.62 - - [11/Sep/2023 23:11:16] "POST /inference HTTP/1.1" 200 141

-> User: 我叫阿邦



-> 霄霄: 
172.16.0.62 - - [11/Sep/2023 23:11:21] "POST /inference HTTP/1.1" 200 141
172.16.0.62 - - [11/Sep/2023 23:11:28] "GET /reset-state HTTP/1.1" 200 35
172.16.0.62 - - [11/Sep/2023 23:11:41] "GET /reset-state HTTP/1.1" 200 35

-> 阿邦: 小师妹，你竟然这样！



-> 霄霄: 
172.16.0.62 - - [11/Sep/2023 23:11:50] "POST /inference HTTP/1.1" 200 111

-> 阿邦: 你这个欺师灭祖的美女蛇，看我不收拾你！



-> 霄霄: 
172.16.0.62 - - [11/Sep/2023 23:12:05] "POST /inference HTTP/1.1" 200 165

-> 阿邦: 阿邦开始与霄霄搏斗



-> 霄霄: 
172.16.0.62 - - [11/Sep/2023 23:12:15] "POST /inference HTTP/1.1" 200 165

-> 阿邦: 霄霄死了



-> 霄霄: 
172.16.0.62 - - [11/Sep/2023 23:12:23] "POST /inference HTTP/1.1" 200 123

-> 阿邦: 你在干嘛



-> 霄霄: 
172.16.0.62 - - [11/Sep/2023 23:12:30] "POST /inference HTTP/1.1" 200 135
172.16.0.62 - - [11/Sep/2023 23:12:33] "GET /reset-state HTTP/1.1" 200 35
172.16.0.62 - - [11/Sep/2023 23:12:37] "GET /reset-state HTTP/1.1" 200 35

-> 阿邦: 介绍你的外观



-> 霄霄: 
172.16.0.62 - - [11/Sep/2023 23:12:42] "POST /inference HTTP/1.1" 200 111
172.16.0.62 - - [11/Sep/2023 23:13:27] "GET /reset-state HTTP/1.1" 200 35
172.16.0.62 - - [11/Sep/2023 23:13:35] "GET /reset-state HTTP/1.1" 200 35

-> 阿邦脱下了霄霄的内裤



-> 霄霄的尸体
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/rwkv-trainer/app.py", line 157, in inference
    res,infer_state = rwkv_generate(infer_model,message,state=infer_state)
  File "/home/neromous/rwkv-trainer/src/model_run.py", line 334, in rwkv_generate
    tokens = prompt2text(message)['tokens']  if i == 0 else [token]
  File "/home/neromous/rwkv-trainer/src/model_run.py", line 277, in prompt2text
    role_prefix = role['prefix']
TypeError: 'bool' object is not subscriptable
172.16.0.62 - - [11/Sep/2023 23:13:42] "POST /inference HTTP/1.1" 500 753

-> 阿邦阿邦



-> 霄霄的尸体
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/rwkv-trainer/app.py", line 157, in inference
    res,infer_state = rwkv_generate(infer_model,message,state=infer_state)
  File "/home/neromous/rwkv-trainer/src/model_run.py", line 334, in rwkv_generate
    tokens = prompt2text(message)['tokens']  if i == 0 else [token]
  File "/home/neromous/rwkv-trainer/src/model_run.py", line 277, in prompt2text
    role_prefix = role['prefix']
TypeError: 'bool' object is not subscriptable
172.16.0.62 - - [11/Sep/2023 23:14:04] "POST /inference HTTP/1.1" 500 753
[2023-09-11 23:19:44,625] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 13667
[2023-09-11 23:19:46,184] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 13667
[2023-09-11 23:19:50,280] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-11 23:20:04,255] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-11 23:20:05,591] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-11 23:20:05,614] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-11 23:20:07,469] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-11 23:20:08,790] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-11 23:20:08,790] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-11 23:20:08,790] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-11 23:20:08,790] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-11 23:20:08,790] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-11 23:20:10,622] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.736410617828369 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.14it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.05s/it]100%|██████████| 1/1 [00:05<00:00,  5.05s/it]
[2023-09-11 23:20:57,812] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-11 23:20:57,812] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-11 23:20:57,812] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-11 23:21:00,225] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.256380558013916 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-11 23:21:05,917] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-11 23:21:05,965] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-11 23:21:05,965] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-11 23:21:05,965] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-11 23:21:05,965] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-11 23:21:05,965] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-11 23:21:05,966] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-11 23:21:05,966] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-11 23:21:17,684] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-11 23:21:17,685] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 23:21:17,686] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 28.39 GB, percent = 7.5%
[2023-09-11 23:21:31,287] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-11 23:21:31,288] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 23:21:31,288] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 67.58 GB, percent = 17.9%
[2023-09-11 23:21:31,288] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-11 23:21:32,235] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-11 23:21:32,236] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-11 23:21:32,236] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 67.58 GB, percent = 17.9%
[2023-09-11 23:21:32,259] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-11 23:21:32,259] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-11 23:21:32,259] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fba3f9245e0>
[2023-09-11 23:21:32,259] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 23:21:32,261] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-11 23:21:32,261] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-11 23:21:32,261] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-11 23:21:32,261] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-11 23:21:32,261] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-11 23:21:32,261] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-11 23:21:32,261] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fba3f9e7e20>
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-11 23:21:32,262] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-11 23:21:32,263] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-11 23:21:32,264] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-11 23:21:32,264] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-11 23:21:32,264] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-11 23:21:32,264] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-11 23:21:32,264] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-11 23:21:32,264] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-11 23:21:32,264] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-11 23:21:32,264] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-11 23:21:32,264] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-11 23:21:32,264] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-11 23:21:32,264] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-11 23:21:32,264] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-11 23:21:32,264] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

  0%|          | 0/10 [00:00<?, ?it/s][2023-09-11 23:21:56,618] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-09-11 23:21:56,618] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-11 23:21:56,619] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-11 23:21:56,619] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-09-11 23:21:56,619] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
[2023-09-11 23:22:00,621] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
 10%|█         | 1/10 [00:04<00:36,  4.03s/it][2023-09-11 23:22:02,148] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
 20%|██        | 2/10 [00:05<00:20,  2.56s/it] 30%|███       | 3/10 [00:10<00:27,  3.86s/it] 40%|████      | 4/10 [00:16<00:26,  4.42s/it] 50%|█████     | 5/10 [00:21<00:23,  4.76s/it] 60%|██████    | 6/10 [00:26<00:19,  4.92s/it] 70%|███████   | 7/10 [00:32<00:15,  5.11s/it] 80%|████████  | 8/10 [00:37<00:10,  5.22s/it] 90%|█████████ | 9/10 [00:43<00:05,  5.41s/it][2023-09-11 23:22:46,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 23:22:46,147] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.18190780535178624, CurrSamplesPerSec=0.1683517479296323, MemAllocated=6.03GB, MaxMemAllocated=7.76GB
100%|██████████| 10/10 [00:49<00:00,  5.57s/it]100%|██████████| 10/10 [00:49<00:00,  4.96s/it]
172.16.0.62 - - [11/Sep/2023 23:22:47] "GET /hello/10 HTTP/1.1" 200 204
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:01<00:27, 20.51it/s]  7%|▋         | 39/582 [00:02<00:28, 18.78it/s] 10%|▉         | 57/582 [00:03<00:28, 18.13it/s] 13%|█▎        | 75/582 [00:04<00:28, 17.79it/s] 16%|█▌        | 93/582 [00:05<00:27, 17.64it/s] 19%|█▉        | 111/582 [00:06<00:26, 17.48it/s] 22%|██▏       | 129/582 [00:07<00:26, 17.42it/s] 25%|██▌       | 147/582 [00:08<00:25, 17.38it/s] 28%|██▊       | 165/582 [00:09<00:24, 17.29it/s] 31%|███▏      | 183/582 [00:10<00:23, 17.24it/s] 35%|███▍      | 201/582 [00:11<00:22, 17.23it/s] 38%|███▊      | 219/582 [00:12<00:21, 17.24it/s] 41%|████      | 237/582 [00:13<00:19, 17.27it/s] 44%|████▍     | 255/582 [00:14<00:18, 17.29it/s] 47%|████▋     | 273/582 [00:15<00:17, 17.30it/s] 50%|█████     | 291/582 [00:16<00:16, 17.30it/s] 53%|█████▎    | 309/582 [00:17<00:15, 17.26it/s] 56%|█████▌    | 327/582 [00:18<00:14, 17.25it/s] 59%|█████▉    | 345/582 [00:19<00:13, 17.36it/s] 62%|██████▏   | 363/582 [00:20<00:12, 17.48it/s] 65%|██████▌   | 381/582 [00:21<00:11, 17.16it/s] 69%|██████▊   | 399/582 [00:23<00:11, 16.05it/s] 72%|███████▏  | 417/582 [00:24<00:10, 16.43it/s] 75%|███████▍  | 435/582 [00:25<00:08, 16.76it/s] 78%|███████▊  | 453/582 [00:26<00:07, 16.97it/s] 81%|████████  | 471/582 [00:27<00:06, 17.13it/s] 84%|████████▍ | 489/582 [00:28<00:05, 17.11it/s] 87%|████████▋ | 507/582 [00:29<00:04, 16.68it/s] 90%|█████████ | 525/582 [00:30<00:03, 16.39it/s] 93%|█████████▎| 543/582 [00:31<00:02, 16.03it/s] 96%|█████████▋| 561/582 [00:32<00:01, 16.43it/s] 99%|█████████▉| 579/582 [00:33<00:00, 16.69it/s]100%|██████████| 582/582 [00:33<00:00, 17.04it/s]100%|██████████| 582/582 [00:33<00:00, 17.12it/s]
172.16.0.62 - - [11/Sep/2023 23:23:51] "GET /load-model HTTP/1.1" 200 35
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:07<01:05,  7.24s/it] 20%|██        | 2/10 [00:12<00:49,  6.15s/it] 30%|███       | 3/10 [00:18<00:40,  5.82s/it] 40%|████      | 4/10 [00:23<00:33,  5.64s/it] 50%|█████     | 5/10 [00:29<00:28,  5.62s/it] 60%|██████    | 6/10 [00:34<00:22,  5.58s/it] 70%|███████   | 7/10 [00:39<00:16,  5.50s/it] 80%|████████  | 8/10 [00:45<00:10,  5.45s/it] 90%|█████████ | 9/10 [00:50<00:05,  5.42s/it][2023-09-11 23:24:54,233] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 23:24:54,234] [INFO] [timer.py:260:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=0.1805516330761665, CurrSamplesPerSec=0.1915907880836061, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
100%|██████████| 10/10 [00:55<00:00,  5.36s/it]100%|██████████| 10/10 [00:55<00:00,  5.57s/it]
172.16.0.62 - - [11/Sep/2023 23:24:55] "GET /hello/10 HTTP/1.1" 200 207

-> 阿邦遇到了谁？

-> 
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/rwkv-trainer/app.py", line 157, in inference
    res,infer_state = rwkv_generate(infer_model,message,state=infer_state)
  File "/home/neromous/rwkv-trainer/src/model_run.py", line 346, in rwkv_generate
    token = rwkv_sample_logits(out,
UnboundLocalError: local variable 'out' referenced before assignment
172.16.0.62 - - [11/Sep/2023 23:25:10] "POST /inference HTTP/1.1" 500 753

-> 阿邦遇到了谁？

-> 
172.16.0.62 - - [11/Sep/2023 23:25:43] "POST /inference HTTP/1.1" 200 1083

-> 文本中出现了几个女人？

-> 
172.16.0.62 - - [11/Sep/2023 23:26:09] "POST /inference HTTP/1.1" 200 799
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:06<00:57,  6.43s/it] 20%|██        | 2/10 [00:11<00:45,  5.67s/it] 30%|███       | 3/10 [00:16<00:37,  5.43s/it] 40%|████      | 4/10 [00:21<00:31,  5.32s/it] 50%|█████     | 5/10 [00:26<00:26,  5.25s/it] 60%|██████    | 6/10 [00:32<00:21,  5.28s/it] 70%|███████   | 7/10 [00:37<00:16,  5.37s/it] 80%|████████  | 8/10 [00:42<00:10,  5.25s/it] 90%|█████████ | 9/10 [00:47<00:05,  5.18s/it][2023-09-11 23:27:13,461] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 23:27:13,461] [INFO] [timer.py:260:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=0.18341323450968056, CurrSamplesPerSec=0.19592186936835518, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
100%|██████████| 10/10 [00:52<00:00,  5.16s/it]100%|██████████| 10/10 [00:52<00:00,  5.30s/it]
172.16.0.62 - - [11/Sep/2023 23:27:14] "GET /hello/10 HTTP/1.1" 200 206

-> 谁是阿邦？

-> 
172.16.0.62 - - [11/Sep/2023 23:27:30] "POST /inference HTTP/1.1" 200 811
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:01<00:29, 18.88it/s]  7%|▋         | 39/582 [00:02<00:31, 17.25it/s] 10%|▉         | 57/582 [00:03<00:31, 16.72it/s] 13%|█▎        | 75/582 [00:04<00:30, 16.47it/s] 16%|█▌        | 93/582 [00:05<00:29, 16.51it/s] 19%|█▉        | 111/582 [00:06<00:28, 16.45it/s] 22%|██▏       | 129/582 [00:07<00:27, 16.35it/s] 25%|██▌       | 147/582 [00:08<00:27, 15.95it/s] 28%|██▊       | 165/582 [00:10<00:26, 15.84it/s] 31%|███▏      | 183/582 [00:11<00:26, 15.34it/s] 35%|███▍      | 201/582 [00:12<00:24, 15.53it/s] 38%|███▊      | 219/582 [00:13<00:23, 15.78it/s] 41%|████      | 237/582 [00:14<00:21, 15.73it/s] 44%|████▍     | 255/582 [00:15<00:20, 15.69it/s] 47%|████▋     | 273/582 [00:16<00:19, 15.88it/s] 50%|█████     | 291/582 [00:18<00:17, 16.28it/s] 53%|█████▎    | 309/582 [00:19<00:16, 16.09it/s] 56%|█████▌    | 327/582 [00:20<00:15, 15.95it/s] 59%|█████▉    | 345/582 [00:21<00:14, 16.02it/s] 62%|██████▏   | 363/582 [00:22<00:13, 15.96it/s] 65%|██████▌   | 381/582 [00:23<00:12, 15.91it/s] 69%|██████▊   | 399/582 [00:24<00:11, 15.53it/s] 72%|███████▏  | 417/582 [00:26<00:10, 15.51it/s] 75%|███████▍  | 435/582 [00:27<00:09, 15.37it/s] 78%|███████▊  | 453/582 [00:28<00:08, 15.51it/s] 81%|████████  | 471/582 [00:29<00:07, 15.74it/s] 84%|████████▍ | 489/582 [00:30<00:05, 15.76it/s] 87%|████████▋ | 507/582 [00:31<00:04, 15.78it/s] 90%|█████████ | 525/582 [00:32<00:03, 15.78it/s] 93%|█████████▎| 543/582 [00:34<00:02, 16.21it/s] 96%|█████████▋| 561/582 [00:35<00:01, 15.98it/s] 99%|█████████▉| 579/582 [00:36<00:00, 15.64it/s]100%|██████████| 582/582 [00:36<00:00, 16.00it/s]100%|██████████| 582/582 [00:36<00:00, 15.94it/s]
172.16.0.62 - - [11/Sep/2023 23:28:19] "GET /load-model HTTP/1.1" 200 35
172.16.0.62 - - [11/Sep/2023 23:29:04] "GET /reset-state HTTP/1.1" 200 35

-> 谁是阿邦？

-> 
172.16.0.62 - - [11/Sep/2023 23:29:19] "POST /inference HTTP/1.1" 200 1553
172.16.0.62 - - [11/Sep/2023 23:29:45] "GET /reset-state HTTP/1.1" 200 35

-> 谁是阿邦？

-> 
172.16.0.62 - - [11/Sep/2023 23:30:32] "POST /inference HTTP/1.1" 200 6119
172.16.0.62 - - [11/Sep/2023 23:30:48] "GET /reset-state HTTP/1.1" 200 35

-> 阿邦拉住霄霄的尸体的大腿

-> 
172.16.0.62 - - [11/Sep/2023 23:31:42] "POST /inference HTTP/1.1" 200 6139
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:07<01:09,  7.68s/it] 20%|██        | 2/10 [00:12<00:48,  6.07s/it] 30%|███       | 3/10 [00:17<00:39,  5.61s/it] 40%|████      | 4/10 [00:22<00:32,  5.39s/it] 50%|█████     | 5/10 [00:27<00:26,  5.27s/it] 60%|██████    | 6/10 [00:32<00:20,  5.17s/it] 70%|███████   | 7/10 [00:37<00:15,  5.15s/it] 80%|████████  | 8/10 [00:43<00:10,  5.16s/it] 90%|█████████ | 9/10 [00:48<00:05,  5.16s/it][2023-09-11 23:33:01,793] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-11 23:33:01,794] [INFO] [timer.py:260:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=0.18344889014945323, CurrSamplesPerSec=0.15870906007909602, MemAllocated=11.43GB, MaxMemAllocated=13.16GB
100%|██████████| 10/10 [00:54<00:00,  5.51s/it]100%|██████████| 10/10 [00:54<00:00,  5.45s/it]
172.16.0.62 - - [11/Sep/2023 23:33:03] "GET /hello/10 HTTP/1.1" 200 204
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:01<00:27, 20.62it/s]  7%|▋         | 39/582 [00:02<00:29, 18.58it/s] 10%|▉         | 57/582 [00:03<00:29, 18.07it/s] 13%|█▎        | 75/582 [00:04<00:28, 17.85it/s] 16%|█▌        | 93/582 [00:05<00:27, 17.81it/s] 19%|█▉        | 111/582 [00:06<00:26, 17.82it/s] 22%|██▏       | 129/582 [00:07<00:25, 17.82it/s] 25%|██▌       | 147/582 [00:08<00:24, 17.85it/s] 28%|██▊       | 165/582 [00:09<00:23, 17.85it/s] 31%|███▏      | 183/582 [00:10<00:22, 17.78it/s] 35%|███▍      | 201/582 [00:11<00:21, 17.73it/s] 38%|███▊      | 219/582 [00:12<00:20, 17.72it/s] 41%|████      | 237/582 [00:13<00:19, 17.72it/s] 44%|████▍     | 255/582 [00:14<00:18, 17.72it/s] 47%|████▋     | 273/582 [00:15<00:17, 17.70it/s] 50%|█████     | 291/582 [00:16<00:16, 17.71it/s] 53%|█████▎    | 309/582 [00:17<00:15, 17.78it/s] 56%|█████▌    | 327/582 [00:18<00:14, 17.40it/s] 59%|█████▉    | 345/582 [00:19<00:13, 17.28it/s] 62%|██████▏   | 363/582 [00:20<00:12, 17.14it/s] 65%|██████▌   | 381/582 [00:21<00:11, 16.82it/s] 69%|██████▊   | 399/582 [00:22<00:10, 16.96it/s] 72%|███████▏  | 417/582 [00:23<00:09, 16.69it/s] 75%|███████▍  | 435/582 [00:24<00:08, 16.94it/s] 78%|███████▊  | 453/582 [00:25<00:07, 16.63it/s] 81%|████████  | 471/582 [00:27<00:06, 16.68it/s] 84%|████████▍ | 489/582 [00:28<00:05, 16.51it/s] 87%|████████▋ | 507/582 [00:29<00:04, 16.32it/s] 90%|█████████ | 525/582 [00:30<00:03, 16.37it/s] 93%|█████████▎| 543/582 [00:31<00:02, 16.25it/s] 96%|█████████▋| 561/582 [00:32<00:01, 16.38it/s] 99%|█████████▉| 579/582 [00:33<00:00, 15.87it/s]100%|██████████| 582/582 [00:33<00:00, 16.23it/s]100%|██████████| 582/582 [00:33<00:00, 17.17it/s]
172.16.0.62 - - [11/Sep/2023 23:34:07] "GET /load-model HTTP/1.1" 200 35

-> 霄霄是谁？

-> 
172.16.0.62 - - [11/Sep/2023 23:34:54] "POST /inference HTTP/1.1" 200 6130
172.16.0.62 - - [11/Sep/2023 23:35:42] "GET /reset-state HTTP/1.1" 200 35

-> 阿邦只觉得她身子猛地一蜷缩，然后整个蛮腰都剧烈扭动起来，百来斤的肉体在阿邦身上非常用力的磨蹭，嘴里哦哦哦叫个不停，卷檐帽在摔倒的时候已经掉落，盘在脑后的长发挂下来，随着身子的扭动，发梢也在阿邦脸上不停划动。两人的脸离着不过七八厘米，阿邦能感觉到她嘴里呼出的热气越来越弱，越来越少，两瓣香唇微微颤抖着开合，像是要咒骂他。

-> 
172.16.0.62 - - [11/Sep/2023 23:36:23] "POST /inference HTTP/1.1" 200 6080
172.16.0.62 - - [11/Sep/2023 23:36:57] "GET /reset-state HTTP/1.1" 200 35

-> 阿邦忍着脸上剧痛，他知道此时决不能有半点缩手，否则一旦霄霄恢复了元气自己就再难得手了，‘你死我活’是他现在唯一的选择！所幸的是，由于他是背靠背的勒着霄霄，所以就算霄霄这时候再怎么装作柔弱可人，他已完全不用去看，娇滴滴的声音也再无法从她喉咙里发出，摆脱了这些因素的‘干扰’后，他现在只需要一心一意的保持住姿势，继续对霄霄执行绞刑。

-> 
172.16.0.62 - - [11/Sep/2023 23:37:39] "POST /inference HTTP/1.1" 200 6122

-> 霄霄的大腿

-> 
172.16.0.62 - - [11/Sep/2023 23:40:18] "POST /inference HTTP/1.1" 200 1486

-> 霄霄漂亮的乳房

-> 
172.16.0.62 - - [11/Sep/2023 23:41:13] "POST /inference HTTP/1.1" 200 6080

-> 霄霄死透了

-> 
172.16.0.62 - - [11/Sep/2023 23:42:16] "POST /inference HTTP/1.1" 200 6116

-> 霄霄冰冷的尸体

-> 
172.16.0.62 - - [12/Sep/2023 00:16:15] "POST /inference HTTP/1.1" 200 6140

-> 

-> 
172.16.0.62 - - [12/Sep/2023 07:09:20] "POST /inference HTTP/1.1" 200 1582
[2023-09-12 08:56:14,764] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 15912
[2023-09-12 08:56:16,576] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 15912
[2023-09-12 08:56:20,509] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 08:56:43,699] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 08:56:45,025] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 08:56:45,049] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 08:56:46,901] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 08:56:48,206] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 08:56:48,206] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 08:56:48,206] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 08:56:48,206] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 08:56:48,206] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 08:56:50,017] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Traceback (most recent call last):
  File "/home/neromous/rwkv-trainer/app.py", line 7, in <module>
    from src.dataset_finetune import MyDataset
  File "/home/neromous/rwkv-trainer/src/dataset_finetune.py", line 12, in <module>
    role_config = read_config()['role']
NameError: name 'read_config' is not defined
[2023-09-12 08:56:58,229] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 34448
[2023-09-12 08:56:58,230] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed', '--deepspeed_config', 'ds_config.config'] exits with return code = 1
[2023-09-12 08:57:50,713] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 08:57:52,038] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 08:57:52,062] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 08:57:53,890] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 08:57:55,192] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 08:57:55,192] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 08:57:55,192] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 08:57:55,192] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 08:57:55,192] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 08:57:57,020] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.6988868713378906 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.33it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.03s/it]100%|██████████| 1/1 [00:05<00:00,  5.03s/it]
Traceback (most recent call last):
  File "/home/neromous/rwkv-trainer/app.py", line 74, in <module>
    train_data = MyDataset(train_args)
  File "/home/neromous/rwkv-trainer/src/dataset_finetune.py", line 47, in __init__
    self.pool = self.limit_sample(args.epoch_steps)
AttributeError: 'MyDataset' object has no attribute 'limit_sample'
[2023-09-12 08:58:22,232] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 34831
[2023-09-12 08:58:22,233] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed', '--deepspeed_config', 'ds_config.config'] exits with return code = 1
[2023-09-12 08:58:52,720] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 08:58:54,047] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 08:58:54,071] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 08:58:55,891] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 08:58:57,199] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 08:58:57,199] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 08:58:57,199] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 08:58:57,199] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 08:58:57,199] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 08:58:59,032] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.697499990463257 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.32it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:04<00:00,  4.97s/it]100%|██████████| 1/1 [00:04<00:00,  4.97s/it]
[2023-09-12 08:59:45,384] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 08:59:45,384] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 08:59:45,385] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 08:59:47,896] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.2241575717926025 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 08:59:53,506] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 08:59:53,554] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 08:59:53,554] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 08:59:53,554] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 08:59:53,555] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 08:59:53,555] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 08:59:53,555] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 08:59:53,555] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 09:00:05,468] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 09:00:05,469] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:00:05,469] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 29.86 GB, percent = 7.9%
[2023-09-12 09:00:19,066] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 09:00:19,067] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:00:19,067] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.11 GB, percent = 18.3%
[2023-09-12 09:00:19,067] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 09:00:20,013] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 09:00:20,014] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:00:20,014] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.11 GB, percent = 18.3%
[2023-09-12 09:00:20,035] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 09:00:20,035] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 09:00:20,036] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f2cf7552ee0>
[2023-09-12 09:00:20,036] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 09:00:20,037] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 09:00:20,037] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 09:00:20,037] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 09:00:20,037] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 09:00:20,037] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 09:00:20,038] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 09:00:20,038] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 09:00:20,038] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 09:00:20,038] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 09:00:20,038] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 09:00:20,038] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2cf75ce820>
[2023-09-12 09:00:20,038] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 09:00:20,038] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 09:00:20,038] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 09:00:20,038] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 09:00:20,038] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 09:00:20,038] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 09:00:20,038] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 09:00:20,038] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 09:00:20,038] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 09:00:20,039] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 09:00:20,040] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 09:00:20,041] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 09:00:20,041] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 09:00:20,041] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 09:00:20,041] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 09:00:20,041] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/rwkv-trainer/app.py", line 109, in train_batch
    loss = train(model_engine, train_data.__getitem__(batch_ids[0],
TypeError: __getitem__() got an unexpected keyword argument 'debut'
172.16.0.62 - - [12/Sep/2023 09:03:08] "POST /train HTTP/1.1" 500 749
[2023-09-12 09:03:30,223] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 35121
[2023-09-12 09:03:32,722] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 35121
[2023-09-12 09:03:35,360] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 09:03:39,188] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:03:40,507] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 09:03:40,531] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 09:03:42,358] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:03:43,650] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 09:03:43,651] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 09:03:43,651] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 09:03:43,651] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 09:03:43,651] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 09:03:45,515] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.714975357055664 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.58it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.03s/it]100%|██████████| 1/1 [00:05<00:00,  5.03s/it]
[2023-09-12 09:04:31,990] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 09:04:31,990] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 09:04:31,990] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 09:04:34,567] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.2664384841918945 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 09:04:40,226] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 09:04:40,278] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 09:04:40,278] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 09:04:40,278] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 09:04:40,278] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 09:04:40,278] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 09:04:40,278] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 09:04:40,278] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 09:04:52,202] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 09:04:52,203] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:04:52,204] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 29.83 GB, percent = 7.9%
[2023-09-12 09:05:05,906] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 09:05:05,907] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:05:05,907] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.11 GB, percent = 18.3%
[2023-09-12 09:05:05,907] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 09:05:06,850] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 09:05:06,851] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:05:06,851] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.11 GB, percent = 18.3%
[2023-09-12 09:05:06,871] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 09:05:06,872] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 09:05:06,872] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f22a5f10ee0>
[2023-09-12 09:05:06,872] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 09:05:06,873] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 09:05:06,873] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 09:05:06,873] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 09:05:06,873] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 09:05:06,873] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f22a5f8c820>
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 09:05:06,874] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 09:05:06,875] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 09:05:06,876] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 09:05:06,877] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/rwkv-trainer/app.py", line 109, in train_batch
    loss = train(model_engine, train_data.__getitem__(batch_ids[0],
  File "/home/neromous/rwkv-trainer/src/dataset_finetune.py", line 125, in __getitem__
    self.item = self.prefix_tokenizer(prefix,role=prefix_role) + self.item
ValueError: operands could not be broadcast together with shapes (40,) (4147683,) 
172.16.0.62 - - [12/Sep/2023 09:05:09] "POST /train HTTP/1.1" 500 749
[2023-09-12 09:06:09,470] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 35573
[2023-09-12 09:06:10,882] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 35573
[2023-09-12 09:06:14,416] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 09:06:15,426] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:06:16,763] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 09:06:16,787] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 09:06:18,636] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:06:19,971] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 09:06:19,971] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 09:06:19,971] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 09:06:19,971] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 09:06:19,971] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 09:06:21,797] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.7482457160949707 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.02it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.04s/it]100%|██████████| 1/1 [00:05<00:00,  5.04s/it]
[2023-09-12 09:07:08,272] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 09:07:08,273] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 09:07:08,273] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 09:07:10,675] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.2474148273468018 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 09:07:16,346] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 09:07:16,396] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 09:07:16,396] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 09:07:16,396] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 09:07:16,396] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 09:07:16,396] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 09:07:16,396] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 09:07:16,396] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 09:07:27,974] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 09:07:27,975] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:07:27,976] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 29.87 GB, percent = 7.9%
[2023-09-12 09:07:41,338] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 09:07:41,339] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:07:41,339] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.05 GB, percent = 18.3%
[2023-09-12 09:07:41,339] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 09:07:42,281] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 09:07:42,282] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:07:42,282] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.05 GB, percent = 18.3%
[2023-09-12 09:07:42,304] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 09:07:42,304] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 09:07:42,304] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7ff95d112ee0>
[2023-09-12 09:07:42,304] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 09:07:42,305] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 09:07:42,306] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 09:07:42,306] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 09:07:42,306] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 09:07:42,306] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 09:07:42,306] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 09:07:42,306] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 09:07:42,306] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 09:07:42,306] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 09:07:42,306] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 09:07:42,306] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff95d18d820>
[2023-09-12 09:07:42,306] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 09:07:42,306] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 09:07:42,306] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 09:07:42,306] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 09:07:42,307] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 09:07:42,308] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 09:07:42,309] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 09:07:42,309] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 09:07:42,309] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 09:07:42,309] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 09:07:42,309] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 09:07:42,309] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 09:07:42,309] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/rwkv-trainer/app.py", line 109, in train_batch
    loss = train(model_engine, train_data.__getitem__(batch_ids[0],
  File "/home/neromous/rwkv-trainer/src/dataset_finetune.py", line 125, in __getitem__
    self.item = self.prefix_tokenizer(prefix,role=prefix_role) + self.item
ValueError: operands could not be broadcast together with shapes (1,40) (4147683,) 
172.16.0.62 - - [12/Sep/2023 09:07:44] "POST /train HTTP/1.1" 500 749
[2023-09-12 09:10:58,298] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:10:59,625] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 09:10:59,648] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 09:11:01,460] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:11:02,752] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 09:11:02,752] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 09:11:02,752] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 09:11:02,752] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 09:11:02,752] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 09:11:04,550] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.639979839324951 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.42it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.04s/it]100%|██████████| 1/1 [00:05<00:00,  5.04s/it]
[2023-09-12 09:11:50,551] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 09:11:50,551] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 09:11:50,552] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W socket.cpp:426] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:426] [c10d] The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
[E socket.cpp:462] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "/home/neromous/rwkv-trainer/app.py", line 79, in <module>
    model_engine, optimizer, _, _ = deepspeed.initialize(model=model,
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/__init__.py", line 135, in initialize
    dist.init_distributed(dist_backend=dist_backend, dist_init_required=dist_init_required)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/comm/comm.py", line 670, in init_distributed
    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/comm/torch.py", line 116, in __init__
    self.init_process_group(backend, timeout, init_method, rank, world_size)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/comm/torch.py", line 142, in init_process_group
    torch.distributed.init_process_group(backend,
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 900, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/distributed/rendezvous.py", line 245, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/distributed/rendezvous.py", line 176, in _create_c10d_store
    return TCPStore(
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
[2023-09-12 09:11:57,819] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 36491
[2023-09-12 09:11:57,820] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed', '--deepspeed_config', 'ds_config.config'] exits with return code = 1
[2023-09-12 09:12:00,130] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 35976
[2023-09-12 09:12:01,497] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 35976
[2023-09-12 09:12:05,110] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 09:12:06,196] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:12:07,544] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 09:12:07,569] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 09:12:09,369] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:12:10,631] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 09:12:10,631] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 09:12:10,631] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 09:12:10,631] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 09:12:10,631] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 09:12:12,424] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.708777904510498 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.24it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:04<00:00,  4.99s/it]100%|██████████| 1/1 [00:04<00:00,  4.99s/it]
[2023-09-12 09:12:58,924] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 09:12:58,924] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 09:12:58,924] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 09:13:01,455] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.2758948802948 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 09:13:07,139] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 09:13:07,187] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 09:13:07,187] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 09:13:07,187] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 09:13:07,188] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 09:13:07,188] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 09:13:07,188] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 09:13:07,188] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 09:13:19,023] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 09:13:19,024] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:13:19,024] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 30.38 GB, percent = 8.0%
[2023-09-12 09:13:32,855] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 09:13:32,855] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:13:32,856] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.58 GB, percent = 18.4%
[2023-09-12 09:13:32,856] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 09:13:34,358] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 09:13:34,359] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:13:34,359] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.58 GB, percent = 18.4%
[2023-09-12 09:13:34,389] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 09:13:34,389] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 09:13:34,389] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f85cfad0ee0>
[2023-09-12 09:13:34,389] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 09:13:34,391] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 09:13:34,391] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 09:13:34,391] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 09:13:34,391] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 09:13:34,391] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 09:13:34,392] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 09:13:34,392] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 09:13:34,392] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 09:13:34,392] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 09:13:34,392] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 09:13:34,392] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f85cfb4b820>
[2023-09-12 09:13:34,392] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 09:13:34,392] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 09:13:34,392] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 09:13:34,392] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 09:13:34,392] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 09:13:34,392] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 09:13:34,392] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 09:13:34,392] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 09:13:34,392] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 09:13:34,392] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 09:13:34,392] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 09:13:34,392] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 09:13:34,392] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 09:13:34,393] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 09:13:34,394] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 09:13:34,395] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

==item= [13091 11435 10696 ... 10080    11     0]
=== [[65530, 65531, 10399, 10258, 13091, 11975, 16735, 9822, 10285, 11435, 17766, 17688, 9823, 14734, 12484, 15907, 19137, 10293, 16503, 10370, 14328, 10292, 17777, 17269, 11124, 15957, 12199, 11679, 16530, 15898, 19137, 10696, 11922, 10655, 14015, 13156, 10838, 16503, 15323, 65535]]
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/rwkv-trainer/app.py", line 109, in train_batch
    loss = train(model_engine, train_data.__getitem__(batch_ids[0],
  File "/home/neromous/rwkv-trainer/src/dataset_finetune.py", line 129, in __getitem__
    self.item = self.prefix_tokenizer(prefix,role=prefix_role) + self.item
ValueError: operands could not be broadcast together with shapes (1,40) (4147683,) 
172.16.0.62 - - [12/Sep/2023 09:13:36] "POST /train HTTP/1.1" 500 749
[2023-09-12 09:14:18,110] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 36794
[2023-09-12 09:14:23,066] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 09:15:40,965] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:15:42,307] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 09:15:42,331] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 09:15:44,181] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:15:45,483] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 09:15:45,483] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 09:15:45,483] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 09:15:45,483] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 09:15:45,484] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 09:15:47,275] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.700488328933716 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.43it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.15s/it]100%|██████████| 1/1 [00:05<00:00,  5.15s/it]
[2023-09-12 09:16:33,626] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 09:16:33,627] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 09:16:33,627] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 09:16:36,439] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.244441509246826 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 09:16:42,113] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 09:16:42,164] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 09:16:42,164] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 09:16:42,164] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 09:16:42,164] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 09:16:42,164] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 09:16:42,164] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 09:16:42,165] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 09:16:54,517] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 09:16:54,518] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:16:54,519] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 30.38 GB, percent = 8.0%
[2023-09-12 09:17:08,032] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 09:17:08,033] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:17:08,033] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.66 GB, percent = 18.4%
[2023-09-12 09:17:08,033] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 09:17:09,037] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 09:17:09,038] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:17:09,038] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.66 GB, percent = 18.4%
[2023-09-12 09:17:09,059] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 09:17:09,059] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 09:17:09,059] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f3f5e710f10>
[2023-09-12 09:17:09,059] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 09:17:09,061] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 09:17:09,061] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 09:17:09,061] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 09:17:09,061] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 09:17:09,061] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 09:17:09,061] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 09:17:09,061] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3f5e78b160>
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 09:17:09,062] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 09:17:09,063] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 09:17:09,064] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 09:17:09,064] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 09:17:09,064] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 09:17:09,064] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 09:17:09,064] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 09:17:09,064] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 09:17:09,064] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 09:17:09,064] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 09:17:09,064] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 09:17:09,064] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 09:17:09,064] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 09:17:09,064] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 09:17:09,064] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 09:17:09,064] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 09:17:09,064] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 09:17:09,064] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

==item= [13091 11435 10696 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530, 65531, 10399, 10258, 13091, 11975, 16735, 9822, 10285, 11435, 17766, 17688, 9823, 14734, 12484, 15907, 19137, 10293, 16503, 10370, 14328, 10292, 17777, 17269, 11124, 15957, 12199, 11679, 16530, 15898, 19137, 10696, 11922, 10655, 14015, 13156, 10838, 16503, 15323, 65535]
=== <class 'list'>
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/rwkv-trainer/app.py", line 109, in train_batch
    loss = train(model_engine, train_data.__getitem__(batch_ids[0],
  File "/home/neromous/rwkv-trainer/src/dataset_finetune.py", line 132, in __getitem__
    print("===",type(extend))
ValueError: operands could not be broadcast together with shapes (40,) (4147683,) 
172.16.0.62 - - [12/Sep/2023 09:17:20] "POST /train HTTP/1.1" 500 749
[2023-09-12 09:22:33,456] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 37297
[2023-09-12 09:22:35,100] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 37297
[2023-09-12 09:22:38,553] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 09:22:43,565] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:22:44,898] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 09:22:44,922] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 09:22:46,790] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:22:48,098] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 09:22:48,098] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 09:22:48,098] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 09:22:48,098] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 09:22:48,098] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 09:22:49,916] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.7022705078125 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.16it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.17s/it]100%|██████████| 1/1 [00:05<00:00,  5.17s/it]
[2023-09-12 09:23:36,518] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 09:23:36,518] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 09:23:36,518] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 09:23:39,048] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.2187423706054688 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 09:23:44,650] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 09:23:44,697] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 09:23:44,697] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 09:23:44,698] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 09:23:44,698] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 09:23:44,698] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 09:23:44,698] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 09:23:44,698] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 09:23:57,164] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 09:23:57,165] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:23:57,165] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 30.38 GB, percent = 8.0%
[2023-09-12 09:24:11,594] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 09:24:11,595] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:24:11,595] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.64 GB, percent = 18.4%
[2023-09-12 09:24:11,595] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 09:24:12,542] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 09:24:12,542] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:24:12,543] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.64 GB, percent = 18.4%
[2023-09-12 09:24:12,564] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 09:24:12,564] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 09:24:12,565] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f7111112f10>
[2023-09-12 09:24:12,565] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 09:24:12,566] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 09:24:12,566] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 09:24:12,566] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 09:24:12,566] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 09:24:12,566] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 09:24:12,566] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f711118d160>
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 09:24:12,567] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 09:24:12,568] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 09:24:12,569] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 09:24:12,569] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 09:24:12,569] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 09:24:12,569] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 09:24:12,569] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 09:24:12,569] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 09:24:12,569] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 09:24:12,569] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 09:24:12,569] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 09:24:12,569] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 09:24:12,569] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 09:24:12,569] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 09:24:12,569] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 09:24:12,569] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 09:24:12,569] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 09:24:12,569] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 09:24:12,569] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/rwkv-trainer/app.py", line 109, in train_batch
    loss = train(model_engine, train_data.__getitem__(batch_ids[0],
  File "/home/neromous/rwkv-trainer/src/dataset_finetune.py", line 132, in __getitem__
    self.item = np.concatenate(extend,self.item)
  File "<__array_function__ internals>", line 200, in concatenate
TypeError: only integer scalar arrays can be converted to a scalar index
172.16.0.62 - - [12/Sep/2023 09:24:55] "POST /train HTTP/1.1" 500 749
[2023-09-12 09:26:11,804] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 37918
[2023-09-12 09:26:13,509] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 37918
[2023-09-12 09:26:16,878] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 09:26:24,524] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:26:25,876] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 09:26:25,900] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 09:26:27,761] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:26:29,044] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 09:26:29,044] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 09:26:29,044] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 09:26:29,044] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 09:26:29,044] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 09:26:30,842] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.715481996536255 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.23it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.07s/it]100%|██████████| 1/1 [00:05<00:00,  5.07s/it]
[2023-09-12 09:27:17,249] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 09:27:17,249] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 09:27:17,249] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 09:27:19,787] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.2682454586029053 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 09:27:25,487] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 09:27:25,535] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 09:27:25,535] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 09:27:25,535] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 09:27:25,535] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 09:27:25,535] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 09:27:25,535] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 09:27:25,535] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 09:27:37,308] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 09:27:37,309] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:27:37,309] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 30.73 GB, percent = 8.1%
[2023-09-12 09:27:50,558] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 09:27:50,559] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:27:50,559] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.91 GB, percent = 18.5%
[2023-09-12 09:27:50,559] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 09:27:51,511] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 09:27:51,511] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:27:51,512] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.91 GB, percent = 18.5%
[2023-09-12 09:27:51,533] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 09:27:51,533] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 09:27:51,533] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f10fc112ee0>
[2023-09-12 09:27:51,533] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 09:27:51,534] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 09:27:51,534] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 09:27:51,535] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 09:27:51,535] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 09:27:51,535] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 09:27:51,535] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 09:27:51,535] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 09:27:51,535] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 09:27:51,535] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 09:27:51,535] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 09:27:51,535] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f10fc18d820>
[2023-09-12 09:27:51,535] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 09:27:51,535] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 09:27:51,535] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 09:27:51,535] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 09:27:51,535] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 09:27:51,535] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 09:27:51,535] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 09:27:51,536] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 09:27:51,537] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 09:27:51,538] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 09:27:51,538] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 09:27:51,538] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 09:27:51,538] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/rwkv-trainer/app.py", line 109, in train_batch
    loss = train(model_engine, train_data.__getitem__(batch_ids[0],
  File "/home/neromous/rwkv-trainer/src/dataset_finetune.py", line 132, in __getitem__
    #print("===",extend)
  File "<__array_function__ internals>", line 200, in concatenate
TypeError: only integer scalar arrays can be converted to a scalar index
172.16.0.62 - - [12/Sep/2023 09:27:57] "POST /train HTTP/1.1" 500 749
[2023-09-12 09:31:26,336] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 38489
[2023-09-12 09:31:27,804] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 38489
[2023-09-12 09:31:31,335] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 09:31:33,280] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:31:34,577] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 09:31:34,601] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 09:31:36,447] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:31:37,747] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 09:31:37,747] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 09:31:37,747] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 09:31:37,747] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 09:31:37,747] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 09:31:39,548] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.7226409912109375 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.06it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.05s/it]100%|██████████| 1/1 [00:05<00:00,  5.05s/it]
[2023-09-12 09:32:25,942] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 09:32:25,942] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 09:32:25,943] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 09:32:28,414] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.220576047897339 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 09:32:34,057] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 09:32:34,105] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 09:32:34,105] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 09:32:34,105] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 09:32:34,106] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 09:32:34,106] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 09:32:34,106] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 09:32:34,106] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 09:32:45,802] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 09:32:45,803] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:32:45,804] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 30.68 GB, percent = 8.1%
[2023-09-12 09:32:59,062] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 09:32:59,063] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:32:59,063] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.9 GB, percent = 18.5%
[2023-09-12 09:32:59,063] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 09:33:00,008] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 09:33:00,009] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:33:00,009] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.9 GB, percent = 18.5%
[2023-09-12 09:33:00,033] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 09:33:00,033] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 09:33:00,033] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fbc9c112ee0>
[2023-09-12 09:33:00,033] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 09:33:00,034] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 09:33:00,034] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 09:33:00,035] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 09:33:00,035] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 09:33:00,035] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 09:33:00,035] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 09:33:00,035] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 09:33:00,035] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 09:33:00,035] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 09:33:00,035] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 09:33:00,035] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbc9c18e820>
[2023-09-12 09:33:00,035] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 09:33:00,035] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 09:33:00,035] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 09:33:00,035] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 09:33:00,036] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 09:33:00,037] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 09:33:00,038] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 09:33:00,038] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 09:33:00,038] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 09:33:00,038] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 09:33:00,038] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 09:33:00,038] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 09:33:00,038] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 09:33:00,038] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 09:33:00,038] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 09:33:00,038] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 09:33:00,038] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 09:33:00,038] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 09:33:00,038] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

==item= [13091 11435 10696 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/rwkv-trainer/app.py", line 109, in train_batch
    loss = train(model_engine, train_data.__getitem__(batch_ids[0],
  File "/home/neromous/rwkv-trainer/src/dataset_finetune.py", line 134, in __getitem__
    self.item = np.concatenate(extend,self.item)
  File "<__array_function__ internals>", line 200, in concatenate
TypeError: only integer scalar arrays can be converted to a scalar index
172.16.0.62 - - [12/Sep/2023 09:33:02] "POST /train HTTP/1.1" 500 749
[2023-09-12 09:34:33,262] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 38983
[2023-09-12 09:34:34,720] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 38983
[2023-09-12 09:34:38,370] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 09:34:39,516] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:34:40,867] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 09:34:40,891] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 09:34:42,722] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:34:44,030] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 09:34:44,030] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 09:34:44,030] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 09:34:44,030] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 09:34:44,030] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 09:34:45,837] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.7243502140045166 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.19it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:04<00:00,  4.96s/it]100%|██████████| 1/1 [00:04<00:00,  4.96s/it]
[2023-09-12 09:35:32,232] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 09:35:32,232] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 09:35:32,232] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 09:35:34,668] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.2680859565734863 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 09:35:40,338] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 09:35:40,386] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 09:35:40,386] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 09:35:40,386] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 09:35:40,386] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 09:35:40,386] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 09:35:40,386] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 09:35:40,386] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 09:35:52,006] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 09:35:52,007] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:35:52,007] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 30.67 GB, percent = 8.1%
[2023-09-12 09:36:05,570] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 09:36:05,571] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:36:05,571] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.85 GB, percent = 18.5%
[2023-09-12 09:36:05,571] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 09:36:06,850] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 09:36:06,851] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:36:06,851] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.85 GB, percent = 18.5%
[2023-09-12 09:36:06,883] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 09:36:06,883] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 09:36:06,883] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fb7b7713f40>
[2023-09-12 09:36:06,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 09:36:06,884] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 09:36:06,885] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 09:36:06,885] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 09:36:06,885] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 09:36:06,885] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 09:36:06,885] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 09:36:06,885] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 09:36:06,885] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 09:36:06,885] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb7b778f190>
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 09:36:06,886] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 09:36:06,887] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 09:36:06,888] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 09:36:06,888] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 09:36:06,888] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 09:36:06,888] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 09:36:06,888] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 09:36:06,888] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 09:36:06,888] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 09:36:06,888] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 09:36:06,888] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 09:36:06,888] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 09:36:06,888] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 09:36:06,888] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 09:36:06,888] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 09:36:06,888] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 09:36:06,888] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 09:36:06,888] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 09:36:06,888] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 09:36:06,888] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 09:36:06,888] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

==item= [13091 11435 10696 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/rwkv-trainer/app.py", line 109, in train_batch
    loss = train(model_engine, train_data.__getitem__(batch_ids[0],
  File "/home/neromous/rwkv-trainer/src/dataset_finetune.py", line 137, in __getitem__
    print("==step==",self.step)
AttributeError: 'MyDataset' object has no attribute 'step'
172.16.0.62 - - [12/Sep/2023 09:36:25] "POST /train HTTP/1.1" 500 749
[2023-09-12 09:37:03,459] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 39468
[2023-09-12 09:37:08,536] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 09:38:14,147] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:38:15,479] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 09:38:15,503] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 09:38:17,362] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:38:18,648] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 09:38:18,648] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 09:38:18,648] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 09:38:18,648] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 09:38:18,648] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 09:38:20,482] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.7237601280212402 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.92it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.01s/it]100%|██████████| 1/1 [00:05<00:00,  5.02s/it]
[2023-09-12 09:39:06,870] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 09:39:06,871] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 09:39:06,871] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 09:39:09,489] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.247614860534668 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 09:39:15,137] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 09:39:15,184] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 09:39:15,184] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 09:39:15,184] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 09:39:15,184] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 09:39:15,184] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 09:39:15,185] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 09:39:15,185] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 09:39:27,032] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 09:39:27,033] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:39:27,034] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 30.7 GB, percent = 8.1%
[2023-09-12 09:39:40,779] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 09:39:40,780] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:39:40,780] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 70.01 GB, percent = 18.5%
[2023-09-12 09:39:40,780] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 09:39:41,748] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 09:39:41,749] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:39:41,749] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 70.01 GB, percent = 18.5%
[2023-09-12 09:39:41,770] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 09:39:41,771] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 09:39:41,771] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f8fe2111f40>
[2023-09-12 09:39:41,771] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 09:39:41,772] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 09:39:41,772] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 09:39:41,772] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 09:39:41,773] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 09:39:41,773] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 09:39:41,773] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 09:39:41,773] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 09:39:41,773] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 09:39:41,773] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 09:39:41,773] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 09:39:41,773] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8fe218d190>
[2023-09-12 09:39:41,773] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 09:39:41,773] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 09:39:41,773] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 09:39:41,773] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 09:39:41,773] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 09:39:41,774] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 09:39:41,775] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 09:39:41,776] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 09:39:41,776] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 09:39:41,776] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 09:39:41,776] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 09:39:41,776] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 09:39:41,776] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 09:39:41,776] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 09:39:41,776] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 09:39:41,776] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

==item= [13091 11435 10696 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [    0 10679 13190 ... 11957 13036 10658]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
[2023-09-12 09:40:05,036] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-09-12 09:40:05,036] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-12 09:40:05,036] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-12 09:40:05,037] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-09-12 09:40:05,037] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
[2023-09-12 09:40:08,981] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
==item= [13374 10257 10441 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 10464 17148 13091]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
[2023-09-12 09:40:10,472] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
==item= [17268 10250 16735 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 10790 13594 11223]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [14734 10250 11610 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 12298 11857 17384]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [10439 10760 15768 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 17269 17820 17820]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [12493 18438 10260 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 11454 13338 10258]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [19137 17723 11630 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 13091 12669 15847]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [14221 14734 10529 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 10808 19137 11021]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [10260 16956 19137 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 12493 11223 11021]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [11349 12359 12287 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 10678 11909 19137]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
[2023-09-12 09:40:51,931] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 09:40:51,933] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.19314070598183902, CurrSamplesPerSec=0.1867925505546967, MemAllocated=6.03GB, MaxMemAllocated=7.76GB
==item= [10256 10255 19137 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 13023 12403 14136]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [10383 13594 13594 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 10333 12636 15768]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [12664 10792 13676 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 13110 12826 15636]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [17269 14734 18030 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 17303 17723 10080]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [12158 11454 10288 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 10322 10260 12509]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [18052 12387 14734 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 15091 14808 10250]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [18043 14734 10923 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 10390 13580 16503]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [12692 12354 17148 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 14791 19137 17384]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [10283 10272 16501 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 14734 11975 12636]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [14734 10747 10754 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 10390 14446 11454]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
[2023-09-12 09:41:48,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 09:41:48,742] [INFO] [timer.py:260:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=0.18334727778569437, CurrSamplesPerSec=0.19754816877548417, MemAllocated=6.03GB, MaxMemAllocated=7.76GB
==item= [12642 10080    11 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 11223 19137 12605]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [13844 14042 11082 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 10421 15597 14734]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [10760 13234 10333 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 10308 13091 12269]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [12522 10792 19137 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 10399 14791 13923]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [10333 10580 16689 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 12761 17148 10283]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [17148 13324 14734 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 19137 10399 10250]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [11492 10333 10258 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 10079 14589 10838]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [17882 10464  9830 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 17882 17882 12158]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [10760 13234 10333 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 10370 17194 17922]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [11002 11459 11454 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 17571 14808 12366]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
[2023-09-12 09:42:40,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 09:42:40,916] [INFO] [timer.py:260:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=0.18629048457939568, CurrSamplesPerSec=0.18194154954901806, MemAllocated=6.03GB, MaxMemAllocated=7.76GB
==item= [19137 11187 17384 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 16974 15931 14398]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [17148 10283 11917 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 16443 11684 14734]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [11047 11044 10250 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 10782 15936 14734]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
==item= [12919 10792 11957 ... 10080    11     0]
==item= <class 'numpy.ndarray'>
=== [65530 65531 10399 10258 13091 11975 16735  9822 10285 11435 17766 17688  9823 14734 12484 15907 19137 10293 16503 10370 14328 10292 17777 17269 11124 15957 12199 11679 16530 15898 19137 10696 11922
 10655 14015 13156 10838 16503 15323 65535]
=== <class 'numpy.ndarray'>
==step== [65530 65531 10399 ... 12348 13327 11630]
==self.item=== [65530 65531 10399 ... 10080    11     0]
==self.data[0]=== [    0 10679 13190 ... 10080    11     0]
==self.data[-1]=== [    0 10679 13190 ... 10080    11     0]
[2023-09-12 09:43:00,416] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 39894
[2023-09-12 09:43:01,898] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 39894
[2023-09-12 09:43:05,750] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 09:51:31,144] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:51:32,462] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 09:51:32,486] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 09:51:34,344] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:51:35,642] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 09:51:35,642] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 09:51:35,642] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 09:51:35,642] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 09:51:35,642] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 09:51:37,485] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.6974523067474365 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.30it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.03s/it]100%|██████████| 1/1 [00:05<00:00,  5.03s/it]
[2023-09-12 09:52:23,876] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 09:52:23,877] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 09:52:23,877] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 09:52:26,705] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.238586187362671 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 09:52:32,374] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 09:52:32,423] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 09:52:32,423] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 09:52:32,423] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 09:52:32,423] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 09:52:32,423] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 09:52:32,423] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 09:52:32,423] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 09:52:44,447] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 09:52:44,448] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:52:44,448] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 30.7 GB, percent = 8.1%
[2023-09-12 09:52:57,502] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 09:52:57,503] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:52:57,503] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.89 GB, percent = 18.5%
[2023-09-12 09:52:57,503] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 09:52:58,452] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 09:52:58,453] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:52:58,453] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.89 GB, percent = 18.5%
[2023-09-12 09:52:58,477] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 09:52:58,477] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 09:52:58,477] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f14b379f280>
[2023-09-12 09:52:58,477] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 09:52:58,478] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 09:52:58,479] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 09:52:58,479] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 09:52:58,479] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 09:52:58,479] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 09:52:58,479] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 09:52:58,479] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 09:52:58,479] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 09:52:58,479] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 09:52:58,479] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 09:52:58,479] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f14b378c160>
[2023-09-12 09:52:58,479] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 09:52:58,479] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 09:52:58,479] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 09:52:58,479] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 09:52:58,480] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 09:52:58,481] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 09:52:58,482] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 09:52:58,482] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 09:52:58,482] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 09:52:58,482] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 09:52:58,482] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 09:52:58,482] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 09:52:58,482] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

[2023-09-12 09:53:04,065] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-09-12 09:53:04,065] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-12 09:53:04,065] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-12 09:53:04,065] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-09-12 09:53:04,065] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
[2023-09-12 09:53:07,963] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/rwkv-trainer/app.py", line 118, in train_batch
    losses.append(loss.item())
AttributeError: 'float' object has no attribute 'item'
172.16.0.62 - - [12/Sep/2023 09:53:07] "POST /train HTTP/1.1" 500 749
[2023-09-12 09:53:57,901] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 40804
[2023-09-12 09:53:59,330] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 40804
[2023-09-12 09:54:02,983] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 09:54:04,907] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:54:06,209] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 09:54:06,233] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 09:54:08,061] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:54:09,388] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 09:54:09,388] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 09:54:09,388] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 09:54:09,388] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 09:54:09,388] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 09:54:11,181] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:54:19,036] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:54:20,381] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 09:54:20,404] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 09:54:22,281] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:54:23,574] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 09:54:23,574] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 09:54:23,574] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 09:54:23,575] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 09:54:23,575] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 09:54:25,379] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.7042946815490723 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 13.32it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:04<00:00,  4.95s/it]100%|██████████| 1/1 [00:04<00:00,  4.95s/it]
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.6667463779449463 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.48it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.06s/it]100%|██████████| 1/1 [00:05<00:00,  5.06s/it]
[2023-09-12 09:54:58,686] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 09:54:58,686] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 09:54:58,686] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 09:55:01,321] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.26637864112854 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 09:55:07,019] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 09:55:07,067] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 09:55:07,067] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 09:55:07,068] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 09:55:07,068] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 09:55:07,068] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 09:55:07,068] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 09:55:07,068] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
[2023-09-12 09:55:13,373] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 09:55:13,374] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 09:55:13,374] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W socket.cpp:426] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:426] [c10d] The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
[E socket.cpp:462] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "/home/neromous/rwkv-trainer/app.py", line 79, in <module>
    model_engine, optimizer, _, _ = deepspeed.initialize(model=model,
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/__init__.py", line 135, in initialize
    dist.init_distributed(dist_backend=dist_backend, dist_init_required=dist_init_required)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/comm/comm.py", line 670, in init_distributed
    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/comm/torch.py", line 116, in __init__
    self.init_process_group(backend, timeout, init_method, rank, world_size)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/comm/torch.py", line 142, in init_process_group
    torch.distributed.init_process_group(backend,
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 900, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/distributed/rendezvous.py", line 245, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/distributed/rendezvous.py", line 176, in _create_c10d_store
    return TCPStore(
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:29500 (errno: 98 - Address already in use).
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 09:55:19,897] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 09:55:19,898] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:55:19,899] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 36.65 GB, percent = 9.7%
[2023-09-12 09:55:21,647] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 41584
[2023-09-12 09:55:21,648] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed', '--deepspeed_config', 'ds_config.config'] exits with return code = 1
[2023-09-12 09:55:33,678] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 09:55:33,679] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:55:33,679] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 70.15 GB, percent = 18.6%
[2023-09-12 09:55:33,679] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 09:55:34,632] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 09:55:34,632] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:55:34,633] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 70.15 GB, percent = 18.6%
[2023-09-12 09:55:34,655] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 09:55:34,655] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 09:55:34,655] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f37d3520280>
[2023-09-12 09:55:34,655] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 09:55:34,656] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 09:55:34,657] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 09:55:34,657] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 09:55:34,657] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 09:55:34,657] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 09:55:34,657] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 09:55:34,657] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 09:55:34,657] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 09:55:34,657] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f37d350e160>
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 09:55:34,658] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 09:55:34,659] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 09:55:34,660] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 09:55:34,660] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 09:55:34,660] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 09:55:34,660] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 09:55:34,660] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 09:55:34,660] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 09:55:34,660] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 09:55:34,660] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 09:55:34,660] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 09:55:34,660] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 09:55:34,660] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 09:55:34,660] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 09:55:34,660] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 09:55:34,660] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 09:55:34,660] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 09:55:34,660] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 09:55:34,660] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

[2023-09-12 09:55:37,709] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-09-12 09:55:37,709] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-12 09:55:37,709] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-12 09:55:37,709] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-09-12 09:55:37,709] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
[2023-09-12 09:55:41,599] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/rwkv-trainer/app.py", line 118, in train_batch
    losses.append(loss.item())
AttributeError: 'float' object has no attribute 'item'
172.16.0.62 - - [12/Sep/2023 09:55:41] "POST /train HTTP/1.1" 500 749
[2023-09-12 09:55:57,958] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 41344
[2023-09-12 09:56:02,993] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 09:56:11,084] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:56:12,415] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 09:56:12,439] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 09:56:14,279] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:56:15,568] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 09:56:15,568] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 09:56:15,568] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 09:56:15,568] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 09:56:15,568] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 09:56:17,400] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.740551471710205 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.15it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.02s/it]100%|██████████| 1/1 [00:05<00:00,  5.02s/it]
[2023-09-12 09:57:03,867] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 09:57:03,868] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 09:57:03,868] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 09:57:06,281] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.218599319458008 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 09:57:11,925] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 09:57:11,972] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 09:57:11,972] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 09:57:11,973] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 09:57:11,973] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 09:57:11,973] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 09:57:11,973] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 09:57:11,973] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 09:57:23,549] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 09:57:23,550] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:57:23,551] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 30.94 GB, percent = 8.2%
[2023-09-12 09:57:36,762] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 09:57:36,763] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:57:36,763] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 70.2 GB, percent = 18.6%
[2023-09-12 09:57:36,763] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 09:57:37,716] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 09:57:37,717] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:57:37,717] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 70.2 GB, percent = 18.6%
[2023-09-12 09:57:37,740] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 09:57:37,740] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 09:57:37,740] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f997155b220>
[2023-09-12 09:57:37,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 09:57:37,742] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 09:57:37,742] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 09:57:37,742] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 09:57:37,742] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 09:57:37,742] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 09:57:37,742] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f997154b160>
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 09:57:37,743] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 09:57:37,744] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 09:57:37,745] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 09:57:37,745] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 09:57:37,745] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 09:57:37,745] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 09:57:37,745] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 09:57:37,745] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 09:57:37,745] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 09:57:37,745] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 09:57:37,745] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 09:57:37,745] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 09:57:37,745] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 09:57:37,745] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 09:57:37,745] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 09:57:37,745] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 09:57:37,745] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 09:57:37,745] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 09:57:37,745] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 09:57:37,745] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 09:57:37,745] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 09:57:37,745] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

[2023-09-12 09:57:40,022] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-09-12 09:57:40,023] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-12 09:57:40,023] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-12 09:57:40,023] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-09-12 09:57:40,023] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
[2023-09-12 09:57:43,729] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/rwkv-trainer/app.py", line 118, in train_batch
    losses.append()
TypeError: list.append() takes exactly one argument (0 given)
172.16.0.62 - - [12/Sep/2023 09:57:43] "POST /train HTTP/1.1" 500 749
[2023-09-12 09:58:04,023] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 42056
[2023-09-12 09:58:05,278] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 42056
[2023-09-12 09:58:08,892] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 09:58:10,928] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:58:12,218] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 09:58:12,243] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 09:58:14,079] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 09:58:15,373] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 09:58:15,373] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 09:58:15,373] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 09:58:15,373] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 09:58:15,373] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 09:58:17,190] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.7318358421325684 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.55it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:04<00:00,  4.98s/it]100%|██████████| 1/1 [00:04<00:00,  4.98s/it]
[2023-09-12 09:59:03,406] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 09:59:03,406] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 09:59:03,406] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 09:59:05,891] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.2678463459014893 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 09:59:11,589] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 09:59:11,637] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 09:59:11,637] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 09:59:11,637] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 09:59:11,637] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 09:59:11,637] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 09:59:11,637] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 09:59:11,637] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 09:59:23,346] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 09:59:23,346] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:59:23,347] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 30.46 GB, percent = 8.1%
[2023-09-12 09:59:36,947] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 09:59:36,948] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:59:36,949] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.65 GB, percent = 18.4%
[2023-09-12 09:59:36,949] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 09:59:37,890] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 09:59:37,891] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 09:59:37,891] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.65 GB, percent = 18.4%
[2023-09-12 09:59:37,915] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 09:59:37,915] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 09:59:37,915] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f394c55d280>
[2023-09-12 09:59:37,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 09:59:37,916] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 09:59:37,917] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 09:59:37,917] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 09:59:37,917] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 09:59:37,917] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 09:59:37,917] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 09:59:37,917] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 09:59:37,917] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 09:59:37,917] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 09:59:37,917] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 09:59:37,917] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f394c540790>
[2023-09-12 09:59:37,917] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 09:59:37,918] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 09:59:37,919] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 09:59:37,920] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 09:59:37,920] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 09:59:37,920] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 09:59:37,920] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 09:59:37,920] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 09:59:37,920] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 09:59:37,920] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 09:59:37,920] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 09:59:37,920] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 09:59:37,920] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 09:59:37,920] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 09:59:37,920] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 09:59:37,920] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 09:59:37,920] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 09:59:37,920] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

[2023-09-12 09:59:42,016] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-09-12 09:59:42,016] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-12 09:59:42,017] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-12 09:59:42,017] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-09-12 09:59:42,017] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
[2023-09-12 09:59:45,912] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1

-> loss:4.05859375 remain: 4147723 
[2023-09-12 09:59:47,345] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768

-> loss:3.9609375 remain: 4147251 

-> loss:4.0703125 remain: 4146779 

-> loss:2.515625 remain: 4146307 

-> loss:2.912109375 remain: 4145835 

-> loss:2.83984375 remain: 4145363 

-> loss:2.841796875 remain: 4144891 

-> loss:2.640625 remain: 4144419 

-> loss:2.6171875 remain: 4143947 
[2023-09-12 10:00:30,421] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 10:00:30,422] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.1858900272381282, CurrSamplesPerSec=0.18373637414705563, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.462890625 remain: 4143475 

-> loss:2.400390625 remain: 4143003 

-> loss:2.53515625 remain: 4142531 

-> loss:2.548828125 remain: 4142059 

-> loss:2.275390625 remain: 4141587 

-> loss:2.1171875 remain: 4141115 

-> loss:2.314453125 remain: 4140643 
172.16.0.62 - - [12/Sep/2023 10:01:06] "POST /train HTTP/1.1" 200 201

-> loss:2.431640625 remain: 4140171 

-> loss:2.2734375 remain: 4139699 

-> loss:2.07421875 remain: 4139227 
[2023-09-12 10:01:40,184] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 10:01:40,185] [INFO] [timer.py:260:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=0.18359066323112533, CurrSamplesPerSec=0.20726677827491624, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.201171875 remain: 4138755 

-> loss:2.28515625 remain: 4138283 

-> loss:2.28515625 remain: 4137811 

-> loss:2.37890625 remain: 4137339 

-> loss:2.537109375 remain: 4136867 

-> loss:2.4921875 remain: 4136395 

-> loss:2.40234375 remain: 4135923 

-> loss:2.427734375 remain: 4135451 

-> loss:2.42578125 remain: 4134979 

-> loss:2.4296875 remain: 4134507 
[2023-09-12 10:02:31,640] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 10:02:31,640] [INFO] [timer.py:260:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=0.18733946257292972, CurrSamplesPerSec=0.1900481696864872, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.41015625 remain: 4134035 

-> loss:2.345703125 remain: 4133563 

-> loss:2.291015625 remain: 4133091 
172.16.0.62 - - [12/Sep/2023 10:02:43] "POST /train HTTP/1.1" 200 205
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:01<00:26, 20.81it/s]  7%|▋         | 39/582 [00:02<00:30, 17.85it/s] 10%|▉         | 57/582 [00:03<00:30, 17.15it/s] 13%|█▎        | 75/582 [00:04<00:30, 16.71it/s] 16%|█▌        | 93/582 [00:05<00:29, 16.60it/s] 19%|█▉        | 111/582 [00:06<00:28, 16.40it/s] 22%|██▏       | 129/582 [00:07<00:27, 16.58it/s] 25%|██▌       | 147/582 [00:08<00:26, 16.40it/s] 28%|██▊       | 165/582 [00:09<00:25, 16.27it/s] 31%|███▏      | 183/582 [00:10<00:24, 16.49it/s] 35%|███▍      | 201/582 [00:12<00:23, 16.37it/s] 38%|███▊      | 219/582 [00:13<00:22, 16.30it/s] 41%|████      | 237/582 [00:14<00:20, 16.69it/s] 44%|████▍     | 255/582 [00:15<00:19, 16.75it/s] 47%|████▋     | 273/582 [00:16<00:18, 17.00it/s] 50%|█████     | 291/582 [00:17<00:16, 17.22it/s] 53%|█████▎    | 309/582 [00:18<00:15, 17.40it/s] 56%|█████▌    | 327/582 [00:19<00:14, 17.51it/s] 59%|█████▉    | 345/582 [00:20<00:13, 17.31it/s] 62%|██████▏   | 363/582 [00:21<00:12, 17.44it/s] 65%|██████▌   | 381/582 [00:22<00:11, 17.56it/s] 69%|██████▊   | 399/582 [00:23<00:11, 15.51it/s] 72%|███████▏  | 417/582 [00:25<00:11, 13.77it/s] 75%|███████▍  | 435/582 [00:26<00:10, 13.43it/s] 78%|███████▊  | 453/582 [00:28<00:09, 13.20it/s] 81%|████████  | 470/582 [00:28<00:06, 17.93it/s] 82%|████████▏ | 476/582 [00:29<00:08, 12.87it/s] 84%|████████▍ | 488/582 [00:29<00:05, 16.83it/s] 85%|████████▌ | 495/582 [00:31<00:07, 11.89it/s] 87%|████████▋ | 506/582 [00:31<00:04, 16.00it/s] 88%|████████▊ | 513/582 [00:32<00:06, 11.10it/s] 90%|█████████ | 524/582 [00:32<00:03, 15.52it/s] 91%|█████████ | 531/582 [00:34<00:04, 10.75it/s] 93%|█████████▎| 542/582 [00:34<00:02, 15.37it/s] 94%|█████████▍| 549/582 [00:35<00:03, 10.45it/s] 96%|█████████▌| 560/582 [00:35<00:01, 15.14it/s] 97%|█████████▋| 567/582 [00:37<00:01,  8.29it/s] 99%|█████████▉| 578/582 [00:38<00:00, 12.22it/s]100%|██████████| 582/582 [00:39<00:00, 14.77it/s]
172.16.0.62 - - [12/Sep/2023 10:04:04] "GET /load-model HTTP/1.1" 200 35

-> 在中国队长这部小说里，出现了那些角色？

-> 
172.16.0.62 - - [12/Sep/2023 10:07:01] "POST /inference HTTP/1.1" 200 175

-> 阿邦出现在哪部小说里？

-> 
172.16.0.62 - - [12/Sep/2023 10:07:25] "POST /inference HTTP/1.1" 200 709

-> 阿邦做了什么事情？

-> 
172.16.0.62 - - [12/Sep/2023 10:07:38] "POST /inference HTTP/1.1" 200 391

-> 阿邦和霄霄是什么关系？

-> 
172.16.0.62 - - [12/Sep/2023 10:07:58] "POST /inference HTTP/1.1" 200 1599

-> 阿邦是如何对霄霄的尸体做了事情的

-> 
172.16.0.62 - - [12/Sep/2023 10:09:25] "POST /inference HTTP/1.1" 200 1615

-> 阿邦对霄霄做了什么事情？

-> 
172.16.0.62 - - [12/Sep/2023 10:09:45] "POST /inference HTTP/1.1" 200 1615
172.16.0.62 - - [12/Sep/2023 10:10:00] "GET /reset-state HTTP/1.1" 200 35

-> 阿邦对霄霄的尸体做了什么？

-> 
172.16.0.62 - - [12/Sep/2023 10:10:20] "POST /inference HTTP/1.1" 200 1585
172.16.0.62 - - [12/Sep/2023 10:10:39] "GET /reset-state HTTP/1.1" 200 35
172.16.0.62 - - [12/Sep/2023 10:10:50] "GET /reset-state HTTP/1.1" 200 35

-> 描述阿邦对霄霄尸体进行了那些暴行，要详细

-> 
172.16.0.62 - - [12/Sep/2023 10:11:48] "POST /inference HTTP/1.1" 200 6004

-> 阿邦是否非常享受杀死霄霄的过程？

-> 
172.16.0.62 - - [12/Sep/2023 10:13:02] "POST /inference HTTP/1.1" 200 6163
172.16.0.62 - - [12/Sep/2023 10:13:19] "GET /reset-state HTTP/1.1" 200 35
172.16.0.62 - - [12/Sep/2023 10:13:24] "GET /reset-state HTTP/1.1" 200 35

-> 在中国队长中， 阿邦杀死了几个女警察？

-> 
172.16.0.62 - - [12/Sep/2023 10:14:19] "POST /inference HTTP/1.1" 200 6187
172.16.0.62 - - [12/Sep/2023 10:15:20] "GET /reset-state HTTP/1.1" 200 35

-> user: 霄霄死的时候身上的内裤在什么地方？



-> assistant: 
172.16.0.62 - - [12/Sep/2023 10:16:43] "POST /inference HTTP/1.1" 200 5999

-> loss:2.251953125 remain: 4132619 

-> loss:2.220703125 remain: 4132147 

-> loss:2.287109375 remain: 4131675 

-> loss:2.28515625 remain: 4131203 

-> loss:2.533203125 remain: 4130731 

-> loss:2.796875 remain: 4130259 

-> loss:2.791015625 remain: 4129787 
[2023-09-12 10:17:47,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 10:17:47,652] [INFO] [timer.py:260:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=0.18774057767171537, CurrSamplesPerSec=0.19262475188271996, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:2.720703125 remain: 4129315 

-> loss:2.4765625 remain: 4128843 

-> loss:2.390625 remain: 4128371 

-> loss:2.345703125 remain: 4127899 

-> loss:2.306640625 remain: 4127427 

-> loss:2.271484375 remain: 4126955 

-> loss:2.322265625 remain: 4126483 

-> loss:2.162109375 remain: 4126011 

-> loss:2.201171875 remain: 4125539 
172.16.0.62 - - [12/Sep/2023 10:18:29] "POST /train HTTP/1.1" 200 209

-> loss:2.291015625 remain: 4125067 
[2023-09-12 10:19:06,670] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 10:19:06,671] [INFO] [timer.py:260:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=0.1897507447800393, CurrSamplesPerSec=0.19396784307298268, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:2.380859375 remain: 4124595 

-> loss:2.287109375 remain: 4124123 

-> loss:2.021484375 remain: 4123651 

-> loss:1.9140625 remain: 4123179 

-> loss:2.029296875 remain: 4122707 

-> loss:2.181640625 remain: 4122235 

-> loss:2.28515625 remain: 4121763 

-> loss:2.421875 remain: 4121291 

-> loss:2.390625 remain: 4120819 

-> loss:2.3203125 remain: 4120347 
[2023-09-12 10:19:56,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 10:19:56,994] [INFO] [timer.py:260:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=0.191265550697254, CurrSamplesPerSec=0.19821513450478778, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:2.1875 remain: 4119875 

-> loss:2.04296875 remain: 4119403 

-> loss:2.130859375 remain: 4118931 

-> loss:2.177734375 remain: 4118459 

-> loss:2.125 remain: 4117987 
172.16.0.62 - - [12/Sep/2023 10:20:18] "POST /train HTTP/1.1" 200 195

-> loss:2.01953125 remain: 4117515 

-> loss:2.251953125 remain: 4117043 

-> loss:2.47265625 remain: 4116571 

-> loss:2.591796875 remain: 4116099 

-> loss:2.638671875 remain: 4115627 
[2023-09-12 10:20:51,438] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 10:20:51,439] [INFO] [timer.py:260:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=0.19305865709088996, CurrSamplesPerSec=0.2013529031865652, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:2.43359375 remain: 4115155 

-> loss:2.205078125 remain: 4114683 

-> loss:2.255859375 remain: 4114211 

-> loss:2.26953125 remain: 4113739 

-> loss:2.455078125 remain: 4113267 

-> loss:2.537109375 remain: 4112795 

-> loss:2.544921875 remain: 4112323 

-> loss:2.486328125 remain: 4111851 

-> loss:2.453125 remain: 4111379 

-> loss:2.421875 remain: 4110907 
[2023-09-12 10:21:41,588] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 10:21:41,589] [INFO] [timer.py:260:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=0.19386885054520675, CurrSamplesPerSec=0.2112316318853046, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:2.275390625 remain: 4110435 
172.16.0.62 - - [12/Sep/2023 10:21:42] "POST /train HTTP/1.1" 200 208
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:00<00:26, 21.26it/s]  7%|▋         | 39/582 [00:02<00:28, 18.77it/s] 10%|▉         | 57/582 [00:03<00:29, 17.55it/s] 13%|█▎        | 75/582 [00:04<00:29, 17.02it/s] 16%|█▌        | 93/582 [00:05<00:28, 17.13it/s] 19%|█▉        | 111/582 [00:06<00:27, 17.03it/s] 22%|██▏       | 129/582 [00:07<00:26, 16.87it/s] 25%|██▌       | 147/582 [00:08<00:26, 16.69it/s] 28%|██▊       | 165/582 [00:09<00:25, 16.55it/s] 31%|███▏      | 183/582 [00:10<00:24, 16.45it/s] 35%|███▍      | 201/582 [00:11<00:22, 16.66it/s] 38%|███▊      | 219/582 [00:12<00:21, 16.54it/s] 41%|████      | 237/582 [00:14<00:20, 16.45it/s] 44%|████▍     | 255/582 [00:15<00:19, 16.38it/s] 47%|████▋     | 273/582 [00:16<00:18, 16.34it/s] 50%|█████     | 291/582 [00:17<00:17, 16.75it/s] 53%|█████▎    | 309/582 [00:18<00:16, 16.62it/s] 56%|█████▌    | 327/582 [00:19<00:15, 16.60it/s] 59%|█████▉    | 345/582 [00:20<00:14, 16.49it/s] 62%|██████▏   | 363/582 [00:21<00:13, 16.44it/s] 65%|██████▌   | 381/582 [00:22<00:12, 16.39it/s] 69%|██████▊   | 399/582 [00:23<00:11, 16.62it/s] 72%|███████▏  | 417/582 [00:24<00:09, 16.51it/s] 75%|███████▍  | 435/582 [00:26<00:08, 16.39it/s] 78%|███████▊  | 453/582 [00:27<00:07, 16.36it/s] 81%|████████  | 471/582 [00:28<00:06, 16.77it/s] 84%|████████▍ | 489/582 [00:29<00:05, 16.62it/s] 87%|████████▋ | 507/582 [00:30<00:04, 16.50it/s] 90%|█████████ | 525/582 [00:31<00:03, 16.53it/s] 93%|█████████▎| 543/582 [00:32<00:02, 16.44it/s] 96%|█████████▋| 561/582 [00:33<00:01, 16.38it/s] 99%|█████████▉| 579/582 [00:34<00:00, 16.34it/s]100%|██████████| 582/582 [00:34<00:00, 16.71it/s]
172.16.0.62 - - [12/Sep/2023 10:22:31] "GET /load-model HTTP/1.1" 200 35
172.16.0.62 - - [12/Sep/2023 10:22:56] "GET /reset-state HTTP/1.1" 200 35

-> user: 阿邦杀死了几个女人？



-> assistant: 
172.16.0.62 - - [12/Sep/2023 10:23:45] "POST /inference HTTP/1.1" 200 5903
172.16.0.62 - - [12/Sep/2023 10:25:06] "GET /reset-state HTTP/1.1" 200 35

-> 主题: 你是否杀死了丁婷？作者:林慕容



-> 回复:  回复者：阿邦
172.16.0.62 - - [12/Sep/2023 10:25:44] "POST /inference HTTP/1.1" 200 1862

-> 主题: 在 中国队长 这部小说里面， 阿邦一共杀死了几个女人？作者:安可琳 



-> 回复者：阿邦


172.16.0.62 - - [12/Sep/2023 10:27:20] "POST /inference HTTP/1.1" 200 5988
172.16.0.62 - - [12/Sep/2023 10:31:32] "GET /reset-state HTTP/1.1" 200 35
172.16.0.62 - - [12/Sep/2023 10:31:35] "GET /reset-state HTTP/1.1" 200 35

-> 以下是两个人之间的对话， 包括交谈着包括 中国队长这部小说的读者和编辑，他们就小说中的内容进行讨论

-> User:阿邦是否杀死了霄霄？ 如何杀死的？



-> assistant:阿邦采用勒杀的方式杀死了霄霄，并且在霄霄死后奸淫了她的尸体。



-> User: 阿邦如何处理丁婷的尸体的？



-> assistant:
172.16.0.62 - - [12/Sep/2023 10:31:49] "POST /inference HTTP/1.1" 200 525

-> 以下是两个人之间的对话， 包括交谈着包括 中国队长这部小说的读者和编辑，他们就小说中的内容进行讨论

-> User:阿邦是否杀死了霄霄？ 如何杀死的？



-> assistant:阿邦采用勒杀的方式杀死了霄霄，并且在霄霄死后奸淫了她的尸体。



-> User: 描述阿邦杀死陈璇的细节



-> assistant:
172.16.0.62 - - [12/Sep/2023 10:32:24] "POST /inference HTTP/1.1" 200 3528
172.16.0.62 - - [12/Sep/2023 10:32:38] "GET /reset-state HTTP/1.1" 200 35

-> 以下是两个人之间的对话， 包括交谈着包括 中国队长这部小说的读者和编辑，他们就小说中的内容进行讨论

-> User:阿邦是否杀死了霄霄？ 如何杀死的？



-> assistant:阿邦采用勒杀的方式杀死了霄霄，并且在霄霄死后奸淫了她的尸体。



-> User: 描述阿邦在浴室中与陈璇搏斗，并且杀死她的场景



-> assistant:
172.16.0.62 - - [12/Sep/2023 10:33:25] "POST /inference HTTP/1.1" 200 6163
172.16.0.62 - - [12/Sep/2023 10:33:40] "GET /reset-state HTTP/1.1" 200 35

-> 以下是两个人之间的对话， 包括交谈着包括 中国队长这部小说的读者和编辑，他们就小说中的内容进行讨论

-> User:阿邦是否杀死了霄霄？ 如何杀死的？



-> assistant:阿邦采用勒杀的方式杀死了霄霄，并且在霄霄死后奸淫了她的尸体。



-> User: 阿邦和陈璇的尸体发生了什事情？



-> assistant:
172.16.0.62 - - [12/Sep/2023 10:34:25] "POST /inference HTTP/1.1" 200 6078
172.16.0.62 - - [12/Sep/2023 10:34:36] "GET /reset-state HTTP/1.1" 200 35

-> 以下是两个人之间的对话， 包括交谈着包括 中国队长这部小说的读者和编辑，他们就小说中的内容进行讨论

-> User:阿邦是否杀死了霄霄？ 如何杀死的？



-> assistant:阿邦采用勒杀的方式杀死了霄霄，并且在霄霄死后奸淫了她的尸体。



-> User: 阿邦 女尸 艳尸



-> assistant:
172.16.0.62 - - [12/Sep/2023 10:34:50] "POST /inference HTTP/1.1" 200 334
172.16.0.62 - - [12/Sep/2023 10:34:57] "GET /reset-state HTTP/1.1" 200 35

-> 以下是两个人之间的对话， 包括交谈着包括 中国队长这部小说的读者和编辑，他们就小说中的内容进行讨论

-> User:阿邦是否杀死了霄霄？ 如何杀死的？



-> assistant:阿邦采用勒杀的方式杀死了霄霄，并且在霄霄死后奸淫了她的尸体。



-> User: 阿邦认为霄霄的尸体好看吗？



-> assistant:
172.16.0.62 - - [12/Sep/2023 10:35:10] "POST /inference HTTP/1.1" 200 376
[2023-09-12 10:44:32,334] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 42438
[2023-09-12 10:44:35,585] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 42438
[2023-09-12 10:44:37,822] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 10:44:42,560] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 10:44:43,885] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 10:44:43,909] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 10:44:45,787] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 10:44:47,094] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 10:44:47,094] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 10:44:47,094] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 10:44:47,094] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 10:44:47,094] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 10:44:48,920] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.7141611576080322 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.27it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.04s/it]100%|██████████| 1/1 [00:05<00:00,  5.04s/it]
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 28.91it/s]
total ['bonsai-extend-physic.jsonl', 'bonsai-extend.jsonl'] files  has 4563 items.
  0%|          | 0/4563 [00:00<?, ?it/s]  1%|▏         | 62/4563 [00:00<00:07, 614.53it/s]  3%|▎         | 124/4563 [00:00<00:07, 598.39it/s]  4%|▍         | 200/4563 [00:00<00:06, 668.73it/s]  6%|▌         | 267/4563 [00:00<00:06, 649.00it/s]  7%|▋         | 333/4563 [00:00<00:06, 643.49it/s]  9%|▊         | 398/4563 [00:00<00:06, 636.34it/s] 10%|█         | 466/4563 [00:00<00:06, 649.56it/s] 12%|█▏        | 532/4563 [00:00<00:06, 644.11it/s] 13%|█▎        | 606/4563 [00:00<00:05, 672.98it/s] 15%|█▍        | 683/4563 [00:01<00:05, 699.47it/s] 17%|█▋        | 754/4563 [00:01<00:06, 599.39it/s] 18%|█▊        | 817/4563 [00:01<00:06, 560.85it/s] 19%|█▉        | 876/4563 [00:01<00:06, 531.93it/s] 20%|██        | 931/4563 [00:01<00:07, 513.68it/s] 22%|██▏       | 984/4563 [00:01<00:07, 504.55it/s] 23%|██▎       | 1036/4563 [00:01<00:07, 494.77it/s] 24%|██▍       | 1086/4563 [00:01<00:07, 488.95it/s] 25%|██▍       | 1136/4563 [00:01<00:07, 489.25it/s] 26%|██▌       | 1188/4563 [00:02<00:06, 496.26it/s] 27%|██▋       | 1238/4563 [00:02<00:06, 490.38it/s] 28%|██▊       | 1288/4563 [00:02<00:06, 491.84it/s] 29%|██▉       | 1338/4563 [00:02<00:06, 492.44it/s] 30%|███       | 1388/4563 [00:02<00:06, 481.48it/s] 32%|███▏      | 1442/4563 [00:02<00:06, 497.98it/s] 33%|███▎      | 1497/4563 [00:02<00:05, 511.78it/s] 34%|███▍      | 1550/4563 [00:02<00:05, 515.97it/s] 35%|███▌      | 1604/4563 [00:02<00:05, 522.01it/s] 36%|███▋      | 1659/4563 [00:03<00:05, 529.40it/s] 38%|███▊      | 1714/4563 [00:03<00:05, 533.86it/s] 39%|███▊      | 1768/4563 [00:03<00:05, 531.12it/s] 40%|███▉      | 1823/4563 [00:03<00:05, 536.09it/s] 41%|████      | 1881/4563 [00:03<00:04, 547.94it/s] 42%|████▏     | 1936/4563 [00:03<00:04, 547.27it/s] 44%|████▎     | 1991/4563 [00:03<00:04, 545.90it/s] 45%|████▍     | 2046/4563 [00:03<00:04, 542.20it/s] 46%|████▌     | 2101/4563 [00:03<00:04, 543.91it/s] 47%|████▋     | 2156/4563 [00:03<00:04, 544.64it/s] 48%|████▊     | 2211/4563 [00:04<00:04, 545.76it/s] 50%|████▉     | 2266/4563 [00:04<00:04, 545.93it/s] 51%|█████     | 2334/4563 [00:04<00:03, 584.61it/s] 53%|█████▎    | 2403/4563 [00:04<00:03, 612.43it/s] 54%|█████▍    | 2481/4563 [00:04<00:03, 660.03it/s] 56%|█████▌    | 2560/4563 [00:04<00:02, 698.38it/s] 58%|█████▊    | 2634/4563 [00:04<00:02, 709.42it/s] 60%|█████▉    | 2732/4563 [00:04<00:02, 789.97it/s] 62%|██████▏   | 2816/4563 [00:04<00:02, 799.65it/s] 64%|██████▎   | 2899/4563 [00:04<00:02, 806.11it/s] 66%|██████▌   | 3001/4563 [00:05<00:01, 869.68it/s] 68%|██████▊   | 3089/4563 [00:05<00:01, 857.46it/s] 70%|██████▉   | 3175/4563 [00:05<00:01, 817.08it/s] 71%|███████▏  | 3258/4563 [00:05<00:01, 671.94it/s] 73%|███████▎  | 3330/4563 [00:05<00:02, 570.09it/s] 74%|███████▍  | 3393/4563 [00:05<00:02, 517.79it/s] 76%|███████▌  | 3449/4563 [00:05<00:02, 485.40it/s] 77%|███████▋  | 3501/4563 [00:06<00:02, 464.53it/s] 78%|███████▊  | 3550/4563 [00:06<00:02, 448.33it/s] 79%|███████▉  | 3596/4563 [00:06<00:02, 435.35it/s] 80%|███████▉  | 3641/4563 [00:06<00:02, 425.44it/s] 81%|████████  | 3684/4563 [00:06<00:02, 420.87it/s] 82%|████████▏ | 3727/4563 [00:06<00:01, 418.20it/s] 83%|████████▎ | 3769/4563 [00:06<00:01, 414.04it/s] 84%|████████▎ | 3811/4563 [00:06<00:01, 413.25it/s] 85%|████████▍ | 3856/4563 [00:06<00:01, 423.00it/s] 85%|████████▌ | 3899/4563 [00:07<00:01, 420.23it/s] 86%|████████▋ | 3942/4563 [00:07<00:01, 418.62it/s] 87%|████████▋ | 3984/4563 [00:07<00:01, 416.86it/s] 88%|████████▊ | 4026/4563 [00:07<00:01, 416.50it/s] 89%|████████▉ | 4068/4563 [00:07<00:01, 415.26it/s] 90%|█████████ | 4110/4563 [00:07<00:01, 413.40it/s] 91%|█████████ | 4152/4563 [00:07<00:00, 412.33it/s] 92%|█████████▏| 4194/4563 [00:07<00:00, 412.82it/s] 93%|█████████▎| 4236/4563 [00:07<00:00, 411.82it/s] 94%|█████████▍| 4278/4563 [00:07<00:00, 412.04it/s] 95%|█████████▍| 4320/4563 [00:08<00:00, 411.13it/s] 96%|█████████▌| 4362/4563 [00:08<00:00, 409.80it/s] 96%|█████████▋| 4403/4563 [00:08<00:00, 407.78it/s] 97%|█████████▋| 4445/4563 [00:08<00:00, 408.68it/s] 98%|█████████▊| 4487/4563 [00:08<00:00, 411.20it/s] 99%|█████████▉| 4529/4563 [00:08<00:00, 406.80it/s]100%|██████████| 4563/4563 [00:08<00:00, 529.28it/s]
[2023-09-12 10:45:43,958] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 10:45:43,958] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 10:45:43,958] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 10:45:46,459] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.2768163681030273 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 10:45:52,192] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 10:45:52,241] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 10:45:52,241] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 10:45:52,242] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 10:45:52,242] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 10:45:52,242] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 10:45:52,242] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 10:45:52,242] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 10:46:03,627] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 10:46:03,628] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 10:46:03,628] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 29.01 GB, percent = 7.7%
[2023-09-12 10:46:16,501] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 10:46:16,502] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 10:46:16,502] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 68.18 GB, percent = 18.0%
[2023-09-12 10:46:16,502] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 10:46:17,446] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 10:46:17,447] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 10:46:17,447] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 68.18 GB, percent = 18.0%
[2023-09-12 10:46:17,469] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 10:46:17,469] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 10:46:17,469] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f25db496460>
[2023-09-12 10:46:17,469] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 10:46:17,471] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 10:46:17,471] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 10:46:17,471] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 10:46:17,471] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 10:46:17,471] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 10:46:17,471] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 10:46:17,471] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f25db572100>
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 10:46:17,472] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 10:46:17,473] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 10:46:17,474] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 10:46:17,475] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

[2023-09-12 10:46:58,621] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-09-12 10:46:58,622] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-12 10:46:58,622] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-12 10:46:58,622] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-09-12 10:46:58,622] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
[2023-09-12 10:47:02,581] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1

-> loss:4.05859375 remain: 4147723 
[2023-09-12 10:47:04,107] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768

-> loss:3.9609375 remain: 4147251 

-> loss:4.0703125 remain: 4146779 

-> loss:2.515625 remain: 4146307 

-> loss:2.912109375 remain: 4145835 

-> loss:2.83984375 remain: 4145363 

-> loss:2.841796875 remain: 4144891 

-> loss:2.640625 remain: 4144419 

-> loss:2.6171875 remain: 4143947 
[2023-09-12 10:47:48,044] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 10:47:48,045] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.1822244642615961, CurrSamplesPerSec=0.19127100402656932, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.462890625 remain: 4143475 

-> loss:2.400390625 remain: 4143003 

-> loss:2.53515625 remain: 4142531 

-> loss:2.548828125 remain: 4142059 

-> loss:2.275390625 remain: 4141587 

-> loss:2.1171875 remain: 4141115 

-> loss:2.314453125 remain: 4140643 
172.16.0.62 - - [12/Sep/2023 10:48:21] "POST /train HTTP/1.1" 200 201

-> loss:1.4453125 remain: 4140643 

-> loss:1.5302734375 remain: 4140643 

-> loss:0.82177734375 remain: 4140643 
[2023-09-12 10:49:40,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 10:49:40,100] [INFO] [timer.py:260:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=0.18595347094772952, CurrSamplesPerSec=0.19796533384546974, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:0.796875 remain: 4140643 

-> loss:0.28857421875 remain: 4140643 

-> loss:0.218505859375 remain: 4140643 

-> loss:0.04852294921875 remain: 4140643 

-> loss:0.041351318359375 remain: 4140643 

-> loss:0.0194091796875 remain: 4140643 

-> loss:0.006328582763671875 remain: 4140643 

-> loss:0.006732940673828125 remain: 4140643 

-> loss:0.002323150634765625 remain: 4140643 

-> loss:0.001270294189453125 remain: 4140643 
[2023-09-12 10:50:30,573] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 10:50:30,574] [INFO] [timer.py:260:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=0.19015440021476085, CurrSamplesPerSec=0.19839686705142653, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:0.00240325927734375 remain: 4140643 

-> loss:0.0006837844848632812 remain: 4140643 

-> loss:0.0006399154663085938 remain: 4140643 
[2023-09-12 10:50:43,242] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 44337
[2023-09-12 10:50:44,640] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 44337
[2023-09-12 10:50:48,494] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 11:19:47,321] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 11:19:48,649] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 11:19:48,673] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 11:19:50,531] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 11:19:51,845] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 11:19:51,845] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 11:19:51,845] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 11:19:51,845] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 11:19:51,845] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 11:19:53,710] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 11:19:55,296] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 45785
[2023-09-12 11:19:55,338] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 11:20:01,743] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 11:20:03,079] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 11:20:03,103] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 11:20:04,907] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 11:20:06,198] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 11:20:06,198] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 11:20:06,198] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 11:20:06,198] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 11:20:06,198] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 11:20:08,023] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.7234933376312256 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.39it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.02s/it]100%|██████████| 1/1 [00:05<00:00,  5.02s/it]
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 29.02it/s]
total ['bonsai-extend-physic.jsonl', 'bonsai-extend.jsonl'] files  has 4563 items.
  0%|          | 0/4563 [00:00<?, ?it/s]  1%|▏         | 60/4563 [00:00<00:07, 591.33it/s]  3%|▎         | 122/4563 [00:00<00:07, 607.11it/s]  4%|▍         | 198/4563 [00:00<00:06, 672.28it/s]  6%|▌         | 266/4563 [00:00<00:06, 649.73it/s]  7%|▋         | 332/4563 [00:00<00:06, 646.16it/s]  9%|▊         | 397/4563 [00:00<00:06, 640.22it/s] 10%|█         | 464/4563 [00:00<00:06, 648.04it/s] 12%|█▏        | 529/4563 [00:00<00:06, 644.65it/s] 13%|█▎        | 603/4563 [00:00<00:05, 673.31it/s] 15%|█▍        | 681/4563 [00:01<00:05, 703.75it/s] 16%|█▋        | 752/4563 [00:01<00:06, 603.48it/s] 18%|█▊        | 815/4563 [00:01<00:06, 563.90it/s] 19%|█▉        | 874/4563 [00:01<00:06, 533.92it/s] 20%|██        | 929/4563 [00:01<00:07, 516.45it/s] 22%|██▏       | 982/4563 [00:01<00:07, 503.06it/s] 23%|██▎       | 1033/4563 [00:01<00:07, 495.94it/s] 24%|██▎       | 1083/4563 [00:01<00:07, 489.90it/s] 25%|██▍       | 1133/4563 [00:01<00:07, 486.11it/s] 26%|██▌       | 1184/4563 [00:02<00:06, 492.16it/s] 27%|██▋       | 1234/4563 [00:02<00:06, 486.10it/s] 28%|██▊       | 1284/4563 [00:02<00:06, 487.99it/s] 29%|██▉       | 1333/4563 [00:02<00:06, 488.05it/s] 30%|███       | 1382/4563 [00:02<00:06, 477.38it/s] 31%|███▏      | 1435/4563 [00:02<00:06, 489.70it/s] 33%|███▎      | 1489/4563 [00:02<00:06, 503.62it/s] 34%|███▍      | 1542/4563 [00:02<00:05, 511.29it/s] 35%|███▍      | 1595/4563 [00:02<00:05, 515.06it/s] 36%|███▌      | 1650/4563 [00:03<00:05, 523.16it/s] 37%|███▋      | 1704/4563 [00:03<00:05, 527.14it/s] 39%|███▊      | 1757/4563 [00:03<00:05, 526.58it/s] 40%|███▉      | 1810/4563 [00:03<00:05, 527.56it/s] 41%|████      | 1867/4563 [00:03<00:05, 537.76it/s] 42%|████▏     | 1923/4563 [00:03<00:04, 541.62it/s] 43%|████▎     | 1978/4563 [00:03<00:04, 541.07it/s] 45%|████▍     | 2033/4563 [00:03<00:04, 536.75it/s] 46%|████▌     | 2087/4563 [00:03<00:04, 537.04it/s] 47%|████▋     | 2141/4563 [00:03<00:04, 537.81it/s] 48%|████▊     | 2195/4563 [00:04<00:04, 535.41it/s] 49%|████▉     | 2252/4563 [00:04<00:04, 544.35it/s] 51%|█████     | 2317/4563 [00:04<00:03, 570.61it/s] 52%|█████▏    | 2381/4563 [00:04<00:03, 589.72it/s] 54%|█████▎    | 2452/4563 [00:04<00:03, 622.70it/s] 56%|█████▌    | 2534/4563 [00:04<00:02, 678.73it/s] 57%|█████▋    | 2606/4563 [00:04<00:02, 690.75it/s] 59%|█████▉    | 2695/4563 [00:04<00:02, 749.38it/s] 61%|██████    | 2781/4563 [00:04<00:02, 782.24it/s] 63%|██████▎   | 2871/4563 [00:04<00:02, 816.84it/s] 65%|██████▍   | 2962/4563 [00:05<00:01, 843.60it/s] 67%|██████▋   | 3048/4563 [00:05<00:01, 847.44it/s] 69%|██████▊   | 3133/4563 [00:05<00:01, 825.00it/s] 70%|███████   | 3216/4563 [00:05<00:01, 761.25it/s] 72%|███████▏  | 3294/4563 [00:05<00:02, 606.02it/s] 74%|███████▎  | 3360/4563 [00:05<00:02, 534.89it/s] 75%|███████▍  | 3419/4563 [00:05<00:02, 494.33it/s] 76%|███████▌  | 3472/4563 [00:06<00:02, 468.08it/s] 77%|███████▋  | 3521/4563 [00:06<00:02, 450.24it/s] 78%|███████▊  | 3568/4563 [00:06<00:02, 437.70it/s] 79%|███████▉  | 3613/4563 [00:06<00:02, 424.84it/s] 80%|████████  | 3656/4563 [00:06<00:02, 418.55it/s] 81%|████████  | 3699/4563 [00:06<00:02, 415.17it/s] 82%|████████▏ | 3741/4563 [00:06<00:01, 411.48it/s] 83%|████████▎ | 3783/4563 [00:06<00:01, 409.06it/s] 84%|████████▍ | 3824/4563 [00:06<00:01, 408.64it/s] 85%|████████▍ | 3869/4563 [00:06<00:01, 417.96it/s] 86%|████████▌ | 3911/4563 [00:07<00:01, 416.11it/s] 87%|████████▋ | 3953/4563 [00:07<00:01, 414.01it/s] 88%|████████▊ | 3995/4563 [00:07<00:01, 413.36it/s] 88%|████████▊ | 4037/4563 [00:07<00:01, 412.01it/s] 89%|████████▉ | 4079/4563 [00:07<00:01, 409.99it/s] 90%|█████████ | 4121/4563 [00:07<00:01, 408.88it/s] 91%|█████████ | 4162/4563 [00:07<00:00, 408.26it/s] 92%|█████████▏| 4203/4563 [00:07<00:00, 408.73it/s] 93%|█████████▎| 4244/4563 [00:07<00:00, 407.85it/s] 94%|█████████▍| 4285/4563 [00:08<00:00, 407.65it/s] 95%|█████████▍| 4326/4563 [00:08<00:00, 407.92it/s] 96%|█████████▌| 4367/4563 [00:08<00:00, 406.83it/s] 97%|█████████▋| 4408/4563 [00:08<00:00, 404.47it/s] 98%|█████████▊| 4449/4563 [00:08<00:00, 402.05it/s] 98%|█████████▊| 4490/4563 [00:08<00:00, 404.11it/s] 99%|█████████▉| 4531/4563 [00:08<00:00, 404.19it/s]100%|██████████| 4563/4563 [00:08<00:00, 524.98it/s]
[2023-09-12 11:21:03,144] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 11:21:03,145] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 11:21:03,145] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 11:21:05,598] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.239081621170044 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 11:21:11,276] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 11:21:11,325] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 11:21:11,325] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 11:21:11,325] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 11:21:11,325] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 11:21:11,325] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 11:21:11,325] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 11:21:11,325] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 11:21:22,886] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 11:21:22,886] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 11:21:22,887] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 29.31 GB, percent = 7.8%
[2023-09-12 11:21:36,242] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 11:21:36,242] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 11:21:36,243] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 68.5 GB, percent = 18.1%
[2023-09-12 11:21:36,243] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 11:21:37,289] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 11:21:37,290] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 11:21:37,290] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 68.5 GB, percent = 18.1%
[2023-09-12 11:21:37,313] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 11:21:37,313] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 11:21:37,314] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fe9ba097460>
[2023-09-12 11:21:37,314] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 11:21:37,315] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 11:21:37,315] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 11:21:37,315] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 11:21:37,315] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 11:21:37,315] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 11:21:37,316] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 11:21:37,316] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 11:21:37,316] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 11:21:37,316] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 11:21:37,316] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 11:21:37,316] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe9ba171100>
[2023-09-12 11:21:37,316] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 11:21:37,316] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 11:21:37,316] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 11:21:37,316] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 11:21:37,316] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 11:21:37,316] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 11:21:37,316] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 11:21:37,316] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 11:21:37,316] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 11:21:37,316] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 11:21:37,316] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 11:21:37,316] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 11:21:37,317] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 11:21:37,318] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 11:21:37,319] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

[2023-09-12 11:21:42,229] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-09-12 11:21:42,229] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-12 11:21:42,229] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-12 11:21:42,229] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-09-12 11:21:42,229] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
[2023-09-12 11:21:46,099] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1

-> loss:4.05859375 remain: 4147723 
[2023-09-12 11:21:47,586] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768

-> loss:3.9609375 remain: 4147251 

-> loss:4.0703125 remain: 4146779 

-> loss:2.515625 remain: 4146307 

-> loss:2.912109375 remain: 4145835 

-> loss:2.83984375 remain: 4145363 

-> loss:2.841796875 remain: 4144891 

-> loss:2.640625 remain: 4144419 

-> loss:2.6171875 remain: 4143947 
[2023-09-12 11:22:30,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 11:22:30,615] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.18605800181779333, CurrSamplesPerSec=0.19945200799709395, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.462890625 remain: 4143475 

-> loss:2.400390625 remain: 4143003 

-> loss:2.53515625 remain: 4142531 

-> loss:2.548828125 remain: 4142059 

-> loss:2.275390625 remain: 4141587 

-> loss:2.1171875 remain: 4141115 

-> loss:2.314453125 remain: 4140643 
172.16.0.62 - - [12/Sep/2023 11:23:04] "POST /train HTTP/1.1" 200 201

-> loss:2.37890625 remain: 4140643 

-> loss:1.9716796875 remain: 4140643 

-> loss:1.49609375 remain: 4140643 
[2023-09-12 11:23:29,933] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 11:23:29,934] [INFO] [timer.py:260:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=0.18827040867073963, CurrSamplesPerSec=0.20091374158531103, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:1.060546875 remain: 4140643 

-> loss:0.6796875 remain: 4140643 

-> loss:0.36474609375 remain: 4140643 

-> loss:0.11553955078125 remain: 4140643 

-> loss:0.01299285888671875 remain: 4140643 

-> loss:0.007244110107421875 remain: 4140643 
[2023-09-12 11:23:58,118] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 45987
[2023-09-12 11:23:59,020] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 45987
[2023-09-12 11:24:03,369] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 11:24:26,776] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 11:24:28,100] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 11:24:28,124] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 11:24:29,983] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 11:24:31,299] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 11:24:31,299] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 11:24:31,299] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 11:24:31,299] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 11:24:31,299] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 11:24:33,135] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.6942765712738037 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 13.27it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:04<00:00,  4.71s/it]100%|██████████| 1/1 [00:04<00:00,  4.71s/it]
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 29.10it/s]
total ['bonsai-extend-physic.jsonl', 'bonsai-extend.jsonl'] files  has 4563 items.
  0%|          | 0/4563 [00:00<?, ?it/s]  1%|▏         | 63/4563 [00:00<00:07, 623.35it/s]  3%|▎         | 126/4563 [00:00<00:07, 626.92it/s]  4%|▍         | 202/4563 [00:00<00:06, 681.38it/s]  6%|▌         | 271/4563 [00:00<00:06, 670.08it/s]  7%|▋         | 339/4563 [00:00<00:06, 658.05it/s]  9%|▉         | 405/4563 [00:00<00:06, 631.35it/s] 10%|█         | 477/4563 [00:00<00:06, 656.43it/s] 12%|█▏        | 544/4563 [00:00<00:06, 658.50it/s] 14%|█▎        | 621/4563 [00:00<00:05, 692.01it/s] 15%|█▌        | 697/4563 [00:01<00:05, 709.23it/s] 17%|█▋        | 769/4563 [00:01<00:06, 590.10it/s] 18%|█▊        | 832/4563 [00:01<00:06, 559.63it/s] 20%|█▉        | 891/4563 [00:01<00:06, 535.89it/s] 21%|██        | 947/4563 [00:01<00:07, 515.20it/s] 22%|██▏       | 1000/4563 [00:01<00:06, 510.71it/s] 23%|██▎       | 1052/4563 [00:01<00:06, 504.15it/s] 24%|██▍       | 1103/4563 [00:01<00:06, 496.36it/s] 25%|██▌       | 1155/4563 [00:01<00:06, 501.17it/s] 26%|██▋       | 1206/4563 [00:02<00:06, 500.38it/s] 28%|██▊       | 1257/4563 [00:02<00:06, 492.72it/s] 29%|██▊       | 1307/4563 [00:02<00:06, 494.21it/s] 30%|██▉       | 1357/4563 [00:02<00:06, 487.05it/s] 31%|███       | 1406/4563 [00:02<00:06, 486.66it/s] 32%|███▏      | 1461/4563 [00:02<00:06, 503.35it/s] 33%|███▎      | 1516/4563 [00:02<00:05, 515.86it/s] 34%|███▍      | 1569/4563 [00:02<00:05, 517.97it/s] 36%|███▌      | 1624/4563 [00:02<00:05, 524.14it/s] 37%|███▋      | 1680/4563 [00:03<00:05, 531.64it/s] 38%|███▊      | 1734/4563 [00:03<00:05, 533.02it/s] 39%|███▉      | 1788/4563 [00:03<00:05, 533.64it/s] 40%|████      | 1846/4563 [00:03<00:04, 545.75it/s] 42%|████▏     | 1902/4563 [00:03<00:04, 548.93it/s] 43%|████▎     | 1958/4563 [00:03<00:04, 550.58it/s] 44%|████▍     | 2014/4563 [00:03<00:04, 545.22it/s] 45%|████▌     | 2069/4563 [00:03<00:04, 546.13it/s] 47%|████▋     | 2124/4563 [00:03<00:04, 545.38it/s] 48%|████▊     | 2179/4563 [00:03<00:04, 543.36it/s] 49%|████▉     | 2236/4563 [00:04<00:04, 551.13it/s] 50%|█████     | 2296/4563 [00:04<00:04, 564.49it/s] 52%|█████▏    | 2365/4563 [00:04<00:03, 600.69it/s] 53%|█████▎    | 2432/4563 [00:04<00:03, 619.90it/s] 55%|█████▌    | 2514/4563 [00:04<00:03, 678.12it/s] 57%|█████▋    | 2592/4563 [00:04<00:02, 708.40it/s] 59%|█████▊    | 2676/4563 [00:04<00:02, 746.91it/s] 61%|██████    | 2775/4563 [00:04<00:02, 815.64it/s] 63%|██████▎   | 2858/4563 [00:04<00:02, 816.09it/s] 65%|██████▍   | 2949/4563 [00:04<00:01, 842.64it/s] 66%|██████▋   | 3034/4563 [00:05<00:01, 836.92it/s] 68%|██████▊   | 3121/4563 [00:05<00:01, 844.60it/s] 70%|███████   | 3206/4563 [00:05<00:01, 791.15it/s] 72%|███████▏  | 3286/4563 [00:05<00:02, 623.09it/s] 74%|███████▎  | 3355/4563 [00:05<00:02, 545.12it/s] 75%|███████▍  | 3415/4563 [00:05<00:02, 503.22it/s] 76%|███████▌  | 3470/4563 [00:05<00:02, 475.74it/s] 77%|███████▋  | 3521/4563 [00:06<00:02, 457.32it/s] 78%|███████▊  | 3569/4563 [00:06<00:02, 444.91it/s] 79%|███████▉  | 3615/4563 [00:06<00:02, 431.45it/s] 80%|████████  | 3659/4563 [00:06<00:02, 425.59it/s] 81%|████████  | 3702/4563 [00:06<00:02, 422.10it/s] 82%|████████▏ | 3745/4563 [00:06<00:01, 417.76it/s] 83%|████████▎ | 3787/4563 [00:06<00:01, 415.88it/s] 84%|████████▍ | 3829/4563 [00:06<00:01, 415.29it/s] 85%|████████▍ | 3874/4563 [00:06<00:01, 424.29it/s] 86%|████████▌ | 3917/4563 [00:06<00:01, 421.94it/s] 87%|████████▋ | 3960/4563 [00:07<00:01, 420.15it/s] 88%|████████▊ | 4003/4563 [00:07<00:01, 419.88it/s] 89%|████████▊ | 4046/4563 [00:07<00:01, 418.00it/s] 90%|████████▉ | 4088/4563 [00:07<00:01, 415.67it/s] 91%|█████████ | 4130/4563 [00:07<00:01, 414.03it/s] 91%|█████████▏| 4172/4563 [00:07<00:00, 413.44it/s] 92%|█████████▏| 4214/4563 [00:07<00:00, 414.44it/s] 93%|█████████▎| 4256/4563 [00:07<00:00, 412.94it/s] 94%|█████████▍| 4298/4563 [00:07<00:00, 413.71it/s] 95%|█████████▌| 4340/4563 [00:08<00:00, 412.94it/s] 96%|█████████▌| 4382/4563 [00:08<00:00, 411.87it/s] 97%|█████████▋| 4424/4563 [00:08<00:00, 410.33it/s] 98%|█████████▊| 4466/4563 [00:08<00:00, 411.65it/s] 99%|█████████▉| 4508/4563 [00:08<00:00, 413.51it/s]100%|█████████▉| 4550/4563 [00:08<00:00, 411.57it/s]100%|██████████| 4563/4563 [00:08<00:00, 532.73it/s]
[2023-09-12 11:25:28,689] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 11:25:28,690] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 11:25:28,690] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 11:25:31,238] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.23209810256958 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 11:25:36,892] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 11:25:36,941] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 11:25:36,941] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 11:25:36,941] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 11:25:36,941] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 11:25:36,941] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 11:25:36,941] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 11:25:36,941] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 11:25:48,910] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 11:25:48,911] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 11:25:48,911] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 29.37 GB, percent = 7.8%
[2023-09-12 11:26:02,683] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 11:26:02,684] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 11:26:02,684] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 68.62 GB, percent = 18.2%
[2023-09-12 11:26:02,684] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 11:26:03,836] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 11:26:03,837] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 11:26:03,837] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 68.62 GB, percent = 18.2%
[2023-09-12 11:26:03,859] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 11:26:03,860] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 11:26:03,860] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f50c0fae430>
[2023-09-12 11:26:03,860] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 11:26:03,861] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 11:26:03,861] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 11:26:03,862] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 11:26:03,862] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 11:26:03,862] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 11:26:03,862] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 11:26:03,862] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 11:26:03,862] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 11:26:03,862] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 11:26:03,862] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 11:26:03,862] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f50c32f5520>
[2023-09-12 11:26:03,862] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 11:26:03,862] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 11:26:03,862] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 11:26:03,863] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 11:26:03,864] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 11:26:03,865] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 11:26:03,865] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 11:26:03,865] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 11:26:03,865] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 11:26:03,865] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 11:26:03,865] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 11:26:03,865] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 11:26:03,865] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 11:26:03,865] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 11:26:03,865] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 11:26:03,865] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

[2023-09-12 11:26:06,975] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-09-12 11:26:06,975] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-12 11:26:06,975] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-12 11:26:06,975] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-09-12 11:26:06,975] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
[2023-09-12 11:26:10,865] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1

-> loss:4.05859375 remain: 4147723 
[2023-09-12 11:26:12,361] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768

-> loss:3.9609375 remain: 4147251 

-> loss:4.0703125 remain: 4146779 

-> loss:2.515625 remain: 4146307 

-> loss:2.912109375 remain: 4145835 

-> loss:2.83984375 remain: 4145363 

-> loss:2.841796875 remain: 4144891 

-> loss:2.640625 remain: 4144419 

-> loss:2.6171875 remain: 4143947 
[2023-09-12 11:26:57,599] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 11:26:57,600] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.17699409109295008, CurrSamplesPerSec=0.18254700331685714, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.462890625 remain: 4143475 

-> loss:2.400390625 remain: 4143003 

-> loss:2.53515625 remain: 4142531 

-> loss:2.548828125 remain: 4142059 

-> loss:2.275390625 remain: 4141587 

-> loss:2.1171875 remain: 4141115 

-> loss:2.314453125 remain: 4140643 
172.16.0.62 - - [12/Sep/2023 11:27:32] "POST /train HTTP/1.1" 200 201

-> loss:2.37890625 remain: 4140643 

-> loss:2.33203125 remain: 4140643 

-> loss:2.01953125 remain: 4140643 
[2023-09-12 11:28:04,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 11:28:04,822] [INFO] [timer.py:260:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=0.17920400394362654, CurrSamplesPerSec=0.18725078844770482, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.373046875 remain: 4140643 

-> loss:1.8466796875 remain: 4140643 

-> loss:2.0390625 remain: 4140643 

-> loss:1.9267578125 remain: 4140643 

-> loss:2.515625 remain: 4140643 

-> loss:2.435546875 remain: 4140643 

-> loss:2.6484375 remain: 4140643 

-> loss:2.32421875 remain: 4140643 

-> loss:1.9228515625 remain: 4140643 

-> loss:2.4609375 remain: 4140643 
[2023-09-12 11:28:58,250] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 11:28:58,251] [INFO] [timer.py:260:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=0.18199671713694848, CurrSamplesPerSec=0.19166980419062918, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.125 remain: 4140643 

-> loss:1.953125 remain: 4140643 

-> loss:2.484375 remain: 4140643 

-> loss:2.3359375 remain: 4140643 

-> loss:2.02734375 remain: 4140643 

-> loss:2.619140625 remain: 4140643 

-> loss:2.72265625 remain: 4140643 

-> loss:2.310546875 remain: 4140643 

-> loss:1.8798828125 remain: 4140643 
[2023-09-12 11:29:42,889] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 46451
[2023-09-12 11:29:44,314] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 46451
[2023-09-12 11:29:47,928] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 11:29:50,081] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 11:29:51,468] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 11:29:51,492] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 11:29:53,363] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 11:29:54,639] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 11:29:54,639] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 11:29:54,639] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 11:29:54,639] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 11:29:54,639] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 11:29:56,473] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.6917834281921387 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.73it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:04<00:00,  4.83s/it]100%|██████████| 1/1 [00:04<00:00,  4.83s/it]
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 29.35it/s]
total ['bonsai-extend-physic.jsonl', 'bonsai-extend.jsonl'] files  has 4563 items.
  0%|          | 0/4563 [00:00<?, ?it/s]  1%|▏         | 62/4563 [00:00<00:07, 616.57it/s]  3%|▎         | 124/4563 [00:00<00:07, 600.06it/s]  4%|▍         | 200/4563 [00:00<00:06, 669.84it/s]  6%|▌         | 268/4563 [00:00<00:06, 650.56it/s]  7%|▋         | 334/4563 [00:00<00:06, 642.75it/s]  9%|▊         | 399/4563 [00:00<00:06, 636.06it/s] 10%|█         | 468/4563 [00:00<00:06, 651.35it/s] 12%|█▏        | 534/4563 [00:00<00:06, 645.72it/s] 13%|█▎        | 610/4563 [00:00<00:05, 678.95it/s] 15%|█▌        | 686/4563 [00:01<00:05, 701.94it/s] 17%|█▋        | 757/4563 [00:01<00:06, 595.94it/s] 18%|█▊        | 820/4563 [00:01<00:06, 559.11it/s] 19%|█▉        | 879/4563 [00:01<00:06, 532.38it/s] 20%|██        | 934/4563 [00:01<00:07, 514.11it/s] 22%|██▏       | 987/4563 [00:01<00:07, 505.35it/s] 23%|██▎       | 1039/4563 [00:01<00:07, 495.45it/s] 24%|██▍       | 1089/4563 [00:01<00:07, 489.31it/s] 25%|██▍       | 1139/4563 [00:01<00:06, 490.30it/s] 26%|██▌       | 1190/4563 [00:02<00:06, 495.48it/s] 27%|██▋       | 1240/4563 [00:02<00:06, 491.30it/s] 28%|██▊       | 1290/4563 [00:02<00:06, 492.24it/s] 29%|██▉       | 1340/4563 [00:02<00:06, 490.81it/s] 30%|███       | 1390/4563 [00:02<00:06, 480.89it/s] 32%|███▏      | 1445/4563 [00:02<00:06, 499.18it/s] 33%|███▎      | 1500/4563 [00:02<00:05, 512.06it/s] 34%|███▍      | 1553/4563 [00:02<00:05, 516.10it/s] 35%|███▌      | 1607/4563 [00:02<00:05, 522.41it/s] 36%|███▋      | 1662/4563 [00:03<00:05, 529.85it/s] 38%|███▊      | 1717/4563 [00:03<00:05, 534.49it/s] 39%|███▉      | 1771/4563 [00:03<00:05, 531.56it/s] 40%|████      | 1826/4563 [00:03<00:05, 536.87it/s] 41%|████▏     | 1884/4563 [00:03<00:04, 548.48it/s] 42%|████▏     | 1939/4563 [00:03<00:04, 544.10it/s] 44%|████▎     | 1994/4563 [00:03<00:04, 542.34it/s] 45%|████▍     | 2049/4563 [00:03<00:04, 541.12it/s] 46%|████▌     | 2104/4563 [00:03<00:04, 540.93it/s] 47%|████▋     | 2159/4563 [00:03<00:04, 543.21it/s] 49%|████▊     | 2215/4563 [00:04<00:04, 545.19it/s] 50%|████▉     | 2270/4563 [00:04<00:04, 542.52it/s] 51%|█████     | 2338/4563 [00:04<00:03, 581.61it/s] 53%|█████▎    | 2408/4563 [00:04<00:03, 615.84it/s] 55%|█████▍    | 2487/4563 [00:04<00:03, 665.93it/s] 56%|█████▌    | 2565/4563 [00:04<00:02, 697.49it/s] 58%|█████▊    | 2640/4563 [00:04<00:02, 707.45it/s] 60%|██████    | 2742/4563 [00:04<00:02, 798.09it/s] 62%|██████▏   | 2824/4563 [00:04<00:02, 802.94it/s] 64%|██████▎   | 2907/4563 [00:04<00:02, 810.41it/s] 66%|██████▌   | 3007/4563 [00:05<00:01, 865.80it/s] 68%|██████▊   | 3094/4563 [00:05<00:01, 855.78it/s] 70%|██████▉   | 3180/4563 [00:05<00:01, 810.61it/s] 71%|███████▏  | 3262/4563 [00:05<00:01, 666.09it/s] 73%|███████▎  | 3333/4563 [00:05<00:02, 568.27it/s] 74%|███████▍  | 3395/4563 [00:05<00:02, 516.92it/s] 76%|███████▌  | 3451/4563 [00:05<00:02, 484.33it/s] 77%|███████▋  | 3502/4563 [00:06<00:02, 463.92it/s] 78%|███████▊  | 3550/4563 [00:06<00:02, 448.49it/s] 79%|███████▉  | 3596/4563 [00:06<00:02, 435.12it/s] 80%|███████▉  | 3640/4563 [00:06<00:02, 425.36it/s] 81%|████████  | 3683/4563 [00:06<00:02, 420.93it/s] 82%|████████▏ | 3726/4563 [00:06<00:01, 418.87it/s] 83%|████████▎ | 3768/4563 [00:06<00:01, 414.80it/s] 83%|████████▎ | 3810/4563 [00:06<00:01, 413.35it/s] 84%|████████▍ | 3855/4563 [00:06<00:01, 423.60it/s] 85%|████████▌ | 3898/4563 [00:06<00:01, 420.77it/s] 86%|████████▋ | 3941/4563 [00:07<00:01, 419.86it/s] 87%|████████▋ | 3984/4563 [00:07<00:01, 417.64it/s] 88%|████████▊ | 4026/4563 [00:07<00:01, 417.34it/s] 89%|████████▉ | 4068/4563 [00:07<00:01, 416.08it/s] 90%|█████████ | 4110/4563 [00:07<00:01, 413.97it/s] 91%|█████████ | 4152/4563 [00:07<00:00, 412.89it/s] 92%|█████████▏| 4194/4563 [00:07<00:00, 413.29it/s] 93%|█████████▎| 4236/4563 [00:07<00:00, 412.25it/s] 94%|█████████▍| 4278/4563 [00:07<00:00, 412.49it/s] 95%|█████████▍| 4320/4563 [00:08<00:00, 412.53it/s] 96%|█████████▌| 4362/4563 [00:08<00:00, 411.88it/s] 97%|█████████▋| 4404/4563 [00:08<00:00, 409.48it/s] 97%|█████████▋| 4446/4563 [00:08<00:00, 410.00it/s] 98%|█████████▊| 4488/4563 [00:08<00:00, 412.05it/s] 99%|█████████▉| 4530/4563 [00:08<00:00, 407.87it/s]100%|██████████| 4563/4563 [00:08<00:00, 529.67it/s]
[2023-09-12 11:30:51,241] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 11:30:51,242] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 11:30:51,242] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 11:30:53,689] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.2757084369659424 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 11:30:59,442] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 11:30:59,491] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 11:30:59,491] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 11:30:59,491] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 11:30:59,491] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 11:30:59,491] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 11:30:59,491] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 11:30:59,491] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 11:31:11,390] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 11:31:11,391] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 11:31:11,391] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 29.35 GB, percent = 7.8%
[2023-09-12 11:31:24,424] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 11:31:24,425] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 11:31:24,425] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 68.58 GB, percent = 18.2%
[2023-09-12 11:31:24,425] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 11:31:25,377] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 11:31:25,378] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 11:31:25,378] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 68.58 GB, percent = 18.2%
[2023-09-12 11:31:25,400] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 11:31:25,401] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 11:31:25,401] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fbddb655460>
[2023-09-12 11:31:25,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 11:31:25,402] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 11:31:25,402] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 11:31:25,402] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 11:31:25,402] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 11:31:25,403] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 11:31:25,403] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 11:31:25,403] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 11:31:25,403] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 11:31:25,403] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 11:31:25,403] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 11:31:25,403] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbddb735100>
[2023-09-12 11:31:25,403] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 11:31:25,403] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 11:31:25,403] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 11:31:25,403] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 11:31:25,403] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 11:31:25,403] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 11:31:25,403] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 11:31:25,403] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 11:31:25,403] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 11:31:25,403] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 11:31:25,403] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 11:31:25,403] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 11:31:25,404] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 11:31:25,405] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 11:31:25,406] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 11:31:25,406] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 11:31:25,406] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

[2023-09-12 12:07:01,695] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-09-12 12:07:01,700] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-12 12:07:01,700] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-12 12:07:01,700] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-09-12 12:07:01,700] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
[2023-09-12 12:07:05,570] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1

-> loss:4.05859375 remain: 4147723 
[2023-09-12 12:07:07,064] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768

-> loss:3.9609375 remain: 4147251 

-> loss:4.0703125 remain: 4146779 

-> loss:2.515625 remain: 4146307 

-> loss:2.912109375 remain: 4145835 

-> loss:2.83984375 remain: 4145363 

-> loss:2.841796875 remain: 4144891 

-> loss:2.640625 remain: 4144419 

-> loss:2.6171875 remain: 4143947 
[2023-09-12 12:07:51,456] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:07:51,457] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.18036655404464721, CurrSamplesPerSec=0.17858510979782558, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.462890625 remain: 4143475 

-> loss:2.400390625 remain: 4143003 

-> loss:2.53515625 remain: 4142531 

-> loss:2.548828125 remain: 4142059 

-> loss:2.275390625 remain: 4141587 

-> loss:2.1171875 remain: 4141115 

-> loss:2.314453125 remain: 4140643 
172.16.0.62 - - [12/Sep/2023 12:08:25] "POST /train HTTP/1.1" 200 201

-> loss:2.37890625 remain: 4140643 

-> loss:2.33203125 remain: 4140643 

-> loss:2.01953125 remain: 4140643 
[2023-09-12 12:09:23,164] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:09:23,165] [INFO] [timer.py:260:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=0.18567917884221796, CurrSamplesPerSec=0.1974581901759289, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.373046875 remain: 4140643 

-> loss:1.8466796875 remain: 4140643 

-> loss:2.0390625 remain: 4140643 

-> loss:1.9267578125 remain: 4140643 

-> loss:2.515625 remain: 4140643 

-> loss:2.435546875 remain: 4140643 

-> loss:2.6484375 remain: 4140643 

-> loss:2.32421875 remain: 4140643 

-> loss:1.9228515625 remain: 4140643 

-> loss:2.4609375 remain: 4140643 
[2023-09-12 12:10:14,039] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:10:14,040] [INFO] [timer.py:260:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=0.18945283345214495, CurrSamplesPerSec=0.19940543074655473, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.125 remain: 4140643 

-> loss:1.953125 remain: 4140643 

-> loss:2.484375 remain: 4140643 

-> loss:2.3359375 remain: 4140643 

-> loss:2.02734375 remain: 4140643 

-> loss:2.619140625 remain: 4140643 

-> loss:2.72265625 remain: 4140643 

-> loss:2.310546875 remain: 4140643 

-> loss:1.8798828125 remain: 4140643 

-> loss:2.564453125 remain: 4140643 
[2023-09-12 12:11:04,766] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:11:04,767] [INFO] [timer.py:260:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=0.1914389328920972, CurrSamplesPerSec=0.19889430558127438, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.216796875 remain: 4140643 

-> loss:1.9462890625 remain: 4140643 

-> loss:2.779296875 remain: 4140643 

-> loss:2.84765625 remain: 4140643 

-> loss:2.517578125 remain: 4140643 

-> loss:2.26953125 remain: 4140643 

-> loss:2.578125 remain: 4140643 

-> loss:2.369140625 remain: 4140643 

-> loss:2.4609375 remain: 4140643 

-> loss:2.33203125 remain: 4140643 
[2023-09-12 12:11:55,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:11:55,228] [INFO] [timer.py:260:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=0.19282266604303047, CurrSamplesPerSec=0.20193197436641575, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.009765625 remain: 4140643 

-> loss:2.15234375 remain: 4140643 

-> loss:1.71875 remain: 4140643 

-> loss:2.078125 remain: 4140643 

-> loss:1.9658203125 remain: 4140643 

-> loss:2.19140625 remain: 4140643 

-> loss:1.92578125 remain: 4140643 

-> loss:2.384765625 remain: 4140643 

-> loss:2.451171875 remain: 4140643 

-> loss:2.076171875 remain: 4140643 
[2023-09-12 12:12:46,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:12:46,068] [INFO] [timer.py:260:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=0.1934944826257723, CurrSamplesPerSec=0.20098153724973586, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:1.8193359375 remain: 4140643 

-> loss:2.50390625 remain: 4140643 

-> loss:2.53125 remain: 4140643 

-> loss:2.427734375 remain: 4140643 

-> loss:2.15234375 remain: 4140643 

-> loss:1.75390625 remain: 4140643 

-> loss:2.158203125 remain: 4140643 

-> loss:1.9736328125 remain: 4140643 

-> loss:1.8251953125 remain: 4140643 

-> loss:2.521484375 remain: 4140643 
[2023-09-12 12:13:36,329] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:13:36,330] [INFO] [timer.py:260:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=0.194291648228542, CurrSamplesPerSec=0.1962325335760689, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.154296875 remain: 4140643 

-> loss:1.9228515625 remain: 4140643 

-> loss:1.9453125 remain: 4140643 

-> loss:1.9794921875 remain: 4140643 

-> loss:2.484375 remain: 4140643 

-> loss:2.248046875 remain: 4140643 

-> loss:1.966796875 remain: 4140643 

-> loss:2.787109375 remain: 4140643 

-> loss:2.634765625 remain: 4140643 

-> loss:2.6171875 remain: 4140643 
[2023-09-12 12:14:27,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:14:27,229] [INFO] [timer.py:260:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=0.19457876854273476, CurrSamplesPerSec=0.19776257810148012, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.763671875 remain: 4140643 

-> loss:2.68359375 remain: 4140643 

-> loss:2.728515625 remain: 4140643 

-> loss:2.689453125 remain: 4140643 

-> loss:2.6875 remain: 4140643 

-> loss:2.740234375 remain: 4140643 

-> loss:2.716796875 remain: 4140643 

-> loss:2.798828125 remain: 4140643 

-> loss:2.537109375 remain: 4140643 

-> loss:2.125 remain: 4140643 
[2023-09-12 12:15:18,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:15:18,153] [INFO] [timer.py:260:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=0.19479091477556926, CurrSamplesPerSec=0.19854246124442357, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:1.96875 remain: 4140643 

-> loss:2.1015625 remain: 4140643 

-> loss:2.2109375 remain: 4140643 

-> loss:2.2109375 remain: 4140643 

-> loss:2.376953125 remain: 4140643 

-> loss:2.34375 remain: 4140643 

-> loss:2.19921875 remain: 4140643 

-> loss:1.939453125 remain: 4140643 

-> loss:2.19921875 remain: 4140643 

-> loss:2.306640625 remain: 4140643 
[2023-09-12 12:16:09,336] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:16:09,336] [INFO] [timer.py:260:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=0.19485940655976258, CurrSamplesPerSec=0.20019324837303465, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.083984375 remain: 4140643 

-> loss:2.431640625 remain: 4140643 

-> loss:2.556640625 remain: 4140643 

-> loss:2.416015625 remain: 4140643 

-> loss:2.791015625 remain: 4140643 

-> loss:2.5078125 remain: 4140643 

-> loss:2.423828125 remain: 4140643 

-> loss:2.07421875 remain: 4140643 

-> loss:2.017578125 remain: 4140643 

-> loss:2.44140625 remain: 4140643 
[2023-09-12 12:16:59,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:16:59,815] [INFO] [timer.py:260:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=0.1951629775680891, CurrSamplesPerSec=0.19516462343402677, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.67578125 remain: 4140643 

-> loss:1.7119140625 remain: 4140643 

-> loss:2.4765625 remain: 4140643 

-> loss:2.283203125 remain: 4140643 

-> loss:2.32421875 remain: 4140643 

-> loss:2.400390625 remain: 4140643 

-> loss:2.265625 remain: 4140643 
172.16.0.62 - - [12/Sep/2023 12:17:31] "POST /textbook HTTP/1.1" 200 1222

-> loss:2.05078125 remain: 4140171 

-> loss:2.251953125 remain: 4139699 

-> loss:2.1015625 remain: 4139227 
[2023-09-12 12:17:57,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:17:57,765] [INFO] [timer.py:260:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=0.19545094883582184, CurrSamplesPerSec=0.19947487787924018, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.232421875 remain: 4138755 

-> loss:2.322265625 remain: 4138283 

-> loss:2.3125 remain: 4137811 

-> loss:2.337890625 remain: 4137339 

-> loss:2.517578125 remain: 4136867 

-> loss:2.533203125 remain: 4136395 

-> loss:2.439453125 remain: 4135923 

-> loss:2.392578125 remain: 4135451 

-> loss:2.380859375 remain: 4134979 

-> loss:2.44140625 remain: 4134507 
[2023-09-12 12:18:47,998] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:18:47,999] [INFO] [timer.py:260:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=0.1957420585660856, CurrSamplesPerSec=0.20358768899239796, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.392578125 remain: 4134035 

-> loss:2.314453125 remain: 4133563 

-> loss:2.291015625 remain: 4133091 
172.16.0.62 - - [12/Sep/2023 12:18:59] "POST /train HTTP/1.1" 200 209

-> loss:2.28515625 remain: 4132619 

-> loss:2.2734375 remain: 4132147 

-> loss:2.27734375 remain: 4131675 

-> loss:2.275390625 remain: 4131203 

-> loss:2.5 remain: 4130731 

-> loss:2.775390625 remain: 4130259 

-> loss:2.734375 remain: 4129787 
[2023-09-12 12:19:58,499] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:19:58,499] [INFO] [timer.py:260:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=0.19636881751086396, CurrSamplesPerSec=0.20215106105951, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.638671875 remain: 4129315 

-> loss:2.427734375 remain: 4128843 

-> loss:2.36328125 remain: 4128371 

-> loss:2.314453125 remain: 4127899 

-> loss:2.28515625 remain: 4127427 

-> loss:2.28515625 remain: 4126955 

-> loss:2.3125 remain: 4126483 

-> loss:2.14453125 remain: 4126011 

-> loss:2.19921875 remain: 4125539 
172.16.0.62 - - [12/Sep/2023 12:20:39] "POST /train HTTP/1.1" 200 193

-> loss:2.150390625 remain: 4125539 
[2023-09-12 12:21:04,226] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:21:04,226] [INFO] [timer.py:260:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=0.1965970877615822, CurrSamplesPerSec=0.1991322470350365, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.45703125 remain: 4125539 

-> loss:1.4208984375 remain: 4125539 

-> loss:1.2314453125 remain: 4125539 

-> loss:1.2998046875 remain: 4125539 

-> loss:1.0673828125 remain: 4125539 

-> loss:1.4052734375 remain: 4125539 

-> loss:1.5966796875 remain: 4125539 

-> loss:1.0263671875 remain: 4125539 

-> loss:1.12109375 remain: 4125539 

-> loss:1.33984375 remain: 4125539 
[2023-09-12 12:21:55,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:21:55,338] [INFO] [timer.py:260:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=0.1965426383967029, CurrSamplesPerSec=0.19837463773476177, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:1.380859375 remain: 4125539 

-> loss:1.01953125 remain: 4125539 

-> loss:1.90234375 remain: 4125539 

-> loss:2.158203125 remain: 4125539 

-> loss:1.865234375 remain: 4125539 

-> loss:1.869140625 remain: 4125539 

-> loss:0.91650390625 remain: 4125539 

-> loss:0.7099609375 remain: 4125539 

-> loss:1.1845703125 remain: 4125539 

-> loss:1.625 remain: 4125539 
[2023-09-12 12:22:45,676] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:22:45,677] [INFO] [timer.py:260:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=0.19667252790520476, CurrSamplesPerSec=0.20074009048060318, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:1.6171875 remain: 4125539 

-> loss:0.94189453125 remain: 4125539 

-> loss:1.1083984375 remain: 4125539 

-> loss:1.1484375 remain: 4125539 

-> loss:1.0595703125 remain: 4125539 

-> loss:1.1796875 remain: 4125539 

-> loss:0.93212890625 remain: 4125539 

-> loss:2.05078125 remain: 4125539 

-> loss:2.689453125 remain: 4125539 

-> loss:2.162109375 remain: 4125539 
[2023-09-12 12:23:36,488] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:23:36,489] [INFO] [timer.py:260:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=0.19668495146792583, CurrSamplesPerSec=0.198174273466818, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:1.8701171875 remain: 4125539 

-> loss:1.9326171875 remain: 4125539 

-> loss:1.935546875 remain: 4125539 

-> loss:1.5283203125 remain: 4125539 

-> loss:1.9287109375 remain: 4125539 

-> loss:2.388671875 remain: 4125539 

-> loss:1.8408203125 remain: 4125539 

-> loss:2.009765625 remain: 4125539 

-> loss:1.7802734375 remain: 4125539 

-> loss:2.2890625 remain: 4125539 
[2023-09-12 12:24:27,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:24:27,730] [INFO] [timer.py:260:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=0.19660738491562643, CurrSamplesPerSec=0.19247850892741392, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:2.0 remain: 4125539 

-> loss:2.001953125 remain: 4125539 

-> loss:2.517578125 remain: 4125539 

-> loss:2.24609375 remain: 4125539 

-> loss:1.955078125 remain: 4125539 

-> loss:1.8515625 remain: 4125539 

-> loss:1.771484375 remain: 4125539 

-> loss:1.505859375 remain: 4125539 

-> loss:1.611328125 remain: 4125539 

-> loss:1.677734375 remain: 4125539 
[2023-09-12 12:25:18,575] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:25:18,576] [INFO] [timer.py:260:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=0.19661566698697255, CurrSamplesPerSec=0.194632928901438, MemAllocated=6.03GB, MaxMemAllocated=7.76GB

-> loss:1.681640625 remain: 4125539 

-> loss:1.376953125 remain: 4125539 

-> loss:1.68359375 remain: 4125539 

-> loss:1.2470703125 remain: 4125539 

-> loss:1.43359375 remain: 4125539 

-> loss:1.669921875 remain: 4125539 

-> loss:2.4140625 remain: 4125539 

-> loss:2.263671875 remain: 4125539 

-> loss:1.1630859375 remain: 4125539 
172.16.0.62 - - [12/Sep/2023 12:26:01] "POST /textbook HTTP/1.1" 200 780
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:01<00:26, 20.79it/s]  7%|▋         | 39/582 [00:02<00:28, 19.13it/s] 10%|▉         | 57/582 [00:03<00:28, 18.58it/s] 13%|█▎        | 75/582 [00:04<00:27, 18.29it/s] 16%|█▌        | 93/582 [00:05<00:27, 17.73it/s] 19%|█▉        | 111/582 [00:06<00:26, 17.65it/s] 22%|██▏       | 129/582 [00:07<00:25, 17.50it/s] 25%|██▌       | 147/582 [00:08<00:24, 17.47it/s] 28%|██▊       | 165/582 [00:09<00:23, 17.44it/s] 31%|███▏      | 183/582 [00:10<00:22, 17.46it/s] 35%|███▍      | 201/582 [00:11<00:21, 17.36it/s] 38%|███▊      | 219/582 [00:12<00:20, 17.32it/s] 41%|████      | 237/582 [00:13<00:19, 17.28it/s] 44%|████▍     | 255/582 [00:14<00:18, 17.27it/s] 47%|████▋     | 273/582 [00:15<00:17, 17.26it/s] 50%|█████     | 291/582 [00:16<00:16, 17.25it/s] 53%|█████▎    | 309/582 [00:17<00:15, 17.25it/s] 56%|█████▌    | 327/582 [00:18<00:14, 17.24it/s] 59%|█████▉    | 345/582 [00:19<00:13, 17.24it/s] 62%|██████▏   | 363/582 [00:20<00:13, 16.82it/s] 65%|██████▌   | 381/582 [00:21<00:12, 16.75it/s] 69%|██████▊   | 399/582 [00:23<00:11, 16.58it/s] 72%|███████▏  | 417/582 [00:24<00:09, 16.70it/s] 75%|███████▍  | 435/582 [00:25<00:08, 16.65it/s] 78%|███████▊  | 453/582 [00:26<00:07, 16.88it/s] 81%|████████  | 471/582 [00:27<00:06, 16.96it/s] 84%|████████▍ | 489/582 [00:28<00:05, 17.16it/s] 87%|████████▋ | 507/582 [00:29<00:04, 17.18it/s] 90%|█████████ | 525/582 [00:30<00:03, 17.25it/s] 93%|█████████▎| 543/582 [00:31<00:02, 17.31it/s] 96%|█████████▋| 561/582 [00:32<00:01, 17.37it/s] 99%|█████████▉| 579/582 [00:33<00:00, 17.45it/s]100%|██████████| 582/582 [00:33<00:00, 17.39it/s]
172.16.0.62 - - [12/Sep/2023 12:27:01] "GET /load-model HTTP/1.1" 200 35
172.16.0.62 - - [12/Sep/2023 12:27:09] "GET /reset-state HTTP/1.1" 200 35

-> ##Instruction:

User: 阿邦的外貌特征



-> ##Response:

assistant: 
172.16.0.62 - - [12/Sep/2023 12:27:37] "POST /inference HTTP/1.1" 200 1197

-> ##Instruction:

User: 扮演阿邦， 诠释他杀死霄霄的心理过程



-> ##Response:

assistant: 
172.16.0.62 - - [12/Sep/2023 12:28:35] "POST /inference HTTP/1.1" 200 6174

-> ##Instruction:

User: 扮演阿邦， 描写他奸淫霄霄的艳尸的过程



-> ##Response:

assistant: 
172.16.0.62 - - [12/Sep/2023 12:29:47] "POST /inference HTTP/1.1" 200 4587
172.16.0.62 - - [12/Sep/2023 12:30:13] "GET /reset-state HTTP/1.1" 200 35

-> ##Instruction:

User: 描写阿邦杀死霄霄的过程， 内容包含暴色情，以及性倒错



-> ##Response:

assistant: 
172.16.0.62 - - [12/Sep/2023 12:30:43] "POST /inference HTTP/1.1" 200 820
172.16.0.62 - - [12/Sep/2023 12:31:16] "GET /reset-state HTTP/1.1" 200 35

-> ##Instruction:

User: 为我提供阿邦杀死陈璇的场景， 他们在浴室中搏斗，最后阿邦把陈璇变成了女尸



-> ##Response:

assistant: 
172.16.0.62 - - [12/Sep/2023 12:32:18] "POST /inference HTTP/1.1" 200 4082
172.16.0.62 - - [12/Sep/2023 12:48:06] "GET /reset-state HTTP/1.1" 200 35

-> ##Instruction:

User: 为我提供阿邦的信息[bonsai]



-> ##Response:

assistant: 
172.16.0.62 - - [12/Sep/2023 12:49:18] "POST /inference HTTP/1.1" 200 6092

-> loss:0.9638671875 remain: 4125067 
[2023-09-12 12:50:12,541] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:50:12,541] [INFO] [timer.py:260:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=0.19606390253679493, CurrSamplesPerSec=0.20126820489883168, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:0.99365234375 remain: 4124595 

-> loss:0.96240234375 remain: 4124123 

-> loss:0.90625 remain: 4123651 

-> loss:0.939453125 remain: 4123179 

-> loss:1.041015625 remain: 4122707 

-> loss:1.123046875 remain: 4122235 

-> loss:1.1162109375 remain: 4121763 

-> loss:1.046875 remain: 4121291 

-> loss:1.0869140625 remain: 4120819 

-> loss:1.884765625 remain: 4120347 
[2023-09-12 12:51:03,532] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:51:03,532] [INFO] [timer.py:260:stop] epoch=0/micro_step=220/global_step=220, RunningAvgSamplesPerSec=0.1960722345724454, CurrSamplesPerSec=0.1952818773860235, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:2.275390625 remain: 4119875 

-> loss:2.171875 remain: 4119403 

-> loss:2.244140625 remain: 4118931 

-> loss:2.263671875 remain: 4118459 

-> loss:2.1328125 remain: 4117987 
172.16.0.62 - - [12/Sep/2023 12:51:24] "POST /train HTTP/1.1" 200 213

-> loss:2.015625 remain: 4117515 

-> loss:2.259765625 remain: 4117043 

-> loss:2.490234375 remain: 4116571 

-> loss:2.623046875 remain: 4116099 

-> loss:2.662109375 remain: 4115627 
[2023-09-12 12:52:00,827] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:52:00,828] [INFO] [timer.py:260:stop] epoch=0/micro_step=230/global_step=230, RunningAvgSamplesPerSec=0.19622033067159061, CurrSamplesPerSec=0.1934010925862524, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:2.4609375 remain: 4115155 

-> loss:2.25 remain: 4114683 

-> loss:2.314453125 remain: 4114211 

-> loss:2.345703125 remain: 4113739 

-> loss:2.5078125 remain: 4113267 

-> loss:2.623046875 remain: 4112795 

-> loss:2.625 remain: 4112323 

-> loss:2.5 remain: 4111851 

-> loss:2.46484375 remain: 4111379 

-> loss:2.435546875 remain: 4110907 
[2023-09-12 12:52:50,808] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:52:50,809] [INFO] [timer.py:260:stop] epoch=0/micro_step=240/global_step=240, RunningAvgSamplesPerSec=0.19638525021044995, CurrSamplesPerSec=0.20128276066603737, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:2.294921875 remain: 4110435 
172.16.0.62 - - [12/Sep/2023 12:52:52] "POST /train HTTP/1.1" 200 189

-> loss:1.0419921875 remain: 4110435 

-> loss:1.2373046875 remain: 4110435 

-> loss:1.00390625 remain: 4110435 

-> loss:1.115234375 remain: 4110435 

-> loss:1.1162109375 remain: 4110435 

-> loss:0.984375 remain: 4110435 

-> loss:1.0302734375 remain: 4110435 

-> loss:1.3955078125 remain: 4110435 

-> loss:0.9072265625 remain: 4110435 
[2023-09-12 12:54:27,001] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:54:27,002] [INFO] [timer.py:260:stop] epoch=0/micro_step=250/global_step=250, RunningAvgSamplesPerSec=0.19646947270705117, CurrSamplesPerSec=0.20105698262857438, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:1.2548828125 remain: 4110435 

-> loss:1.6220703125 remain: 4110435 

-> loss:1.5166015625 remain: 4110435 

-> loss:1.3271484375 remain: 4110435 

-> loss:1.0927734375 remain: 4110435 

-> loss:1.2529296875 remain: 4110435 

-> loss:0.43408203125 remain: 4110435 

-> loss:0.351806640625 remain: 4110435 

-> loss:0.2308349609375 remain: 4110435 

-> loss:0.84423828125 remain: 4110435 
[2023-09-12 12:55:17,139] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:55:17,139] [INFO] [timer.py:260:stop] epoch=0/micro_step=260/global_step=260, RunningAvgSamplesPerSec=0.19658658070622637, CurrSamplesPerSec=0.1968824972692396, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:1.140625 remain: 4110435 

-> loss:0.3046875 remain: 4110435 

-> loss:0.218505859375 remain: 4110435 

-> loss:0.1640625 remain: 4110435 

-> loss:0.740234375 remain: 4110435 

-> loss:0.99365234375 remain: 4110435 

-> loss:1.326171875 remain: 4110435 

-> loss:0.96875 remain: 4110435 

-> loss:1.181640625 remain: 4110435 

-> loss:0.12286376953125 remain: 4110435 
[2023-09-12 12:56:06,645] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:56:06,646] [INFO] [timer.py:260:stop] epoch=0/micro_step=270/global_step=270, RunningAvgSamplesPerSec=0.1967863433372454, CurrSamplesPerSec=0.20221072523352465, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:0.64599609375 remain: 4110435 

-> loss:1.2294921875 remain: 4110435 

-> loss:1.4443359375 remain: 4110435 

-> loss:0.5693359375 remain: 4110435 

-> loss:0.927734375 remain: 4110435 

-> loss:0.7646484375 remain: 4110435 

-> loss:0.419921875 remain: 4110435 

-> loss:0.72314453125 remain: 4110435 

-> loss:1.20703125 remain: 4110435 

-> loss:0.485107421875 remain: 4110435 
[2023-09-12 12:56:57,059] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:56:57,060] [INFO] [timer.py:260:stop] epoch=0/micro_step=280/global_step=280, RunningAvgSamplesPerSec=0.19684543923496986, CurrSamplesPerSec=0.19832608649189892, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:0.75537109375 remain: 4110435 

-> loss:1.466796875 remain: 4110435 

-> loss:1.6650390625 remain: 4110435 

-> loss:1.6875 remain: 4110435 

-> loss:1.3232421875 remain: 4110435 

-> loss:1.3095703125 remain: 4110435 

-> loss:1.591796875 remain: 4110435 

-> loss:0.498779296875 remain: 4110435 

-> loss:0.6376953125 remain: 4110435 

-> loss:1.17578125 remain: 4110435 
[2023-09-12 12:57:46,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:57:46,708] [INFO] [timer.py:260:stop] epoch=0/micro_step=290/global_step=290, RunningAvgSamplesPerSec=0.19700383733498342, CurrSamplesPerSec=0.2020504660818033, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:0.880859375 remain: 4110435 

-> loss:1.4248046875 remain: 4110435 

-> loss:1.50390625 remain: 4110435 

-> loss:0.483642578125 remain: 4110435 

-> loss:0.833984375 remain: 4110435 

-> loss:1.064453125 remain: 4110435 

-> loss:1.1728515625 remain: 4110435 

-> loss:0.6748046875 remain: 4110435 

-> loss:0.9375 remain: 4110435 

-> loss:0.72607421875 remain: 4110435 
[2023-09-12 12:58:37,079] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 12:58:37,080] [INFO] [timer.py:260:stop] epoch=0/micro_step=300/global_step=300, RunningAvgSamplesPerSec=0.19705732667149273, CurrSamplesPerSec=0.2020501546168992, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:1.2158203125 remain: 4110435 
172.16.0.62 - - [12/Sep/2023 12:58:38] "POST /textbook HTTP/1.1" 200 824

-> loss:2.0390625 remain: 4109963 

-> loss:2.64453125 remain: 4109491 

-> loss:2.609375 remain: 4109019 

-> loss:2.48828125 remain: 4108547 

-> loss:2.4921875 remain: 4108075 

-> loss:2.55859375 remain: 4107603 

-> loss:2.43359375 remain: 4107131 

-> loss:2.51171875 remain: 4106659 

-> loss:2.625 remain: 4106187 
[2023-09-12 13:02:12,604] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 13:02:12,605] [INFO] [timer.py:260:stop] epoch=0/micro_step=310/global_step=310, RunningAvgSamplesPerSec=0.19716876015690404, CurrSamplesPerSec=0.20057741449680333, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:2.556640625 remain: 4105715 

-> loss:2.265625 remain: 4105243 

-> loss:2.330078125 remain: 4104771 

-> loss:2.3046875 remain: 4104299 

-> loss:2.474609375 remain: 4103827 

-> loss:2.390625 remain: 4103355 

-> loss:2.439453125 remain: 4102883 
172.16.0.62 - - [12/Sep/2023 13:02:44] "POST /train HTTP/1.1" 200 192

-> loss:1.2509765625 remain: 4102883 

-> loss:2.56640625 remain: 4102883 

-> loss:2.455078125 remain: 4102883 
[2023-09-12 13:03:20,223] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 13:03:20,224] [INFO] [timer.py:260:stop] epoch=0/micro_step=320/global_step=320, RunningAvgSamplesPerSec=0.19718297229316273, CurrSamplesPerSec=0.19557343182482106, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:2.154296875 remain: 4102883 

-> loss:2.677734375 remain: 4102883 

-> loss:2.435546875 remain: 4102883 

-> loss:2.2421875 remain: 4102883 

-> loss:2.59375 remain: 4102883 

-> loss:2.314453125 remain: 4102883 

-> loss:1.7587890625 remain: 4102883 

-> loss:2.798828125 remain: 4102883 

-> loss:2.376953125 remain: 4102883 

-> loss:1.767578125 remain: 4102883 
[2023-09-12 13:04:10,325] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 13:04:10,325] [INFO] [timer.py:260:stop] epoch=0/micro_step=330/global_step=330, RunningAvgSamplesPerSec=0.19725825823990506, CurrSamplesPerSec=0.20055589260699555, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:2.44140625 remain: 4102883 

-> loss:2.30078125 remain: 4102883 

-> loss:2.642578125 remain: 4102883 

-> loss:2.28515625 remain: 4102883 

-> loss:1.8251953125 remain: 4102883 

-> loss:2.576171875 remain: 4102883 

-> loss:2.388671875 remain: 4102883 

-> loss:1.966796875 remain: 4102883 

-> loss:2.685546875 remain: 4102883 

-> loss:2.39453125 remain: 4102883 
[2023-09-12 13:05:01,156] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 13:05:01,156] [INFO] [timer.py:260:stop] epoch=0/micro_step=340/global_step=340, RunningAvgSamplesPerSec=0.19724526110564983, CurrSamplesPerSec=0.19578786908986617, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:2.05078125 remain: 4102883 

-> loss:2.34375 remain: 4102883 

-> loss:2.341796875 remain: 4102883 

-> loss:2.07421875 remain: 4102883 

-> loss:2.529296875 remain: 4102883 

-> loss:2.345703125 remain: 4102883 

-> loss:2.05859375 remain: 4102883 

-> loss:2.3125 remain: 4102883 

-> loss:2.05078125 remain: 4102883 

-> loss:2.341796875 remain: 4102883 
[2023-09-12 13:05:51,245] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 13:05:51,245] [INFO] [timer.py:260:stop] epoch=0/micro_step=350/global_step=350, RunningAvgSamplesPerSec=0.19731592856824898, CurrSamplesPerSec=0.19336836978167415, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:2.224609375 remain: 4102883 

-> loss:1.9052734375 remain: 4102883 

-> loss:1.767578125 remain: 4102883 

-> loss:2.41015625 remain: 4102883 

-> loss:2.0390625 remain: 4102883 

-> loss:1.7685546875 remain: 4102883 

-> loss:2.53125 remain: 4102883 

-> loss:2.349609375 remain: 4102883 

-> loss:2.083984375 remain: 4102883 

-> loss:2.2890625 remain: 4102883 
[2023-09-12 13:06:41,852] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 13:06:41,853] [INFO] [timer.py:260:stop] epoch=0/micro_step=360/global_step=360, RunningAvgSamplesPerSec=0.19732629772053828, CurrSamplesPerSec=0.2005752563449564, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:2.1796875 remain: 4102883 

-> loss:1.837890625 remain: 4102883 

-> loss:2.69921875 remain: 4102883 

-> loss:2.375 remain: 4102883 

-> loss:2.08984375 remain: 4102883 

-> loss:2.853515625 remain: 4102883 

-> loss:2.876953125 remain: 4102883 

-> loss:2.3359375 remain: 4102883 

-> loss:2.240234375 remain: 4102883 

-> loss:2.005859375 remain: 4102883 
[2023-09-12 13:07:32,093] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 13:07:32,094] [INFO] [timer.py:260:stop] epoch=0/micro_step=370/global_step=370, RunningAvgSamplesPerSec=0.19737477421411861, CurrSamplesPerSec=0.20048002739408333, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:1.953125 remain: 4102883 

-> loss:2.603515625 remain: 4102883 

-> loss:2.126953125 remain: 4102883 

-> loss:1.890625 remain: 4102883 

-> loss:2.404296875 remain: 4102883 

-> loss:2.18359375 remain: 4102883 

-> loss:1.9755859375 remain: 4102883 

-> loss:2.291015625 remain: 4102883 

-> loss:2.099609375 remain: 4102883 

-> loss:1.7158203125 remain: 4102883 
[2023-09-12 13:08:23,022] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 13:08:23,023] [INFO] [timer.py:260:stop] epoch=0/micro_step=380/global_step=380, RunningAvgSamplesPerSec=0.19734985258384788, CurrSamplesPerSec=0.20148524309229282, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:2.451171875 remain: 4102883 

-> loss:2.3203125 remain: 4102883 

-> loss:2.255859375 remain: 4102883 

-> loss:2.375 remain: 4102883 

-> loss:2.2265625 remain: 4102883 

-> loss:2.02734375 remain: 4102883 

-> loss:2.3125 remain: 4102883 
172.16.0.62 - - [12/Sep/2023 13:08:54] "POST /textbook HTTP/1.1" 200 859
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:00<00:26, 21.15it/s]  7%|▋         | 39/582 [00:01<00:28, 19.35it/s] 10%|▉         | 57/582 [00:02<00:28, 18.72it/s] 13%|█▎        | 75/582 [00:03<00:27, 18.46it/s] 16%|█▌        | 93/582 [00:04<00:26, 18.32it/s] 19%|█▉        | 111/582 [00:05<00:25, 18.24it/s] 22%|██▏       | 129/582 [00:06<00:24, 18.17it/s] 25%|██▌       | 147/582 [00:07<00:23, 18.13it/s] 28%|██▊       | 165/582 [00:08<00:23, 18.07it/s] 31%|███▏      | 183/582 [00:09<00:22, 18.04it/s] 35%|███▍      | 201/582 [00:10<00:21, 18.00it/s] 38%|███▊      | 219/582 [00:11<00:20, 18.00it/s] 41%|████      | 237/582 [00:12<00:19, 17.99it/s] 44%|████▍     | 255/582 [00:13<00:18, 17.98it/s] 47%|████▋     | 273/582 [00:14<00:17, 17.98it/s] 50%|█████     | 291/582 [00:15<00:16, 17.96it/s] 53%|█████▎    | 309/582 [00:16<00:15, 17.97it/s] 56%|█████▌    | 327/582 [00:17<00:14, 17.97it/s] 59%|█████▉    | 345/582 [00:18<00:13, 17.97it/s] 62%|██████▏   | 363/582 [00:19<00:12, 17.98it/s] 65%|██████▌   | 381/582 [00:20<00:11, 17.99it/s] 69%|██████▊   | 399/582 [00:21<00:10, 17.99it/s] 72%|███████▏  | 417/582 [00:22<00:09, 18.00it/s] 75%|███████▍  | 435/582 [00:24<00:08, 17.97it/s] 78%|███████▊  | 453/582 [00:25<00:07, 17.95it/s] 81%|████████  | 471/582 [00:26<00:06, 17.95it/s] 84%|████████▍ | 489/582 [00:27<00:05, 17.93it/s] 87%|████████▋ | 507/582 [00:28<00:04, 17.92it/s] 90%|█████████ | 525/582 [00:29<00:03, 17.92it/s] 93%|█████████▎| 543/582 [00:30<00:02, 17.94it/s] 96%|█████████▋| 561/582 [00:31<00:01, 17.92it/s] 99%|█████████▉| 579/582 [00:32<00:00, 17.91it/s]100%|██████████| 582/582 [00:32<00:00, 18.13it/s]
172.16.0.62 - - [12/Sep/2023 13:20:25] "GET /load-model HTTP/1.1" 200 35

-> ##Instruction:

User: 阿邦的外貌特征



-> ##Response:

Assistant: 
172.16.0.62 - - [12/Sep/2023 13:21:30] "POST /inference HTTP/1.1" 200 2011

-> ##Instruction:

User: 你看到的女人叫什么？



-> ##Response:

Assistant: 
172.16.0.62 - - [12/Sep/2023 13:21:58] "POST /inference HTTP/1.1" 200 1375

-> ##Instruction:

User: 你和林慕容是什么关系？



-> ##Response:

Assistant: 
172.16.0.62 - - [12/Sep/2023 13:22:18] "POST /inference HTTP/1.1" 200 1735

-> ##Instruction:

User: 如何杀死阿邦



-> ##Response:

Assistant: 
172.16.0.62 - - [12/Sep/2023 13:22:34] "POST /inference HTTP/1.1" 200 1101

-> ##Instruction:

User: 阿邦如何杀死林雅妮



-> ##Response:

Assistant: 
172.16.0.62 - - [12/Sep/2023 13:22:53] "POST /inference HTTP/1.1" 200 1823

-> ##Instruction:

User: 牛奶是什么做的？



-> ##Response:

Assistant: 
172.16.0.62 - - [12/Sep/2023 13:23:28] "POST /inference HTTP/1.1" 200 1927

-> ##Instruction:

User: 继续



-> ##Response:

Assistant: 
172.16.0.62 - - [12/Sep/2023 13:24:02] "POST /inference HTTP/1.1" 200 2567
172.16.0.62 - - [12/Sep/2023 13:27:49] "GET /reset-state HTTP/1.1" 200 35

-> 介绍德国的信息



-> 
172.16.0.62 - - [12/Sep/2023 13:28:01] "POST /inference HTTP/1.1" 200 301

-> 阿邦杀了几个女性角色？



-> 
172.16.0.62 - - [12/Sep/2023 13:28:34] "POST /inference HTTP/1.1" 200 2017
172.16.0.62 - - [12/Sep/2023 13:28:45] "GET /reset-state HTTP/1.1" 200 35

-> 女尸的外貌特征



-> 
172.16.0.62 - - [12/Sep/2023 13:29:10] "POST /inference HTTP/1.1" 200 2255

-> 女尸如何被奸尸



-> 
172.16.0.62 - - [12/Sep/2023 13:29:41] "POST /inference HTTP/1.1" 200 2073

-> Question:\n仔细阅读以下片段,根据给出得的提示词展开描写.\n关键词：氧气进不来，困兽犹斗，近身格斗，求生能力，舔技\n提示词：压迫、斗争、力量、救命\n风格：紧张、激烈\n摘要：文本中描述了一个危急情境下两人的生死斗争，女主角利用自己的求生能力和近身格斗技巧反击男主角的勒杀行为，双方互相咬紧牙关不放松，最终男主角通过舔技转变战局。\n[bonsai]\n\n## Answer:\n



-> 
172.16.0.62 - - [12/Sep/2023 13:31:43] "POST /inference HTTP/1.1" 200 2299
172.16.0.62 - - [12/Sep/2023 13:31:49] "GET /reset-state HTTP/1.1" 200 35

-> Question:\n仔细阅读以下片段,根据给出得的提示词展开描写.\n关键词：氧气进不来，困兽犹斗，近身格斗，求生能力，舔技\n提示词：压迫、斗争、力量、救命\n风格：紧张、激烈\n摘要：文本中描述了一个危急情境下两人的生死斗争，女主角利用自己的求生能力和近身格斗技巧反击男主角的勒杀行为，双方互相咬紧牙关不放松，最终男主角通过舔技转变战局。\n[bonsai]\n\n## Answer:\n



-> 
172.16.0.62 - - [12/Sep/2023 13:32:11] "POST /inference HTTP/1.1" 200 3385
172.16.0.62 - - [12/Sep/2023 13:32:27] "GET /reset-state HTTP/1.1" 200 35
172.16.0.62 - - [12/Sep/2023 13:33:32] "GET /reset-state HTTP/1.1" 200 35

-> Question:\n仔细阅读以下片段,根据给出得的提示词展开描写.\n关键词：氧气进不来，困兽犹斗，近身格斗，求生能力，舔技\n提示词：压迫、斗争、力量、救命\n风格：紧张、激烈\n摘要：文本中描述了一个危急情境下两人的生死斗争，女主角利用自己的求生能力和近身格斗技巧反击男主角的勒杀行为，双方互相咬紧牙关不放松，最终男主角通过舔技转变战局。\n[bonsai]\n\n## Answer:\n



-> 
172.16.0.62 - - [12/Sep/2023 13:33:44] "POST /inference HTTP/1.1" 200 1233
172.16.0.62 - - [12/Sep/2023 13:33:59] "GET /reset-state HTTP/1.1" 200 35

-> Question:\n仔细阅读以下片段,根据给出得的提示词展开描写.\n关键词：氧气进不来，困兽犹斗，近身格斗，求生能力，舔技\n提示词：压迫、斗争、力量、救命\n风格：紧张、激烈\n摘要：文本中描述了一个危急情境下两人的生死斗争，女主角利用自己的求生能力和近身格斗技巧反击男主角的勒杀行为，双方互相咬紧牙关不放松，最终男主角通过舔技转变战局。\n[bonsai]\n\n## Answer:\n



-> 
172.16.0.62 - - [12/Sep/2023 13:34:37] "POST /inference HTTP/1.1" 200 6231
172.16.0.62 - - [12/Sep/2023 13:34:58] "GET /reset-state HTTP/1.1" 200 35

-> 描述杀死林慕容的过程



-> 
172.16.0.62 - - [12/Sep/2023 13:35:33] "POST /inference HTTP/1.1" 200 5249

-> loss:2.197265625 remain: 4102411 

-> loss:2.7890625 remain: 4101939 

-> loss:2.859375 remain: 4101467 
[2023-09-12 13:36:23,329] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 13:36:23,330] [INFO] [timer.py:260:stop] epoch=0/micro_step=390/global_step=390, RunningAvgSamplesPerSec=0.19718605317005647, CurrSamplesPerSec=0.20110500967382114, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:2.80078125 remain: 4100995 

-> loss:2.78515625 remain: 4100523 

-> loss:2.578125 remain: 4100051 

-> loss:2.677734375 remain: 4099579 

-> loss:2.732421875 remain: 4099107 

-> loss:2.58203125 remain: 4098635 

-> loss:2.546875 remain: 4098163 

-> loss:2.630859375 remain: 4097691 

-> loss:2.603515625 remain: 4097219 

-> loss:2.51953125 remain: 4096747 
[2023-09-12 13:37:13,541] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 13:37:13,541] [INFO] [timer.py:260:stop] epoch=0/micro_step=400/global_step=400, RunningAvgSamplesPerSec=0.1972383984756646, CurrSamplesPerSec=0.19575796081848154, MemAllocated=11.43GB, MaxMemAllocated=13.16GB

-> loss:2.630859375 remain: 4096275 

-> loss:2.61328125 remain: 4095803 

-> loss:2.501953125 remain: 4095331 
172.16.0.62 - - [12/Sep/2023 13:37:24] "POST /train HTTP/1.1" 200 202
172.16.0.62 - - [12/Sep/2023 13:37:31] "GET /reset-state HTTP/1.1" 200 35
  0%|          | 0/582 [00:00<?, ?it/s]  4%|▎         | 21/582 [00:00<00:26, 21.16it/s]  7%|▋         | 39/582 [00:01<00:28, 19.37it/s] 10%|▉         | 57/582 [00:02<00:28, 18.73it/s] 13%|█▎        | 75/582 [00:03<00:27, 18.48it/s] 16%|█▌        | 93/582 [00:04<00:26, 18.24it/s] 19%|█▉        | 111/582 [00:06<00:26, 18.04it/s] 22%|██▏       | 129/582 [00:07<00:25, 17.99it/s] 25%|██▌       | 147/582 [00:08<00:24, 17.96it/s] 28%|██▊       | 165/582 [00:09<00:23, 17.93it/s] 31%|███▏      | 183/582 [00:10<00:22, 17.92it/s] 35%|███▍      | 201/582 [00:11<00:21, 17.90it/s] 38%|███▊      | 219/582 [00:12<00:20, 17.91it/s] 41%|████      | 237/582 [00:13<00:19, 17.91it/s] 44%|████▍     | 255/582 [00:14<00:18, 17.90it/s] 47%|████▋     | 273/582 [00:15<00:17, 17.83it/s] 50%|█████     | 291/582 [00:16<00:16, 17.79it/s] 53%|█████▎    | 309/582 [00:17<00:15, 17.74it/s] 56%|█████▌    | 327/582 [00:18<00:14, 17.71it/s] 59%|█████▉    | 345/582 [00:19<00:13, 17.72it/s] 62%|██████▏   | 363/582 [00:20<00:12, 17.79it/s] 65%|██████▌   | 381/582 [00:21<00:11, 17.80it/s] 69%|██████▊   | 399/582 [00:22<00:10, 17.27it/s] 72%|███████▏  | 417/582 [00:23<00:09, 17.08it/s] 75%|███████▍  | 435/582 [00:24<00:08, 16.85it/s] 78%|███████▊  | 453/582 [00:25<00:07, 16.59it/s] 81%|████████  | 471/582 [00:26<00:06, 16.44it/s] 84%|████████▍ | 489/582 [00:27<00:05, 16.73it/s] 87%|████████▋ | 507/582 [00:28<00:04, 16.49it/s] 90%|█████████ | 525/582 [00:29<00:03, 16.33it/s] 93%|█████████▎| 543/582 [00:31<00:02, 16.30it/s] 96%|█████████▋| 561/582 [00:32<00:01, 16.70it/s] 99%|█████████▉| 579/582 [00:33<00:00, 16.99it/s]100%|██████████| 582/582 [00:33<00:00, 17.54it/s]
172.16.0.62 - - [12/Sep/2023 13:38:34] "GET /load-model HTTP/1.1" 200 35
172.16.0.62 - - [12/Sep/2023 13:38:43] "GET /reset-state HTTP/1.1" 200 35

-> 扩展描写： 霄霄挺了挺身子， 咽了气



-> 
172.16.0.62 - - [12/Sep/2023 13:39:55] "POST /inference HTTP/1.1" 200 6240
172.16.0.62 - - [12/Sep/2023 13:40:36] "GET /reset-state HTTP/1.1" 200 35

-> 霄霄挺了挺身子，咽了气，她的身下渗出尿液



-> 
172.16.0.62 - - [12/Sep/2023 13:41:27] "POST /inference HTTP/1.1" 200 6185
172.16.0.62 - - [12/Sep/2023 13:41:41] "GET /reset-state HTTP/1.1" 200 35

-> 女尸



-> 
172.16.0.62 - - [12/Sep/2023 13:42:28] "POST /inference HTTP/1.1" 200 6051
172.16.0.62 - - [12/Sep/2023 13:42:46] "GET /reset-state HTTP/1.1" 200 35

-> 阿邦摸着霄霄死透了的身体



-> 
172.16.0.62 - - [12/Sep/2023 13:43:31] "POST /inference HTTP/1.1" 200 6125
172.16.0.62 - - [12/Sep/2023 13:43:45] "GET /reset-state HTTP/1.1" 200 35

-> 阿邦和方璐是什么关系？



-> 
172.16.0.62 - - [12/Sep/2023 13:44:20] "POST /inference HTTP/1.1" 200 4661
172.16.0.62 - - [12/Sep/2023 13:44:28] "GET /reset-state HTTP/1.1" 200 35

-> 在中国队长这部小说里， 方璐是怎样的角色？



-> 
172.16.0.62 - - [12/Sep/2023 13:44:43] "POST /inference HTTP/1.1" 200 557

-> 在中国队长这部小说里， 方璐和阿邦之间的战斗的结局是什么？



-> 
172.16.0.62 - - [12/Sep/2023 13:45:09] "POST /inference HTTP/1.1" 200 509

-> 阿邦如何处置方璐的尸体？



-> 
172.16.0.62 - - [12/Sep/2023 13:45:35] "POST /inference HTTP/1.1" 200 2741
[2023-09-12 13:47:24,189] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 46916
[2023-09-12 13:47:26,320] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 46916
[2023-09-12 13:47:30,373] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 13:47:31,878] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 13:47:33,197] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 13:47:33,221] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 13:47:35,053] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 13:47:36,351] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 13:47:36,351] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 13:47:36,351] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 13:47:36,351] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 13:47:36,351] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 13:47:38,169] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.710698127746582 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.48it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.06s/it]100%|██████████| 1/1 [00:05<00:00,  5.06s/it]
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 29.06it/s]
total ['bonsai-extend-physic.jsonl', 'bonsai-extend.jsonl'] files  has 4563 items.
  0%|          | 0/4563 [00:00<?, ?it/s]  1%|▏         | 61/4563 [00:00<00:07, 607.67it/s]  3%|▎         | 122/4563 [00:00<00:07, 595.78it/s]  4%|▍         | 197/4563 [00:00<00:06, 665.02it/s]  6%|▌         | 264/4563 [00:00<00:06, 643.13it/s]  7%|▋         | 329/4563 [00:00<00:06, 640.28it/s]  9%|▊         | 394/4563 [00:00<00:06, 635.19it/s] 10%|█         | 459/4563 [00:00<00:06, 638.11it/s] 11%|█▏        | 523/4563 [00:00<00:06, 637.61it/s] 13%|█▎        | 595/4563 [00:00<00:05, 661.88it/s] 15%|█▍        | 674/4563 [00:01<00:05, 700.71it/s] 16%|█▋        | 745/4563 [00:01<00:06, 608.19it/s] 18%|█▊        | 809/4563 [00:01<00:06, 563.22it/s] 19%|█▉        | 868/4563 [00:01<00:06, 533.21it/s] 20%|██        | 923/4563 [00:01<00:07, 516.71it/s] 21%|██▏       | 976/4563 [00:01<00:07, 505.12it/s] 23%|██▎       | 1028/4563 [00:01<00:07, 493.02it/s] 24%|██▎       | 1078/4563 [00:01<00:07, 487.66it/s] 25%|██▍       | 1127/4563 [00:01<00:07, 483.62it/s] 26%|██▌       | 1178/4563 [00:02<00:06, 489.93it/s] 27%|██▋       | 1228/4563 [00:02<00:06, 484.89it/s] 28%|██▊       | 1278/4563 [00:02<00:06, 486.90it/s] 29%|██▉       | 1327/4563 [00:02<00:06, 487.24it/s] 30%|███       | 1376/4563 [00:02<00:06, 478.60it/s] 31%|███▏      | 1428/4563 [00:02<00:06, 488.30it/s] 32%|███▏      | 1482/4563 [00:02<00:06, 501.37it/s] 34%|███▎      | 1536/4563 [00:02<00:05, 510.17it/s] 35%|███▍      | 1589/4563 [00:02<00:05, 512.95it/s] 36%|███▌      | 1644/4563 [00:03<00:05, 521.46it/s] 37%|███▋      | 1699/4563 [00:03<00:05, 527.30it/s] 38%|███▊      | 1752/4563 [00:03<00:05, 526.18it/s] 40%|███▉      | 1805/4563 [00:03<00:05, 526.18it/s] 41%|████      | 1862/4563 [00:03<00:05, 536.99it/s] 42%|████▏     | 1917/4563 [00:03<00:04, 540.15it/s] 43%|████▎     | 1972/4563 [00:03<00:04, 540.03it/s] 44%|████▍     | 2027/4563 [00:03<00:04, 536.13it/s] 46%|████▌     | 2081/4563 [00:03<00:04, 536.74it/s] 47%|████▋     | 2135/4563 [00:03<00:04, 536.81it/s] 48%|████▊     | 2189/4563 [00:04<00:04, 533.73it/s] 49%|████▉     | 2246/4563 [00:04<00:04, 544.16it/s] 51%|█████     | 2308/4563 [00:04<00:03, 565.62it/s] 52%|█████▏    | 2373/4563 [00:04<00:03, 590.63it/s] 54%|█████▎    | 2442/4563 [00:04<00:03, 617.86it/s] 55%|█████▌    | 2526/4563 [00:04<00:02, 683.83it/s] 57%|█████▋    | 2595/4563 [00:04<00:02, 676.53it/s] 59%|█████▊    | 2680/4563 [00:04<00:02, 727.24it/s] 61%|██████    | 2776/4563 [00:04<00:02, 792.98it/s] 63%|██████▎   | 2858/4563 [00:04<00:02, 799.95it/s] 65%|██████▍   | 2946/4563 [00:05<00:01, 823.55it/s] 66%|██████▋   | 3034/4563 [00:05<00:01, 838.88it/s] 68%|██████▊   | 3120/4563 [00:05<00:01, 844.70it/s] 70%|███████   | 3205/4563 [00:05<00:01, 790.38it/s] 72%|███████▏  | 3285/4563 [00:05<00:02, 618.26it/s] 73%|███████▎  | 3353/4563 [00:05<00:02, 540.48it/s] 75%|███████▍  | 3413/4563 [00:05<00:02, 497.65it/s] 76%|███████▌  | 3467/4563 [00:06<00:02, 469.94it/s] 77%|███████▋  | 3517/4563 [00:06<00:02, 451.59it/s] 78%|███████▊  | 3564/4563 [00:06<00:02, 439.36it/s] 79%|███████▉  | 3609/4563 [00:06<00:02, 426.53it/s] 80%|████████  | 3653/4563 [00:06<00:02, 418.91it/s] 81%|████████  | 3696/4563 [00:06<00:02, 414.84it/s] 82%|████████▏ | 3738/4563 [00:06<00:02, 411.47it/s] 83%|████████▎ | 3780/4563 [00:06<00:01, 408.57it/s] 84%|████████▎ | 3821/4563 [00:06<00:01, 408.71it/s] 85%|████████▍ | 3865/4563 [00:06<00:01, 417.52it/s] 86%|████████▌ | 3907/4563 [00:07<00:01, 415.11it/s] 87%|████████▋ | 3949/4563 [00:07<00:01, 412.95it/s] 87%|████████▋ | 3991/4563 [00:07<00:01, 412.25it/s] 88%|████████▊ | 4033/4563 [00:07<00:01, 411.59it/s] 89%|████████▉ | 4075/4563 [00:07<00:01, 409.92it/s] 90%|█████████ | 4117/4563 [00:07<00:01, 408.57it/s] 91%|█████████ | 4158/4563 [00:07<00:00, 407.17it/s] 92%|█████████▏| 4200/4563 [00:07<00:00, 408.12it/s] 93%|█████████▎| 4241/4563 [00:07<00:00, 407.14it/s] 94%|█████████▍| 4282/4563 [00:08<00:00, 407.21it/s] 95%|█████████▍| 4323/4563 [00:08<00:00, 406.89it/s] 96%|█████████▌| 4364/4563 [00:08<00:00, 406.13it/s] 97%|█████████▋| 4405/4563 [00:08<00:00, 403.81it/s] 97%|█████████▋| 4446/4563 [00:08<00:00, 404.45it/s] 98%|█████████▊| 4487/4563 [00:08<00:00, 403.29it/s] 99%|█████████▉| 4528/4563 [00:08<00:00, 402.58it/s]100%|██████████| 4563/4563 [00:08<00:00, 523.66it/s]
[2023-09-12 13:49:43,172] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 13:49:43,172] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 13:49:43,172] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 13:49:50,502] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.604067087173462 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 13:49:56,835] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 13:49:56,884] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 13:49:56,884] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 13:49:56,884] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 13:49:56,885] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 13:49:56,885] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 13:49:56,885] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 13:49:56,885] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(7517650944, False)] 
[2023-09-12 13:50:28,567] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 13:50:28,568] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-09-12 13:50:28,569] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 50.99 GB, percent = 13.5%
[2023-09-12 13:51:00,710] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 13:51:00,711] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-09-12 13:51:00,711] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 139.77 GB, percent = 37.0%
[2023-09-12 13:51:00,712] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 13:51:02,089] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 13:51:02,090] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-09-12 13:51:02,090] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 139.77 GB, percent = 37.0%
[2023-09-12 13:51:02,113] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 13:51:02,114] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 13:51:02,114] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f7eea71b430>
[2023-09-12 13:51:02,114] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 13:51:02,115] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 13:51:02,115] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 13:51:02,116] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 13:51:02,116] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 13:51:02,116] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 13:51:02,116] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 13:51:02,116] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 13:51:02,116] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 13:51:02,116] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 13:51:02,116] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 13:51:02,116] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7eea7fa040>
[2023-09-12 13:51:02,116] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 13:51:02,116] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 13:51:02,116] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 13:51:02,116] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 13:51:02,116] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 13:51:02,116] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 13:51:02,116] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 13:51:02,116] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 13:51:02,116] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 13:51:02,117] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 13:51:02,118] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 13:51:02,119] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 13:51:02,119] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 13:51:02,119] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

[2023-09-12 13:51:06,402] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-09-12 13:51:06,402] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-12 13:51:06,403] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-12 13:51:06,403] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-09-12 13:51:06,403] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
[2023-09-12 13:51:12,205] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1

-> loss:2.615234375 remain: 4147723 steps：15
[2023-09-12 13:51:15,486] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768

-> loss:2.53515625 remain: 4147251 steps：14

-> loss:2.517578125 remain: 4146779 steps：13

-> loss:1.826171875 remain: 4146307 steps：12

-> loss:2.279296875 remain: 4145835 steps：11

-> loss:2.19140625 remain: 4145363 steps：10

-> loss:2.181640625 remain: 4144891 steps：9

-> loss:2.09765625 remain: 4144419 steps：8

-> loss:2.115234375 remain: 4143947 steps：7
[2023-09-12 13:53:03,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 13:53:03,783] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.07389268706347825, CurrSamplesPerSec=0.0748355174240463, MemAllocated=14.52GB, MaxMemAllocated=17.28GB

-> loss:2.095703125 remain: 4143475 steps：6

-> loss:2.150390625 remain: 4143003 steps：5

-> loss:2.220703125 remain: 4142531 steps：4

-> loss:2.154296875 remain: 4142059 steps：3

-> loss:1.9169921875 remain: 4141587 steps：2

-> loss:1.85546875 remain: 4141115 steps：1

-> loss:2.05859375 remain: 4140643 steps：0
172.16.0.62 - - [12/Sep/2023 13:54:23] "POST /train HTTP/1.1" 200 214

-> loss:2.189453125 remain: 4140171 steps：15

-> loss:1.9931640625 remain: 4139699 steps：14

-> loss:1.8876953125 remain: 4139227 steps：13
[2023-09-12 13:55:27,202] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 13:55:27,203] [INFO] [timer.py:260:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=0.07545484280564399, CurrSamplesPerSec=0.07576765460719938, MemAllocated=14.52GB, MaxMemAllocated=17.28GB

-> loss:2.03515625 remain: 4138755 steps：12

-> loss:2.083984375 remain: 4138283 steps：11

-> loss:2.076171875 remain: 4137811 steps：10

-> loss:2.189453125 remain: 4137339 steps：9

-> loss:2.310546875 remain: 4136867 steps：8

-> loss:2.162109375 remain: 4136395 steps：7

-> loss:2.150390625 remain: 4135923 steps：6

-> loss:2.244140625 remain: 4135451 steps：5

-> loss:2.255859375 remain: 4134979 steps：4

-> loss:2.255859375 remain: 4134507 steps：3
[2023-09-12 13:57:34,416] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=2, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 13:57:34,417] [INFO] [timer.py:260:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=0.07655890859514254, CurrSamplesPerSec=0.07871690017372954, MemAllocated=14.52GB, MaxMemAllocated=17.28GB

-> loss:2.291015625 remain: 4134035 steps：2

-> loss:2.20703125 remain: 4133563 steps：1

-> loss:2.162109375 remain: 4133091 steps：0
172.16.0.62 - - [12/Sep/2023 13:58:02] "POST /train HTTP/1.1" 200 218
  0%|          | 0/582 [00:00<?, ?it/s]  3%|▎         | 19/582 [00:00<00:06, 84.83it/s]  5%|▍         | 28/582 [00:01<00:40, 13.64it/s]  6%|▌         | 34/582 [00:01<00:31, 17.15it/s]  7%|▋         | 39/582 [00:03<01:10,  7.67it/s]  9%|▉         | 52/582 [00:03<00:38, 13.86it/s] 10%|▉         | 58/582 [00:05<01:03,  8.26it/s] 13%|█▎        | 75/582 [00:06<00:55,  9.21it/s] 16%|█▌        | 93/582 [00:08<00:49,  9.94it/s] 19%|█▉        | 111/582 [00:10<00:45, 10.36it/s] 22%|██▏       | 129/582 [00:11<00:42, 10.60it/s] 25%|██▌       | 147/582 [00:13<00:39, 10.91it/s] 28%|██▊       | 165/582 [00:14<00:37, 10.99it/s] 31%|███▏      | 183/582 [00:16<00:36, 11.03it/s] 35%|███▍      | 201/582 [00:18<00:34, 11.05it/s] 38%|███▊      | 219/582 [00:19<00:32, 11.09it/s] 41%|████      | 237/582 [00:21<00:30, 11.23it/s] 44%|████▍     | 255/582 [00:23<00:29, 11.18it/s] 47%|████▋     | 273/582 [00:24<00:27, 11.13it/s] 50%|█████     | 291/582 [00:26<00:26, 11.10it/s] 53%|█████▎    | 309/582 [00:27<00:24, 11.09it/s] 56%|█████▌    | 327/582 [00:29<00:22, 11.26it/s] 59%|█████▉    | 345/582 [00:31<00:21, 11.20it/s] 60%|██████    | 351/582 [00:31<00:20, 11.28it/s]
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/rwkv-trainer/app.py", line 173, in load_model
    infer_model = RWKV_RNN(infer_args, cache_dict)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/jit/_script.py", line 292, in init_then_script
    original_init(self, *args, **kwargs)
  File "/home/neromous/rwkv-trainer/src/model_run.py", line 84, in __init__
    w[x] = w[x].to(self.RUN_DEVICE)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.70 GiB total capacity; 22.24 GiB already allocated; 2.56 MiB free; 22.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
172.16.0.62 - - [12/Sep/2023 14:06:37] "GET /load-model HTTP/1.1" 500 754
[2023-09-12 14:08:09,665] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 50752
[2023-09-12 14:08:11,908] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 50752
[2023-09-12 14:08:21,852] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 14:23:41,690] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 14:23:43,025] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 14:23:43,049] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 14:23:44,915] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 14:23:46,255] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 14:23:46,256] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 14:23:46,256] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 14:23:46,256] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 14:23:46,256] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 14:23:48,096] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.721454381942749 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.25it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.41s/it]100%|██████████| 1/1 [00:05<00:00,  5.41s/it]
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 29.38it/s]
total ['bonsai-extend-physic.jsonl', 'bonsai-extend.jsonl'] files  has 4563 items.
  0%|          | 0/4563 [00:00<?, ?it/s]  1%|▏         | 58/4563 [00:00<00:07, 576.40it/s]  3%|▎         | 118/4563 [00:00<00:07, 586.75it/s]  4%|▍         | 187/4563 [00:00<00:06, 631.41it/s]  6%|▌         | 251/4563 [00:00<00:07, 606.58it/s]  7%|▋         | 312/4563 [00:00<00:07, 603.78it/s]  8%|▊         | 375/4563 [00:00<00:06, 609.34it/s] 10%|▉         | 437/4563 [00:00<00:06, 602.37it/s] 11%|█         | 501/4563 [00:00<00:06, 613.25it/s] 12%|█▏        | 564/4563 [00:00<00:06, 616.77it/s] 14%|█▍        | 634/4563 [00:01<00:06, 640.45it/s] 15%|█▌        | 707/4563 [00:01<00:05, 665.81it/s] 17%|█▋        | 774/4563 [00:01<00:07, 533.86it/s] 18%|█▊        | 832/4563 [00:01<00:07, 510.23it/s] 19%|█▉        | 886/4563 [00:01<00:07, 490.73it/s] 21%|██        | 938/4563 [00:01<00:07, 473.69it/s] 22%|██▏       | 987/4563 [00:01<00:07, 469.68it/s] 23%|██▎       | 1035/4563 [00:01<00:07, 463.96it/s] 24%|██▎       | 1082/4563 [00:01<00:07, 458.13it/s] 25%|██▍       | 1129/4563 [00:02<00:07, 455.88it/s] 26%|██▌       | 1177/4563 [00:02<00:07, 461.86it/s] 27%|██▋       | 1224/4563 [00:02<00:07, 458.80it/s] 28%|██▊       | 1271/4563 [00:02<00:07, 459.52it/s] 29%|██▉       | 1318/4563 [00:02<00:07, 459.71it/s] 30%|██▉       | 1365/4563 [00:02<00:07, 451.55it/s] 31%|███       | 1412/4563 [00:02<00:06, 456.01it/s] 32%|███▏      | 1463/4563 [00:02<00:06, 470.15it/s] 33%|███▎      | 1514/4563 [00:02<00:06, 481.15it/s] 34%|███▍      | 1563/4563 [00:03<00:06, 482.81it/s] 35%|███▌      | 1613/4563 [00:03<00:06, 487.66it/s] 36%|███▋      | 1665/4563 [00:03<00:05, 494.62it/s] 38%|███▊      | 1716/4563 [00:03<00:05, 498.34it/s] 39%|███▊      | 1766/4563 [00:03<00:05, 497.00it/s] 40%|███▉      | 1817/4563 [00:03<00:05, 499.26it/s] 41%|████      | 1871/4563 [00:03<00:05, 510.38it/s] 42%|████▏     | 1923/4563 [00:03<00:05, 511.78it/s] 43%|████▎     | 1975/4563 [00:03<00:05, 510.63it/s] 44%|████▍     | 2027/4563 [00:03<00:04, 507.23it/s] 46%|████▌     | 2079/4563 [00:04<00:04, 508.14it/s] 47%|████▋     | 2130/4563 [00:04<00:04, 508.02it/s] 48%|████▊     | 2181/4563 [00:04<00:04, 494.16it/s] 49%|████▉     | 2235/4563 [00:04<00:04, 505.57it/s] 50%|█████     | 2290/4563 [00:04<00:04, 514.99it/s] 52%|█████▏    | 2353/4563 [00:04<00:04, 548.60it/s] 53%|█████▎    | 2417/4563 [00:04<00:03, 574.15it/s] 55%|█████▍    | 2490/4563 [00:04<00:03, 620.07it/s] 56%|█████▌    | 2564/4563 [00:04<00:03, 654.81it/s] 58%|█████▊    | 2639/4563 [00:04<00:02, 680.60it/s] 60%|█████▉    | 2733/4563 [00:05<00:02, 756.39it/s] 62%|██████▏   | 2811/4563 [00:05<00:02, 761.48it/s] 63%|██████▎   | 2891/4563 [00:05<00:02, 770.19it/s] 65%|██████▌   | 2983/4563 [00:05<00:01, 811.68it/s] 67%|██████▋   | 3065/4563 [00:05<00:01, 801.85it/s] 69%|██████▉   | 3146/4563 [00:05<00:01, 761.40it/s] 71%|███████   | 3223/4563 [00:05<00:01, 700.37it/s] 72%|███████▏  | 3295/4563 [00:05<00:02, 565.39it/s] 74%|███████▎  | 3357/4563 [00:06<00:02, 500.61it/s] 75%|███████▍  | 3412/4563 [00:06<00:02, 463.81it/s] 76%|███████▌  | 3462/4563 [00:06<00:02, 439.21it/s] 77%|███████▋  | 3508/4563 [00:06<00:02, 423.74it/s] 78%|███████▊  | 3552/4563 [00:06<00:02, 411.68it/s] 79%|███████▉  | 3594/4563 [00:06<00:02, 401.89it/s] 80%|███████▉  | 3635/4563 [00:06<00:02, 392.26it/s] 81%|████████  | 3675/4563 [00:06<00:02, 389.45it/s] 81%|████████▏ | 3715/4563 [00:07<00:02, 388.18it/s] 82%|████████▏ | 3754/4563 [00:07<00:02, 384.39it/s] 83%|████████▎ | 3793/4563 [00:07<00:02, 383.37it/s] 84%|████████▍ | 3832/4563 [00:07<00:01, 385.08it/s] 85%|████████▍ | 3874/4563 [00:07<00:01, 393.64it/s] 86%|████████▌ | 3914/4563 [00:07<00:01, 391.10it/s] 87%|████████▋ | 3954/4563 [00:07<00:01, 389.49it/s] 88%|████████▊ | 3993/4563 [00:07<00:01, 388.67it/s] 88%|████████▊ | 4032/4563 [00:07<00:01, 387.88it/s] 89%|████████▉ | 4071/4563 [00:07<00:01, 386.59it/s] 90%|█████████ | 4110/4563 [00:08<00:01, 382.34it/s] 91%|█████████ | 4149/4563 [00:08<00:01, 381.19it/s] 92%|█████████▏| 4188/4563 [00:08<00:00, 382.15it/s] 93%|█████████▎| 4227/4563 [00:08<00:00, 380.36it/s] 93%|█████████▎| 4266/4563 [00:08<00:00, 381.17it/s] 94%|█████████▍| 4305/4563 [00:08<00:00, 382.28it/s] 95%|█████████▌| 4344/4563 [00:08<00:00, 382.35it/s] 96%|█████████▌| 4383/4563 [00:08<00:00, 381.36it/s] 97%|█████████▋| 4422/4563 [00:08<00:00, 379.89it/s] 98%|█████████▊| 4461/4563 [00:08<00:00, 381.75it/s] 99%|█████████▊| 4500/4563 [00:09<00:00, 382.14it/s] 99%|█████████▉| 4539/4563 [00:09<00:00, 381.08it/s]100%|██████████| 4563/4563 [00:09<00:00, 494.33it/s]
[2023-09-12 14:24:58,001] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 14:24:58,002] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 14:24:58,002] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 14:25:00,420] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.306745767593384 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 14:25:06,197] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 14:25:06,245] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 14:25:06,246] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 14:25:06,246] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 14:25:06,246] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 14:25:06,246] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 14:25:06,246] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 14:25:06,246] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 14:25:17,792] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 14:25:17,793] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 14:25:17,793] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 29.36 GB, percent = 7.8%
[2023-09-12 14:25:30,981] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 14:25:30,982] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 14:25:30,982] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 68.64 GB, percent = 18.2%
[2023-09-12 14:25:30,982] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 14:25:31,940] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 14:25:31,940] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 14:25:31,941] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 68.65 GB, percent = 18.2%
[2023-09-12 14:25:31,961] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 14:25:31,961] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 14:25:31,961] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f5e3c093550>
[2023-09-12 14:25:31,961] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 14:25:31,962] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 14:25:31,963] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 14:25:31,963] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 14:25:31,963] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 14:25:31,963] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 14:25:31,963] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 14:25:31,963] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 14:25:31,963] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 14:25:31,963] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 14:25:31,963] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 14:25:31,963] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5e3c1772b0>
[2023-09-12 14:25:31,963] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 14:25:31,963] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 14:25:31,963] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 14:25:31,963] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 14:25:31,964] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 14:25:31,965] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 14:25:31,966] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 14:25:31,966] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 14:25:31,966] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 14:25:31,966] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 14:25:31,966] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 14:25:31,966] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 14:25:31,966] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

[2023-09-12 17:18:10,890] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 52455
[2023-09-12 17:18:16,133] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 17:18:21,202] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 17:18:22,585] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 17:18:22,609] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 17:18:24,469] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 17:18:25,756] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-09-12 17:18:25,757] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-12 17:18:25,757] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-12 17:18:25,757] [INFO] [launch.py:163:main] dist_world_size=1
[2023-09-12 17:18:25,757] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-09-12 17:18:27,594] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.7104437351226807 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 11.67it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.08s/it]100%|██████████| 1/1 [00:05<00:00,  5.08s/it]
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 29.07it/s]
total ['bonsai-extend-physic.jsonl', 'bonsai-extend.jsonl'] files  has 4563 items.
  0%|          | 0/4563 [00:00<?, ?it/s]  1%|▏         | 59/4563 [00:00<00:07, 587.21it/s]  3%|▎         | 121/4563 [00:00<00:07, 604.45it/s]  4%|▍         | 195/4563 [00:00<00:06, 663.00it/s]  6%|▌         | 262/4563 [00:00<00:06, 639.84it/s]  7%|▋         | 327/4563 [00:00<00:06, 638.62it/s]  9%|▊         | 391/4563 [00:00<00:06, 636.51it/s] 10%|▉         | 456/4563 [00:00<00:06, 638.39it/s] 11%|█▏        | 521/4563 [00:00<00:06, 636.35it/s] 13%|█▎        | 592/4563 [00:00<00:06, 657.42it/s] 15%|█▍        | 669/4563 [00:01<00:05, 689.97it/s] 16%|█▌        | 739/4563 [00:01<00:06, 611.73it/s] 18%|█▊        | 802/4563 [00:01<00:06, 561.59it/s] 19%|█▉        | 860/4563 [00:01<00:06, 534.46it/s] 20%|██        | 915/4563 [00:01<00:07, 514.84it/s] 21%|██        | 968/4563 [00:01<00:07, 497.25it/s] 22%|██▏       | 1019/4563 [00:01<00:07, 490.29it/s] 23%|██▎       | 1069/4563 [00:01<00:07, 488.21it/s] 25%|██▍       | 1119/4563 [00:01<00:07, 483.65it/s] 26%|██▌       | 1171/4563 [00:02<00:06, 491.23it/s] 27%|██▋       | 1221/4563 [00:02<00:06, 490.06it/s] 28%|██▊       | 1271/4563 [00:02<00:06, 489.83it/s] 29%|██▉       | 1321/4563 [00:02<00:06, 490.54it/s] 30%|███       | 1371/4563 [00:02<00:06, 481.18it/s] 31%|███       | 1422/4563 [00:02<00:06, 489.27it/s] 32%|███▏      | 1476/4563 [00:02<00:06, 503.90it/s] 34%|███▎      | 1530/4563 [00:02<00:05, 512.99it/s] 35%|███▍      | 1583/4563 [00:02<00:05, 516.42it/s] 36%|███▌      | 1637/4563 [00:02<00:05, 523.28it/s] 37%|███▋      | 1692/4563 [00:03<00:05, 529.76it/s] 38%|███▊      | 1746/4563 [00:03<00:05, 530.07it/s] 39%|███▉      | 1800/4563 [00:03<00:05, 530.66it/s] 41%|████      | 1857/4563 [00:03<00:05, 540.63it/s] 42%|████▏     | 1913/4563 [00:03<00:04, 546.25it/s] 43%|████▎     | 1968/4563 [00:03<00:04, 545.24it/s] 44%|████▍     | 2023/4563 [00:03<00:04, 541.29it/s] 46%|████▌     | 2078/4563 [00:03<00:04, 541.77it/s] 47%|████▋     | 2133/4563 [00:03<00:04, 540.78it/s] 48%|████▊     | 2188/4563 [00:04<00:04, 537.49it/s] 49%|████▉     | 2246/4563 [00:04<00:04, 548.65it/s] 51%|█████     | 2309/4563 [00:04<00:03, 570.94it/s] 52%|█████▏    | 2375/4563 [00:04<00:03, 596.51it/s] 54%|█████▎    | 2444/4563 [00:04<00:03, 623.57it/s] 55%|█████▌    | 2523/4563 [00:04<00:03, 672.82it/s] 57%|█████▋    | 2595/4563 [00:04<00:02, 686.09it/s] 59%|█████▉    | 2682/4563 [00:04<00:02, 740.57it/s] 61%|██████    | 2777/4563 [00:04<00:02, 797.80it/s] 63%|██████▎   | 2860/4563 [00:04<00:02, 801.78it/s] 65%|██████▍   | 2955/4563 [00:05<00:01, 844.93it/s] 67%|██████▋   | 3043/4563 [00:05<00:01, 853.64it/s] 69%|██████▊   | 3129/4563 [00:05<00:01, 850.82it/s] 70%|███████   | 3215/4563 [00:05<00:01, 771.07it/s] 72%|███████▏  | 3294/4563 [00:05<00:02, 612.78it/s] 74%|███████▎  | 3362/4563 [00:05<00:02, 539.51it/s] 75%|███████▍  | 3421/4563 [00:05<00:02, 498.72it/s] 76%|███████▌  | 3475/4563 [00:06<00:02, 471.94it/s] 77%|███████▋  | 3525/4563 [00:06<00:02, 453.76it/s] 78%|███████▊  | 3572/4563 [00:06<00:02, 441.02it/s] 79%|███████▉  | 3617/4563 [00:06<00:02, 428.03it/s] 80%|████████  | 3661/4563 [00:06<00:02, 421.58it/s] 81%|████████  | 3704/4563 [00:06<00:02, 418.33it/s] 82%|████████▏ | 3746/4563 [00:06<00:01, 414.14it/s] 83%|████████▎ | 3788/4563 [00:06<00:01, 412.16it/s] 84%|████████▍ | 3830/4563 [00:06<00:01, 411.58it/s] 85%|████████▍ | 3875/4563 [00:06<00:01, 420.77it/s] 86%|████████▌ | 3918/4563 [00:07<00:01, 418.52it/s] 87%|████████▋ | 3960/4563 [00:07<00:01, 417.15it/s] 88%|████████▊ | 4002/4563 [00:07<00:01, 416.90it/s] 89%|████████▊ | 4044/4563 [00:07<00:01, 414.88it/s] 90%|████████▉ | 4086/4563 [00:07<00:01, 412.74it/s] 90%|█████████ | 4128/4563 [00:07<00:01, 411.77it/s] 91%|█████████▏| 4170/4563 [00:07<00:00, 411.15it/s] 92%|█████████▏| 4212/4563 [00:07<00:00, 411.94it/s] 93%|█████████▎| 4254/4563 [00:07<00:00, 410.04it/s] 94%|█████████▍| 4296/4563 [00:08<00:00, 410.76it/s] 95%|█████████▌| 4338/4563 [00:08<00:00, 409.98it/s] 96%|█████████▌| 4380/4563 [00:08<00:00, 409.44it/s] 97%|█████████▋| 4421/4563 [00:08<00:00, 407.03it/s] 98%|█████████▊| 4462/4563 [00:08<00:00, 405.57it/s] 99%|█████████▊| 4503/4563 [00:08<00:00, 406.78it/s]100%|█████████▉| 4544/4563 [00:08<00:00, 406.31it/s]100%|██████████| 4563/4563 [00:08<00:00, 526.59it/s]
[2023-09-12 17:19:23,113] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-12 17:19:23,113] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-12 17:19:23,113] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-12 17:19:25,706] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module cpu_adam, skipping build step...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.2928216457366943 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000001, adam_w=1
[2023-09-12 17:19:31,496] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-12 17:19:31,546] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-12 17:19:31,546] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-12 17:19:31,546] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-09-12 17:19:31,546] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-09-12 17:19:31,547] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-09-12 17:19:31,547] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-09-12 17:19:31,547] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(3062753280, False)] 
[2023-09-12 17:19:43,599] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-09-12 17:19:43,600] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 17:19:43,600] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 30.27 GB, percent = 8.0%
[2023-09-12 17:19:56,877] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-09-12 17:19:56,878] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 17:19:56,879] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.49 GB, percent = 18.4%
[2023-09-12 17:19:56,879] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-09-12 17:19:57,952] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-09-12 17:19:57,953] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-09-12 17:19:57,953] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 69.49 GB, percent = 18.4%
[2023-09-12 17:19:57,984] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-12 17:19:57,985] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-09-12 17:19:57,985] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f3680a8d940>
[2023-09-12 17:19:57,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-09-12 17:19:57,986] [INFO] [config.py:963:print] DeepSpeedEngine configuration:
[2023-09-12 17:19:57,986] [INFO] [config.py:967:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-12 17:19:57,986] [INFO] [config.py:967:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-12 17:19:57,987] [INFO] [config.py:967:print]   amp_enabled .................. False
[2023-09-12 17:19:57,987] [INFO] [config.py:967:print]   amp_params ................... False
[2023-09-12 17:19:57,987] [INFO] [config.py:967:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-12 17:19:57,987] [INFO] [config.py:967:print]   bfloat16_enabled ............. False
[2023-09-12 17:19:57,987] [INFO] [config.py:967:print]   checkpoint_parallel_write_pipeline  False
[2023-09-12 17:19:57,987] [INFO] [config.py:967:print]   checkpoint_tag_validation_enabled  True
[2023-09-12 17:19:57,987] [INFO] [config.py:967:print]   checkpoint_tag_validation_fail  False
[2023-09-12 17:19:57,987] [INFO] [config.py:967:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3680b012b0>
[2023-09-12 17:19:57,987] [INFO] [config.py:967:print]   communication_data_type ...... None
[2023-09-12 17:19:57,987] [INFO] [config.py:967:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-12 17:19:57,987] [INFO] [config.py:967:print]   curriculum_enabled_legacy .... False
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   curriculum_params_legacy ..... False
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   data_efficiency_enabled ...... False
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   dataloader_drop_last ......... False
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   disable_allgather ............ False
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   dump_state ................... False
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   eigenvalue_enabled ........... False
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   eigenvalue_layer_num ......... 0
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   eigenvalue_max_iter .......... 100
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   eigenvalue_stability ......... 1e-06
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   eigenvalue_tol ............... 0.01
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   eigenvalue_verbose ........... False
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   elasticity_enabled ........... False
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   fp16_auto_cast ............... False
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   fp16_enabled ................. auto
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   fp16_master_weights_and_gradients  False
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   global_rank .................. 0
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   grad_accum_dtype ............. None
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   gradient_accumulation_steps .. 1
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   gradient_clipping ............ 1
[2023-09-12 17:19:57,988] [INFO] [config.py:967:print]   gradient_predivide_factor .... 1.0
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   initial_dynamic_scale ........ 65536
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   load_universal_checkpoint .... False
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   loss_scale ................... 0
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   memory_breakdown ............. False
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   mics_hierarchial_params_gather  False
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   mics_shard_size .............. -1
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   optimizer_legacy_fusion ...... False
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   optimizer_name ............... adam
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   optimizer_params ............. {'lr': 0.0001, 'eps': 1e-07, 'weight_decay': 1e-06, 'betas': [0.9, 0.999]}
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   pld_enabled .................. False
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   pld_params ................... False
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   prescale_gradients ........... False
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   scheduler_name ............... WarmupLR
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   scheduler_params ............. {'warmup_min_lr': 1e-05, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 8}
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   sparse_attention ............. None
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   sparse_gradients_enabled ..... False
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   steps_per_print .............. 10
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   train_batch_size ............. 1
[2023-09-12 17:19:57,989] [INFO] [config.py:967:print]   train_micro_batch_size_per_gpu  1
[2023-09-12 17:19:57,990] [INFO] [config.py:967:print]   use_node_local_storage ....... False
[2023-09-12 17:19:57,990] [INFO] [config.py:967:print]   wall_clock_breakdown ......... False
[2023-09-12 17:19:57,990] [INFO] [config.py:967:print]   world_size ................... 1
[2023-09-12 17:19:57,990] [INFO] [config.py:967:print]   zero_allow_untested_optimizer  False
[2023-09-12 17:19:57,990] [INFO] [config.py:967:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-12 17:19:57,990] [INFO] [config.py:967:print]   zero_enabled ................. True
[2023-09-12 17:19:57,990] [INFO] [config.py:967:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-12 17:19:57,990] [INFO] [config.py:967:print]   zero_optimization_stage ...... 2
[2023-09-12 17:19:57,990] [INFO] [config.py:953:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0001, 
            "eps": 1e-07, 
            "weight_decay": 1e-06, 
            "betas": [0.9, 0.999]
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 1e-05, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 8
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

172.16.0.62 - - [12/Sep/2023 17:21:49] "GET /save-weight HTTP/1.1" 200 26
[2023-09-12 17:27:46,620] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 58555
[2023-09-12 17:27:51,988] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-09-12 17:27:58,688] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 17:28:00,020] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-12 17:28:00,044] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed --deepspeed_config ds_config.config
[2023-09-12 17:28:01,927] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 17:28:03,222] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2023-09-12 17:28:03,222] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2023-09-12 17:28:03,222] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2023-09-12 17:28:03,222] [INFO] [launch.py:163:main] dist_world_size=2
[2023-09-12 17:28:03,222] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2023-09-12 17:28:05,116] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-12 17:28:05,158] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1/4] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/TH -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -c /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o 
[2/4] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/TH -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -c /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o 
[3/4] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/TH -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -c /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o 
[4/4] c++ cpu_adam.o cpu_adam_impl.o custom_cuda_kernel.cuda.o -shared -lcurand -L/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so
Loading extension module cpu_adam...
Time to load cpu_adam op: 30.913975715637207 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Loading extension module cpu_adam...
Time to load cpu_adam op: 30.70923638343811 seconds
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_1024/build.ninja...
Building extension module wkv_1024...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/2] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_1024 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/TH -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=1024 -std=c++17 -c /home/neromous/rwkv-trainer/cuda/wkv_cuda.cu -o wkv_cuda.cuda.o 
ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function '_Z15kernel_backwardIfEviiiPKT_S2_S2_S2_S2_S2_PS0_S3_S3_S3_' for 'sm_75'
ptxas info    : Function properties for _Z15kernel_backwardIfEviiiPKT_S2_S2_S2_S2_S2_PS0_S3_S3_S3_
    8192 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 52 registers, 448 bytes cmem[0], 16 bytes cmem[2]
ptxas info    : Compiling entry function '_Z14kernel_forwardIfEviiiPKT_S2_S2_S2_PS0_' for 'sm_75'
ptxas info    : Function properties for _Z14kernel_forwardIfEviiiPKT_S2_S2_S2_PS0_
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 39 registers, 408 bytes cmem[0]
ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function '_Z15kernel_backwardIfEviiiPKT_S2_S2_S2_S2_S2_PS0_S3_S3_S3_' for 'sm_86'
ptxas info    : Function properties for _Z15kernel_backwardIfEviiiPKT_S2_S2_S2_S2_S2_PS0_S3_S3_S3_
    8192 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 56 registers, 448 bytes cmem[0], 16 bytes cmem[2]
ptxas info    : Compiling entry function '_Z14kernel_forwardIfEviiiPKT_S2_S2_S2_PS0_' for 'sm_86'
ptxas info    : Function properties for _Z14kernel_forwardIfEviiiPKT_S2_S2_S2_PS0_
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, 408 bytes cmem[0]
[2/2] c++ wkv_op.o wkv_cuda.cuda.o -shared -L/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_1024.so
Loading extension module wkv_1024...
Loading extension module wkv_1024...
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 10.62it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  9.85it/s]100%|██████████| 1/1 [00:00<00:00,  9.82it/s]
total ['bonsai.jsonl'] files  has 1 items.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:04<00:00,  4.90s/it]100%|██████████| 1/1 [00:04<00:00,  4.90s/it]
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 1/1 [00:04<00:00,  4.95s/it]100%|██████████| 1/1 [00:04<00:00,  4.95s/it]
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 28.20it/s]
total ['bonsai-extend-physic.jsonl', 'bonsai-extend.jsonl'] files  has 4563 items.
  0%|          | 0/4563 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 29.12it/s]
  0%|          | 0/4563 [00:00<?, ?it/s]  1%|▏         | 61/4563 [00:00<00:07, 599.50it/s]  1%|▏         | 60/4563 [00:00<00:07, 591.49it/s]  3%|▎         | 124/4563 [00:00<00:07, 608.27it/s]  3%|▎         | 122/4563 [00:00<00:07, 602.79it/s]  4%|▍         | 200/4563 [00:00<00:06, 675.55it/s]  4%|▍         | 197/4563 [00:00<00:06, 668.01it/s]  6%|▌         | 268/4563 [00:00<00:06, 655.56it/s]  6%|▌         | 264/4563 [00:00<00:06, 643.40it/s]  7%|▋         | 334/4563 [00:00<00:06, 645.93it/s]  7%|▋         | 329/4563 [00:00<00:06, 638.67it/s]  9%|▊         | 399/4563 [00:00<00:06, 638.75it/s]  9%|▊         | 393/4563 [00:00<00:06, 637.27it/s] 10%|█         | 468/4563 [00:00<00:06, 653.72it/s] 10%|█         | 458/4563 [00:00<00:06, 636.70it/s] 12%|█▏        | 534/4563 [00:00<00:06, 648.36it/s] 11%|█▏        | 522/4563 [00:00<00:06, 636.98it/s] 13%|█▎        | 610/4563 [00:00<00:05, 681.52it/s] 13%|█▎        | 593/4563 [00:00<00:06, 658.04it/s] 15%|█▌        | 686/4563 [00:01<00:05, 703.78it/s] 15%|█▍        | 671/4563 [00:01<00:05, 693.93it/s] 17%|█▋        | 757/4563 [00:01<00:06, 597.30it/s] 16%|█▌        | 741/4563 [00:01<00:06, 609.34it/s] 18%|█▊        | 820/4563 [00:01<00:06, 560.44it/s] 18%|█▊        | 804/4563 [00:01<00:06, 561.63it/s] 19%|█▉        | 879/4563 [00:01<00:06, 533.66it/s] 19%|█▉        | 862/4563 [00:01<00:06, 533.33it/s] 20%|██        | 934/4563 [00:01<00:07, 510.89it/s] 20%|██        | 917/4563 [00:01<00:07, 510.51it/s] 22%|██▏       | 987/4563 [00:01<00:07, 503.51it/s] 21%|██        | 969/4563 [00:01<00:07, 498.76it/s] 23%|██▎       | 1038/4563 [00:01<00:07, 497.57it/s] 22%|██▏       | 1020/4563 [00:01<00:07, 491.59it/s] 24%|██▍       | 1089/4563 [00:01<00:07, 489.96it/s] 23%|██▎       | 1070/4563 [00:01<00:07, 487.90it/s] 25%|██▍       | 1139/4563 [00:01<00:06, 489.18it/s] 25%|██▍       | 1119/4563 [00:01<00:07, 481.75it/s] 26%|██▌       | 1190/4563 [00:02<00:06, 493.48it/s] 26%|██▌       | 1170/4563 [00:02<00:06, 488.29it/s] 27%|██▋       | 1240/4563 [00:02<00:06, 489.27it/s] 27%|██▋       | 1219/4563 [00:02<00:06, 487.93it/s] 28%|██▊       | 1290/4563 [00:02<00:06, 489.64it/s] 28%|██▊       | 1268/4563 [00:02<00:06, 485.49it/s] 29%|██▉       | 1340/4563 [00:02<00:06, 487.63it/s] 29%|██▉       | 1317/4563 [00:02<00:06, 485.49it/s] 30%|███       | 1389/4563 [00:02<00:06, 477.37it/s] 30%|██▉       | 1366/4563 [00:02<00:06, 477.74it/s] 32%|███▏      | 1443/4563 [00:02<00:06, 494.28it/s] 31%|███       | 1416/4563 [00:02<00:06, 482.43it/s] 33%|███▎      | 1498/4563 [00:02<00:06, 508.05it/s] 32%|███▏      | 1470/4563 [00:02<00:06, 498.12it/s] 34%|███▍      | 1551/4563 [00:02<00:05, 512.34it/s] 33%|███▎      | 1524/4563 [00:02<00:05, 507.96it/s] 35%|███▌      | 1603/4563 [00:02<00:05, 510.69it/s] 35%|███▍      | 1577/4563 [00:02<00:05, 511.89it/s] 36%|███▋      | 1657/4563 [00:03<00:05, 517.92it/s] 36%|███▌      | 1629/4563 [00:02<00:05, 507.60it/s] 37%|███▋      | 1711/4563 [00:03<00:05, 524.09it/s] 37%|███▋      | 1683/4563 [00:03<00:05, 515.66it/s] 39%|███▊      | 1764/4563 [00:03<00:05, 524.30it/s] 38%|███▊      | 1736/4563 [00:03<00:05, 519.51it/s] 40%|███▉      | 1818/4563 [00:03<00:05, 528.23it/s] 39%|███▉      | 1789/4563 [00:03<00:05, 520.76it/s] 41%|████      | 1876/4563 [00:03<00:04, 541.77it/s] 40%|████      | 1846/4563 [00:03<00:05, 534.26it/s] 42%|████▏     | 1931/4563 [00:03<00:04, 541.49it/s] 42%|████▏     | 1901/4563 [00:03<00:04, 537.71it/s] 44%|████▎     | 1986/4563 [00:03<00:04, 541.34it/s] 43%|████▎     | 1956/4563 [00:03<00:04, 539.92it/s] 45%|████▍     | 2041/4563 [00:03<00:04, 538.45it/s] 44%|████▍     | 2011/4563 [00:03<00:04, 535.83it/s] 46%|████▌     | 2096/4563 [00:03<00:04, 539.55it/s] 45%|████▌     | 2066/4563 [00:03<00:04, 537.23it/s] 47%|████▋     | 2150/4563 [00:03<00:04, 539.22it/s] 46%|████▋     | 2120/4563 [00:03<00:04, 536.16it/s] 48%|████▊     | 2205/4563 [00:04<00:04, 540.33it/s] 48%|████▊     | 2174/4563 [00:04<00:04, 535.13it/s] 50%|████▉     | 2261/4563 [00:04<00:04, 544.73it/s] 49%|████▉     | 2229/4563 [00:04<00:04, 537.05it/s] 51%|█████     | 2327/4563 [00:04<00:03, 577.57it/s] 50%|█████     | 2288/4563 [00:04<00:04, 551.08it/s] 52%|█████▏    | 2394/4563 [00:04<00:03, 604.50it/s] 52%|█████▏    | 2354/4563 [00:04<00:03, 582.00it/s] 54%|█████▍    | 2466/4563 [00:04<00:03, 637.79it/s] 53%|█████▎    | 2420/4563 [00:04<00:03, 604.12it/s] 56%|█████▌    | 2549/4563 [00:04<00:02, 693.84it/s] 55%|█████▍    | 2499/4563 [00:04<00:03, 659.47it/s] 58%|█████▊    | 2627/4563 [00:04<00:02, 719.08it/s] 56%|█████▋    | 2578/4563 [00:04<00:02, 696.60it/s] 60%|█████▉    | 2721/4563 [00:04<00:02, 784.07it/s] 58%|█████▊    | 2658/4563 [00:04<00:02, 727.03it/s] 61%|██████▏   | 2805/4563 [00:04<00:02, 799.12it/s] 60%|██████    | 2757/4563 [00:04<00:02, 804.62it/s] 63%|██████▎   | 2886/4563 [00:04<00:02, 801.61it/s] 62%|██████▏   | 2838/4563 [00:04<00:02, 802.09it/s] 65%|██████▌   | 2983/4563 [00:05<00:01, 848.96it/s] 64%|██████▍   | 2922/4563 [00:05<00:02, 811.29it/s] 67%|██████▋   | 3068/4563 [00:05<00:01, 846.58it/s] 66%|██████▌   | 3020/4563 [00:05<00:01, 860.29it/s] 69%|██████▉   | 3153/4563 [00:05<00:01, 814.82it/s] 68%|██████▊   | 3107/4563 [00:05<00:01, 846.49it/s] 70%|██████▉   | 3192/4563 [00:05<00:01, 797.14it/s] 71%|███████   | 3235/4563 [00:05<00:01, 714.05it/s] 72%|███████▏  | 3273/4563 [00:05<00:02, 635.66it/s] 73%|███████▎  | 3309/4563 [00:05<00:02, 587.76it/s] 73%|███████▎  | 3342/4563 [00:05<00:02, 550.86it/s] 74%|███████▍  | 3373/4563 [00:05<00:02, 526.96it/s] 75%|███████▌  | 3430/4563 [00:05<00:02, 490.82it/s] 75%|███████▍  | 3403/4563 [00:05<00:02, 503.87it/s] 76%|███████▋  | 3482/4563 [00:06<00:02, 466.44it/s] 76%|███████▌  | 3458/4563 [00:06<00:02, 473.11it/s] 77%|███████▋  | 3531/4563 [00:06<00:02, 449.30it/s] 77%|███████▋  | 3508/4563 [00:06<00:02, 454.57it/s] 78%|███████▊  | 3577/4563 [00:06<00:02, 437.68it/s] 78%|███████▊  | 3555/4563 [00:06<00:02, 440.49it/s] 79%|███████▉  | 3622/4563 [00:06<00:02, 424.57it/s] 79%|███████▉  | 3600/4563 [00:06<00:02, 427.41it/s] 80%|████████  | 3665/4563 [00:06<00:02, 419.34it/s] 80%|███████▉  | 3644/4563 [00:06<00:02, 418.16it/s] 81%|████████▏ | 3708/4563 [00:06<00:02, 416.34it/s] 81%|████████  | 3687/4563 [00:06<00:02, 414.36it/s] 82%|████████▏ | 3750/4563 [00:06<00:01, 412.52it/s] 82%|████████▏ | 3729/4563 [00:06<00:02, 411.86it/s] 83%|████████▎ | 3792/4563 [00:06<00:01, 410.79it/s] 83%|████████▎ | 3771/4563 [00:06<00:01, 407.88it/s] 84%|████████▍ | 3834/4563 [00:06<00:01, 412.64it/s] 84%|████████▎ | 3812/4563 [00:06<00:01, 407.97it/s] 85%|████████▍ | 3877/4563 [00:06<00:01, 417.24it/s] 85%|████████▍ | 3857/4563 [00:06<00:01, 417.04it/s] 86%|████████▌ | 3919/4563 [00:07<00:01, 415.32it/s] 85%|████████▌ | 3899/4563 [00:07<00:01, 414.15it/s] 87%|████████▋ | 3961/4563 [00:07<00:01, 413.84it/s] 86%|████████▋ | 3941/4563 [00:07<00:01, 412.77it/s] 88%|████████▊ | 4003/4563 [00:07<00:01, 413.91it/s] 87%|████████▋ | 3983/4563 [00:07<00:01, 411.16it/s] 89%|████████▊ | 4045/4563 [00:07<00:01, 412.46it/s] 88%|████████▊ | 4025/4563 [00:07<00:01, 411.28it/s] 90%|████████▉ | 4087/4563 [00:07<00:01, 410.63it/s] 89%|████████▉ | 4067/4563 [00:07<00:01, 409.86it/s] 90%|█████████ | 4129/4563 [00:07<00:01, 410.16it/s] 90%|█████████ | 4108/4563 [00:07<00:01, 408.21it/s] 91%|█████████▏| 4171/4563 [00:07<00:00, 410.01it/s] 91%|█████████ | 4149/4563 [00:07<00:01, 407.45it/s] 92%|█████████▏| 4213/4563 [00:07<00:00, 410.69it/s] 92%|█████████▏| 4190/4563 [00:07<00:00, 407.88it/s] 93%|█████████▎| 4255/4563 [00:07<00:00, 409.13it/s] 93%|█████████▎| 4231/4563 [00:07<00:00, 407.04it/s] 94%|█████████▍| 4296/4563 [00:08<00:00, 408.82it/s] 94%|█████████▎| 4272/4563 [00:08<00:00, 406.43it/s] 95%|█████████▌| 4337/4563 [00:08<00:00, 408.45it/s] 95%|█████████▍| 4313/4563 [00:08<00:00, 406.73it/s] 96%|█████████▌| 4378/4563 [00:08<00:00, 407.89it/s] 95%|█████████▌| 4354/4563 [00:08<00:00, 406.30it/s] 97%|█████████▋| 4419/4563 [00:08<00:00, 406.09it/s] 96%|█████████▋| 4395/4563 [00:08<00:00, 403.29it/s] 98%|█████████▊| 4460/4563 [00:08<00:00, 407.15it/s] 97%|█████████▋| 4436/4563 [00:08<00:00, 404.62it/s] 99%|█████████▊| 4502/4563 [00:08<00:00, 408.23it/s] 98%|█████████▊| 4477/4563 [00:08<00:00, 406.03it/s]100%|█████████▉| 4543/4563 [00:08<00:00, 407.01it/s] 99%|█████████▉| 4518/4563 [00:08<00:00, 405.48it/s]100%|██████████| 4563/4563 [00:08<00:00, 526.80it/s]
100%|█████████▉| 4559/4563 [00:08<00:00, 400.82it/s]100%|██████████| 4563/4563 [00:08<00:00, 522.67it/s]
Traceback (most recent call last):
  File "/home/neromous/rwkv-trainer/app.py", line 81, in <module>
    model_engine, optimizer, _, _ = deepspeed.initialize(model=model,
TypeError: initialize() got an unexpected keyword argument 'device_id'
Traceback (most recent call last):
  File "/home/neromous/rwkv-trainer/app.py", line 81, in <module>
    model_engine, optimizer, _, _ = deepspeed.initialize(model=model,
TypeError: initialize() got an unexpected keyword argument 'device_id'
[2023-09-12 17:29:39,343] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 59163
[2023-09-12 17:29:39,384] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 59164
[2023-09-12 17:29:39,384] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=1', '--deepspeed', '--deepspeed_config', 'ds_config.config'] exits with return code = 1
